{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KSAZfZcLA5uh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhanu teja\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\bhanu teja\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\bhanu teja\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# This cell imports necessary modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jReatzKWj3ei"
   },
   "outputs": [],
   "source": [
    "# Loading the TrainData and TestData\n",
    "directory = './'\n",
    "\n",
    "data = pd.read_csv('telescope_data.csv')\n",
    "features = data.iloc[:, :-1]\n",
    "labels = data['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, random_state=104,test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zyl_Sn2PlJBO"
   },
   "outputs": [],
   "source": [
    "# These are the functions taken from assignment 2 (template)\n",
    "# Function to calculate distance between two points\n",
    "# Function to calculate distance between two points\n",
    "def dis(x1, x2):\n",
    "    return np.linalg.norm(x1 - x2)\n",
    "\n",
    "# Function to perform classification \n",
    "def myclassifier(Train, Trainlabel, Test):\n",
    "    \" Train is the training data\"\n",
    "    \" Trainlabel is the training labels\"\n",
    "    \" Test is the testing data\"\n",
    "    pred = []\n",
    "\n",
    "    for testpoint in Test:\n",
    "        pred_dis = []\n",
    "        for trainpoint in Train:\n",
    "            pred_dis.append(dis(testpoint, trainpoint))\n",
    "\n",
    "        pred.append(Trainlabel[np.argmin(pred_dis)])\n",
    "\n",
    "    return np.array(pred)\n",
    "\n",
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    # Ensure that the true labels and predicted labels have the same length\n",
    "    if len(true_labels) != len(predicted_labels):\n",
    "        raise ValueError(\"Length of true_labels and predicted_labels must be the same.\")\n",
    "\n",
    "    # Count the number of correct predictions\n",
    "    correct_predictions = sum(1 for true, predicted in zip(true_labels, predicted_labels) if true == predicted)\n",
    "\n",
    "    # Calculate accuracy as the ratio of correct predictions to total predictions\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPMJhEFt7ml5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkOv18RTN_Fc"
   },
   "source": [
    "# PART 1\n",
    "\n",
    "- my_pca function is implemented\n",
    "- covariance matrix, eigen values and eigen vectore are also computed from scratch\n",
    "- number of components is set to 4 to maintain 95% variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 a, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "s77k7BmtdqeY"
   },
   "outputs": [],
   "source": [
    "def compute_covariance_matrix(X):\n",
    "    \"\"\"\n",
    "    Computes the covariance matrix of the input data and returns covariance matrix.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    covariance_matrix = np.dot(X.T, X) / (n_samples - 1)\n",
    "    return covariance_matrix\n",
    "\n",
    "def compute_eigenvalues_and_eigenvectors(cov_matrix):\n",
    "    \"\"\"\n",
    "    Compute the eigenvalues and eigenvectors of the input covariance matrix and returns a tuple containing eigenvalues and eigenvectors.\n",
    "    \"\"\"\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "    return eigenvalues, eigenvectors\n",
    "\n",
    "def my_pca(data,k):\n",
    "    \"\"\"\n",
    "    my_pca performs dimensionality reduction on the input data and returns the\n",
    "    transformed data, eigen values, eigen vector of the train data\n",
    "    which can be used to transform test data, to make test data consistent with trian data\n",
    "    \"\"\"\n",
    "\n",
    "    # We will check the time taken for my_pca function to complete its execution\n",
    "    # So, start timer at the beginning of the function\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    # Next is that we have to find the covariance matrix\n",
    "    # to find the covariance of the data, we use the above defined function\n",
    "    cov_matrix = compute_covariance_matrix(data)\n",
    "\n",
    "    # Next step is that we will calculate the Eigen Values and Eigen Vectors from the cov_matrix\n",
    "    # for that we will use linear algebra module from numpy\n",
    "    # and use eig() function to get the EigVal and EigVec\n",
    "    EigVal, EigVec = compute_eigenvalues_and_eigenvectors(cov_matrix)\n",
    "\n",
    "    # Now we will sort Eigen Values and corresponding Eigen Vectors in descending order\n",
    "    # so that we can choose top k eigen vectors from all the vectors\n",
    "    sorted_indices = np.argsort(EigVal)[::-1]\n",
    "    EigVal = EigVal[sorted_indices]\n",
    "    EigVec = EigVec[:, sorted_indices]\n",
    "\n",
    "    total_variance = np.sum(EigVal)\n",
    "\n",
    "    # # Automatically set k to retain 95% of the variance\n",
    "    threshold_variance = 0.95\n",
    "    cumulative_variance = np.cumsum(EigVal) / total_variance\n",
    "    k = np.argmax(cumulative_variance >= threshold_variance) + 1\n",
    "\n",
    "    k_95 = k\n",
    "    print(f'In order to retian 95% of the variance, we can use : {k_95} components')\n",
    "\n",
    "    # Project the data onto the selected EigVec\n",
    "    transformed_data = np.dot(data, EigVec[:, :k_95])\n",
    "    end_time = time.time()\n",
    "\n",
    "    time_taken = end_time - start_time\n",
    "    print(f\"Time taken to completely run my_pca code: {time_taken:.7f} seconds\")\n",
    "\n",
    "    return transformed_data, EigVal[:k_95], EigVec[:, :k_95], k_95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vk07qhBYOfqL",
    "outputId": "bd6fd6fc-05da-4069-e11e-c12ec2ed10ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In order to retian 95% of the variance, we can use : 4 components\n",
      "Time taken to completely run my_pca code: 0.0009995 seconds\n"
     ]
    }
   ],
   "source": [
    "X_train_pca, pca_EigVal, pca_EigVec, k_95= my_pca(x_train.values,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAam4Dd1lJHL",
    "outputId": "ea3d05cf-a1a4-46a7-aa7e-085a92048386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to completely run PCA from sklearn : 0.0030 seconds\n",
      "Variance retained by 4 components is: \n",
      "0.9228730221307562\n",
      "Time taken to completely run PCA from sklearn : 0.0030 seconds\n",
      "Variance retained by 5 components is: \n",
      "0.9669099828148261\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "\"\"\"\n",
    "Frist we will create an instacne of PCA from sklearn, then fit the train data into the instance using pca.fit() which finds necessary parameters like EigValues, EigVectors etc.,\n",
    "Then we will transform the train data using pca.transform()\n",
    "\"\"\"\n",
    "pca = PCA(n_components=k_95)\n",
    "pca.fit(x_train)\n",
    "transformed_train_data_pca_sk = pca.transform(x_train)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Time taken to completely run PCA from sklearn : {time_taken:.4f} seconds\")\n",
    "print(f'Variance retained by {k_95} components is: ')\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "# since variacne retained is less than 95%, we have to include some more components\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\"\"\"\n",
    "Frist we will create an instacne of PCA from sklearn, then fit the train data into the instance using pca.fit() which finds necessary parameters like EigValues, EigVectors etc.,\n",
    "Then we will transform the train data using pca.transform()\n",
    "\"\"\"\n",
    "pca = PCA(n_components=k_95+1)\n",
    "pca.fit(x_train)\n",
    "transformed_train_data_pca_sk = pca.transform(x_train)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "print(f\"Time taken to completely run PCA from sklearn : {time_taken:.4f} seconds\")\n",
    "print(f'Variance retained by {k_95+1} components is: ')\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1.2 b) Observation:\n",
    "\n",
    "We can see that in order to retain 95% variance, pca implemented from scratch considered 4 components, while pca from sklearn considered 5 components, with 4 components variance is less than 95%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQ42FM64OhkA"
   },
   "source": [
    "# PART 2\n",
    "\n",
    " - Kernal PCA is implemented from scratch\n",
    " - rbf kernel, poly kernel, linear kernels are also implemented from scratch\n",
    " - And training data is transformed as well. and stored in x_train_rbf, x_train_poly, x_train_linear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpca(K, n_samples, n_components):\n",
    "    \n",
    "    # Centering  the kernel matrix\n",
    "    one_n = np.ones((n_samples, n_samples)) / n_samples\n",
    "    K_centered = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n",
    "    \n",
    "    # Eigen decomposition\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(K_centered)\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    # picking up the top eigen vectors, and also transforms the train data\n",
    "    X_kpca = eigenvectors[:, :n_components] * np.sqrt(eigenvalues[:n_components])\n",
    "    return X_kpca, eigenvectors[:, :n_components]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1 RBF kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X1, X2, gamma):\n",
    "    \"\"\"\n",
    "    Computes the RBF kernel matrix given two sets of data points and returns the RBF kernel matrix.\n",
    "    \"\"\"\n",
    "    n_samples1 = X1.shape[0]\n",
    "    n_samples2 = X2.shape[0]\n",
    "    K = np.zeros((n_samples1, n_samples2))\n",
    "    for i in range(n_samples1):\n",
    "        for j in range(n_samples2):\n",
    "            K[i, j] = np.exp(-gamma * np.linalg.norm(X1[i] - X2[j])**2)\n",
    "    return K\n",
    "\n",
    "def kpca_rbf(X, gamma, n_components):\n",
    "    \"\"\"\n",
    "    performs Kernel PCA with RBF kernel and returns the eigenvectors corresponding to the selected components.\n",
    "    \"\"\"\n",
    "    K = rbf_kernel(X, X, gamma)\n",
    "    return kpca(K, X.shape[0], n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training KPCA with RBF kernel model\n",
    "gamma = 0.1\n",
    "X_train_rbf, eigenvectors_rbf = kpca_rbf(x_train.values, gamma, k_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2 Poly Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_kernel(X1, X2, degree, coef0):\n",
    "    \"\"\"\n",
    "    Computes the Polynomial kernel matrix given two sets of data points and returns the polynomial kernel matrix (n_samples1, n_samples2).\n",
    "    \"\"\"\n",
    "    return (np.dot(X1, X2.T) + coef0) ** degree\n",
    "\n",
    "def kpca_poly(X, degree, coef0, n_components):\n",
    "    \"\"\"\n",
    "    performs Kernel PCA with Polynomial kernel and retunrs the eigenvectors corresponding to the selected components.\n",
    "    \"\"\"\n",
    "    K = poly_kernel(X, X, degree, coef0)\n",
    "    return kpca(K, X.shape[0], n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KPCA with Polynomial kernel model\n",
    "degree = 2\n",
    "coef0 = 1\n",
    "X_train_poly, eigenvectors_poly = kpca_poly(x_train.values, degree, coef0, k_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.3 Linear Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(X1, X2):\n",
    "    \"\"\"\n",
    "    Computes the Linear kernel matrix given two sets of data points and finally returns the linear kernel matrix (n_samples1, n_samples2).\n",
    "    \"\"\"\n",
    "    return np.dot(X1, X2.T)\n",
    "\n",
    "def kpca_linear(X, n_components):\n",
    "    \"\"\"\n",
    "    Performs Kernel PCA with Linear kernel and returns the eigenvectors corresponding to the selected components.\n",
    "    \"\"\"\n",
    "    K = linear_kernel(X, X)\n",
    "    return kpca(K, X.shape[0], n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KPCA with Linear kernel model\n",
    "X_train_linear, eigenvectors_linear = kpca_linear(x_train.values, k_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTwhRmL1PKX1"
   },
   "source": [
    "# PART 3\n",
    "\n",
    "- Test data is transformed or decomposed using the returned parameters from each kcpa functionm like EigVec_rbf, scaler_rbf etc., to make the test data consistent with train data\n",
    "\n",
    "- Predictions were obtained using myclassifier\n",
    "- Accuracy of the classification was found using calculate_accuracy function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3.1 Transforming test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test using PCA\n",
    "X_test_pca = np.dot(x_test, pca_EigVec)\n",
    "\n",
    "# Transform test data using KPCA with RBF kernel\n",
    "X_test_rbf = np.dot(rbf_kernel(x_test.values, x_train.values, gamma), eigenvectors_rbf)\n",
    "\n",
    "# Transform test data using KPCA with Polynomial kernel\n",
    "X_test_poly = np.dot(poly_kernel(x_test.values, x_train.values, 3, coef0), eigenvectors_poly)\n",
    "\n",
    "# Transform test data using KPCA with Linear kernel\n",
    "X_test_linear = np.dot(linear_kernel(x_test.values, x_train.values), eigenvectors_linear)\n",
    "\n",
    "# Ensure dimensionality reduction consistency with training data\n",
    "X_test_rbf = X_test_rbf[:, :k_95]\n",
    "X_test_poly = X_test_poly[:, :k_95]\n",
    "X_test_linear = X_test_linear[:, :k_95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgEU5yZ5o9bc",
    "outputId": "ca88c391-2dbe-4dac-91f8-4ff4649af6cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy obtained using my_pca : 0.6585365853658537\n",
      "Accuracy obtained using KCPA with rbf kernel: 0.43902439024390244\n",
      "Accuracy obtained using KCPA with poly kernel: : 0.6829268292682927\n",
      "Accuracy obtained using KCPA with linear kernel: : 0.7073170731707317\n"
     ]
    }
   ],
   "source": [
    "# Now we will use the above obtained transformed test data and transformed train data\n",
    "# to get predictions from minimum distance classifier - myclassifier declared (taken from assignment 2 template)\n",
    "# And then we will calculate accuracy using calculate_accuracy function (from assignment 2 template)\n",
    "\n",
    "# Getting predictions\n",
    "predictions_pca = myclassifier(X_train_pca, y_train.values, X_test_pca)\n",
    "predictions_rbf = myclassifier(X_train_rbf, y_train.values, X_test_rbf)\n",
    "predictions_poly = myclassifier(X_train_poly, y_train.values, X_test_poly)\n",
    "predictions_linear = myclassifier(X_train_linear, y_train.values, X_test_linear)\n",
    "\n",
    "# Finding accuracy\n",
    "accuracy = calculate_accuracy(y_test.values, predictions_pca)\n",
    "print(f'Accuracy obtained using my_pca :', accuracy)\n",
    "\n",
    "accuracy = calculate_accuracy(y_test.values, predictions_rbf)\n",
    "print(f'Accuracy obtained using KCPA with rbf kernel:', accuracy)\n",
    "\n",
    "accuracy = calculate_accuracy(y_test.values, predictions_poly)\n",
    "print(f'Accuracy obtained using KCPA with poly kernel: :', accuracy)\n",
    "\n",
    "accuracy = calculate_accuracy(y_test.values, predictions_linear)\n",
    "print(f'Accuracy obtained using KCPA with linear kernel: :', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPmlWUL_8-5M"
   },
   "source": [
    "# Conclusions:\n",
    "\n",
    "##### Effectiveness of PCA and KPCA in reducing dimensionality :\n",
    "\n",
    "- Our train data has around 160 rows, we reduced the data using PCA and KPCA and still be able achieve accuracy of 63%\n",
    "- If we have used the high dimensional data, the time taken will also be signinficantly higher, but now we were able to complete in less time\n",
    "- This accuracy might also increase if we do hyper-parameter tuning\n",
    "- With this assignment, we can conclude that, PCA and KPCA are very effective in reducing the dimension of the high-dimensional data's\n",
    "\n",
    "##### Impact on classification performance :\n",
    "\n",
    "- We have reduced the data into 4 components (while i ran this script, i got 95% variability with 4 components) but using my_pca\n",
    "- Among all those the reduction using PCA achieved 70.73% accuracy, followed by KPCA with poly_kernel with 68.29% accuracy, followed by linear_kernel with 70.73% accuracy. Redcution with rbf_kernel performed pooly with 43.90%\n",
    "- So, choice of reduction technique is impacting the classification.\n",
    "\n",
    "\n",
    "##### Advantages of using own functions written from scratch:\n",
    "\n",
    "- We can modify the code and make it useful for other requirements, if we want to make any tweeks for specific requirement\n",
    "- Since we can see every detail of the code, we can debug, if we have any errors for any part of data\n",
    "- From the codes this is evident that time taken for our own function, works faster than the reduction using PCA from sklearn\n",
    "\n",
    "##### Disadvantages :\n",
    "\n",
    "- Since this code is written from scratch, optimisations that were written in scikit learn functions will be missing\n",
    "- Data used in this assignment is not high dimensional (210 rows, 10 columns), but when used with high dimensional data with large number of rows and columns, these optimisation that were written in scikit learn functions might make the code faster than our code wirtten from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OEUNsiqBpNH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
