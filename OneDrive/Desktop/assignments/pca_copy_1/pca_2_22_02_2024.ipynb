{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ld9VWTmaffZr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-c3LpvaEFwTl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhanu teja\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\bhanu teja\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\bhanu teja\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Importing neccessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "telescope_data = pd.read_csv('./telescope_data.csv')\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(telescope_data))\n",
    "telescope_data = telescope_data.iloc[shuffled_indices]\n",
    "\n",
    "# Let's use 80% of the data for training, and 20% for testing\n",
    "train_size = int(0.8 * len(telescope_data))\n",
    "\n",
    "# Select the first 80% of shuffled data for training\n",
    "x_train = telescope_data.iloc[:train_size, :-1]\n",
    "y_train = telescope_data.iloc[:train_size, -1]\n",
    "\n",
    "# The remaining data will be for testing\n",
    "x_test = telescope_data.iloc[train_size:, :-1]\n",
    "y_test = telescope_data.iloc[train_size:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate distance between two points\n",
    "def dis(x1, x2):\n",
    "    return np.linalg.norm(x1 - x2)\n",
    "\n",
    "# Function to perform classification \n",
    "def myclassifier(Train, Trainlabel, Test):\n",
    "    \" Train is the training data\"\n",
    "    \" Trainlabel is the training labels\"\n",
    "    \" Test is the testing data\"\n",
    "    pred = []\n",
    "\n",
    "    for testpoint in Test:\n",
    "        pred_dis = []\n",
    "        for trainpoint in Train:\n",
    "            pred_dis.append(dis(testpoint, trainpoint))\n",
    "\n",
    "        pred.append(Trainlabel[np.argmin(pred_dis)])\n",
    "\n",
    "    return np.array(pred)\n",
    "\n",
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    # Ensure that the true labels and predicted labels have the same length\n",
    "    if len(true_labels) != len(predicted_labels):\n",
    "        raise ValueError(\"Length of true_labels and predicted_labels must be the same.\")\n",
    "\n",
    "    # Count the number of correct predictions\n",
    "    correct_predictions = sum(1 for true, predicted in zip(true_labels, predicted_labels) if true == predicted)\n",
    "\n",
    "    # Calculate accuracy as the ratio of correct predictions to total predictions\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEtPV4dwJw5l"
   },
   "source": [
    "**PART 1 - Principal Component Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance_matrix(X):\n",
    "    \"\"\"\n",
    "    The covariance between two features X_i and X_j is calculated using the formula:\n",
    "    \n",
    "    cov(X_i, X_j) = sum[(X_i - mean_i) * (X_j - mean_j)] / (n_samples - 1)\n",
    "    \n",
    "    where:\n",
    "    - X_i and X_j are the features (columns) of the data matrix X.\n",
    "    - X_ik and X_jk are the values of features X_i and X_j at sample k, respectively.\n",
    "    - mean_i and mean_j are the means of features X_i and X_j, respectively.\n",
    "    - n_samples is the number of samples (rows) in the data matrix X.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    covariance_matrix = np.zeros((n_features, n_features))\n",
    "    \n",
    "    # Calculate covariance matrix\n",
    "    for i in range(n_features):\n",
    "        for j in range(n_features):\n",
    "            # Calculate mean of feature i and feature j\n",
    "            mean_i = np.mean(X[:, i])\n",
    "            mean_j = np.mean(X[:, j])\n",
    "            \n",
    "            # Calculate covariance between feature i and feature j\n",
    "            cov_ij = np.sum((X[:, i] - mean_i) * (X[:, j] - mean_j)) / (n_samples - 1)\n",
    "            covariance_matrix[i, j] = cov_ij\n",
    "    \n",
    "    return covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigen(matrix):\n",
    "    \"\"\"\n",
    "    Computes the eigenvalues and eigenvectors of the input matrix and returns them.\n",
    "    \"\"\"\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n",
    "    return eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqSMOOTeeMFP"
   },
   "source": [
    "**a**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EFDMA6bJd8-C",
    "outputId": "38bf5083-afb6-4046-cb81-31612a9051a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain 95% of variance: 5\n"
     ]
    }
   ],
   "source": [
    "def my_pca(data, k):\n",
    "    \"\"\"\n",
    "    Perform Principal Component Analysis (PCA) on the given data.\n",
    "\n",
    "    Parameters:\n",
    "    data (numpy.ndarray): The input data for PCA.\n",
    "    k (int): The number of principal components to retain.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Transformed data after PCA.\n",
    "    numpy.ndarray: Top k eigenvectors.\n",
    "    \"\"\"\n",
    "\n",
    "    cov_matrix = covariance_matrix(x_train.values)\n",
    "\n",
    "    eigenvalues, eigenvectors = compute_eigen(cov_matrix)\n",
    "\n",
    "    # Sorting eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # Choosing the top k eigenvectors to retain variance\n",
    "    top_eigenvectors = eigenvectors[:, :k]\n",
    "\n",
    "    # Projecting the data onto the top k eigenvectors\n",
    "    reduced_data = np.dot(x_train, top_eigenvectors)\n",
    "\n",
    "    return reduced_data, top_eigenvectors\n",
    "\n",
    "k = 10 # we can vary this based on our need\n",
    "# first i will get the all 10 components, then i will find the total variance\n",
    "# then evaluate how many components should i consider to retain 95% of variance\n",
    "reduced_train_data, top_eigenvectors = my_pca(x_train, k)\n",
    "\n",
    "# We are calculating total variance first\n",
    "total_variance = np.sum(np.var(reduced_train_data, axis=0))\n",
    "\n",
    "# Choose an appropriate number of principal components to retain a significant amount of variance\n",
    "threshold_variance = 0.95 # we can vary, but i have taken 95%\n",
    "cumulative_variance = np.cumsum(np.var(reduced_train_data, axis=0)) / total_variance\n",
    "selected_components = np.argmax(cumulative_variance >= threshold_variance) + 1\n",
    "\n",
    "print(f\"Number of components to retain {threshold_variance:.0%} of variance:\", selected_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-khPOcIXsZ_"
   },
   "source": [
    "**1.1 - b, c :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COXWvGfahR6i",
    "outputId": "2354124c-86ba-4779-b0e6-5b22199a3723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.0045 seconds\n"
     ]
    }
   ],
   "source": [
    "# From above code we got that with k = 5 with which we can retain 95% of the variance\n",
    "# so, we will be using  k = 5\n",
    "start_time = time.time()\n",
    "transformed_pca_train, pca_eigenvectors = my_pca(x_train.values, k=120)\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "real_transformed_pca_train = np.real(transformed_pca_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZJa-J0X7R1p"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MIsoYkVd9O8"
   },
   "source": [
    "**1.2 a**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExNgO2Y6cPNC",
    "outputId": "106c701f-afbd-478d-adf6-62910ab31e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.0095 seconds\n",
      "Total variance retained: 0.9697\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Creating a PCA instance with the desired number of components\n",
    "n_components = 5\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fitting PCA on the training data to compute principal components\n",
    "pca.fit(x_train)\n",
    "\n",
    "# Transforming the training data into the new feature space defined by PCs\n",
    "transformed_train_data_pca_sk = pca.transform(x_train)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "\n",
    "# Print the total variance retained\n",
    "total_variance_retained = np.sum(pca.explained_variance_ratio_)\n",
    "print(f\"Total variance retained: {total_variance_retained:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdpxrfrKU2KX"
   },
   "source": [
    "# 1.2 c Key findings :\n",
    "### Sklearn PCA vs my_pca\n",
    "\n",
    "- Although both the PCA from sklearn library and my_pca used same number of components to retain 95% of variance, the time taken to transform the data varies a bit\n",
    "- From the output of above code block we can see that\n",
    "    - Execution time for my_pca is 0.0045 seconds,\n",
    "    - Whereas Exectution time for PCA function from sklearn library is 0.0095 seconds whcih is a little slower\n",
    "    - But if the dimensionality of the data increases, there might be a chance that our own my_pca migh take very high time compared to sklearn's pca\n",
    "\n",
    "**Conlusion : Using sklearn's inbuilt functions are a little slower than using functions that we were written from scratch, but if the dimensionality may increase, then sklearn's inbuild functions might perform faster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UgCpw7XcPQh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKeZPcpoa-q5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zY0lqKdSJ2GN"
   },
   "source": [
    "**PART 2 - Kernal PCA (KPCA)**\n",
    "\n",
    "**2.1 KPCA with rbf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kpFzbXWnJzIe"
   },
   "outputs": [],
   "source": [
    "def pairwise_distances_manual(X1, X2):\n",
    "    # Compute squared Euclidean distances between each pair of samples\n",
    "    n_samples_X1 = X1.shape[0]\n",
    "    n_samples_X2 = X2.shape[0]\n",
    "    pairwise_distances = np.zeros((n_samples_X1, n_samples_X2))\n",
    "    for i in range(n_samples_X1):\n",
    "        for j in range(n_samples_X2):\n",
    "            pairwise_distances[i, j] = np.sum((X1[i] - X2[j])**2)\n",
    "    return pairwise_distances\n",
    "\n",
    "def rbf_kernel(X1, X2, gamma=1.0):\n",
    "    # Compute pairwise distances manually\n",
    "    pairwise_distances = pairwise_distances_manual(X1, X2)\n",
    "    \n",
    "    # Compute kernel matrix using the RBF kernel formula\n",
    "    kernel_matrix = np.exp(-gamma * pairwise_distances)\n",
    "\n",
    "    return kernel_matrix\n",
    "\n",
    "\n",
    "def kpca_rbf(X, gamma=1.0, n_components=None):\n",
    "    n_samples, _ = X.shape\n",
    "\n",
    "    # Calculate the kernel matrix using the RBF kernel defined above\n",
    "    kernel_matrix = rbf_kernel(X, X, gamma=gamma)\n",
    "    \n",
    "    # Centering the kernel matrix\n",
    "    one_n = np.ones((n_samples, n_samples)) / n_samples\n",
    "    kernel_matrix_centered = kernel_matrix - np.dot(one_n, kernel_matrix) - np.dot(kernel_matrix, one_n) + np.dot(np.dot(one_n, kernel_matrix), one_n)\n",
    "\n",
    "    # Computing eigenvalues and eigenvectors of the centered kernel matrix\n",
    "    eigenvalues, eigenvectors = compute_eigen(kernel_matrix_centered)\n",
    "\n",
    "    # Sorting eigenvalues and corresponding eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # Selecting the top n_components eigenvectors if specified\n",
    "    if n_components is not None:\n",
    "        eigenvalues = eigenvalues[:n_components]\n",
    "        eigenvectors = eigenvectors[:, :n_components]\n",
    "\n",
    "    # Projecting the data onto the selected eigenvectors\n",
    "    transformed_data = np.dot(kernel_matrix_centered, eigenvectors)\n",
    "\n",
    "    return transformed_data, eigenvalues, eigenvectors\n",
    "\n",
    "\n",
    "transformed_rbf_train, rbf_eigenvalues, rbf_eigenvectors= kpca_rbf(x_train.values,1.0, n_components=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOjAKwT9XN5I"
   },
   "source": [
    "**2.2 KPCA with Polynomial Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cfpu4xSxJzKt"
   },
   "outputs": [],
   "source": [
    "# Define the Polynomial kernel function\n",
    "def polynomial_kernel(X1, X2, degree=3):\n",
    "    kernel_matrix = (1 + np.dot(X1, X2.T)) ** degree\n",
    "    return kernel_matrix\n",
    "\n",
    "# Define the Kernel PCA function with Polynomial kernel\n",
    "def kpca_poly(X, degree=2, n_components=None):\n",
    "    n_samples, _ = X.shape\n",
    "\n",
    "    kernel_matrix = polynomial_kernel(X, X, degree=degree)\n",
    "\n",
    "    # Center the kernel matrix\n",
    "    one_n = np.ones((n_samples, n_samples)) / n_samples\n",
    "    kernel_matrix_centered = kernel_matrix - np.dot(one_n, kernel_matrix) - np.dot(kernel_matrix, one_n) + np.dot(np.dot(one_n, kernel_matrix), one_n)\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors of the centered kernel matrix\n",
    "    eigenvalues, eigenvectors = compute_eigen(kernel_matrix_centered)\n",
    "\n",
    "    # Sort eigenvalues and corresponding eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # Select the top n_components eigenvectors if specified\n",
    "    if n_components is not None:\n",
    "        eigenvalues = eigenvalues[:n_components]\n",
    "        eigenvectors = eigenvectors[:, :n_components]\n",
    "\n",
    "    # Project the data onto the selected eigenvectors\n",
    "    transformed_data = np.dot(kernel_matrix_centered, eigenvectors)\n",
    "\n",
    "    return transformed_data, eigenvalues, eigenvectors\n",
    "\n",
    "n_components = 5  # Adjust the number of components as needed\n",
    "degree_poly = 3  # Adjust the degree of the polynomial kernel\n",
    "\n",
    "# Perform Polynomial Kernel PCA transformation on the training data\n",
    "transformed_poly_train, poly_eigenvalues, poly_eigenvectors = kpca_poly(x_train.values, degree=degree_poly, n_components=n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGJ4dLUmZqKT"
   },
   "source": [
    "**2.3 KPCA with Linear Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B2-MlZs1JzNH"
   },
   "outputs": [],
   "source": [
    "# Define the Linear kernel function\n",
    "def linear_kernel(X1, X2):\n",
    "    return np.dot(X1, X2.T)\n",
    "\n",
    "# Define the Kernel PCA function with Linear kernel\n",
    "def kpca_linear(X, n_components=None):\n",
    "    n_samples, _ = X.shape\n",
    "\n",
    "    # Calculate the kernel matrix using the Linear kernel\n",
    "    kernel_matrix = linear_kernel(X, X)\n",
    "\n",
    "    # Center the kernel matrix\n",
    "    one_n = np.ones((n_samples, n_samples)) / n_samples\n",
    "    kernel_matrix_centered = kernel_matrix - np.dot(one_n, kernel_matrix) - np.dot(kernel_matrix, one_n) + np.dot(np.dot(one_n, kernel_matrix), one_n)\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors of the centered kernel matrix\n",
    "    eigenvalues, eigenvectors = compute_eigen(kernel_matrix_centered)\n",
    "\n",
    "    # Sort eigenvalues and corresponding eigenvectors in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[sorted_indices]\n",
    "    eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # Select the top n_components eigenvectors if specified\n",
    "    if n_components is not None:\n",
    "        eigenvalues = eigenvalues[:n_components]\n",
    "        eigenvectors = eigenvectors[:, :n_components]\n",
    "\n",
    "    # Project the data onto the selected eigenvectors\n",
    "    transformed_data = np.dot(kernel_matrix_centered, eigenvectors)\n",
    "\n",
    "    return transformed_data, eigenvalues, eigenvectors\n",
    "\n",
    "\n",
    "# Set the number of components\n",
    "n_components = 5\n",
    "\n",
    "# Perform Kernel PCA transformation on the training data\n",
    "transformed_linear_train, linear_eigenvalues, linear_eigenvectors = kpca_linear(x_train.values, n_components=n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjQM_1qxJ5uP"
   },
   "source": [
    "**PART 3.1 a,b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wGnW62YpJzW2"
   },
   "outputs": [],
   "source": [
    "# Use the same transformation on the test data\n",
    "\n",
    "transformed_pca_test = np.dot(x_test, pca_eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TMuAcfu3JzYI"
   },
   "outputs": [],
   "source": [
    "# Use the same transformation on the test data\n",
    "transformed_rbf_test = np.dot(rbf_kernel(x_test.values, x_train.values), rbf_eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NczXY3umJzZr"
   },
   "outputs": [],
   "source": [
    "# Use the same transformation on the test data\n",
    "transformed_poly_test = np.dot(polynomial_kernel(x_test.values, x_train.values), poly_eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "r6MkzCjIJzbG"
   },
   "outputs": [],
   "source": [
    "# Use the same transformation on the test data\n",
    "transformed_linear_test = np.dot(linear_kernel(x_test.values, x_train.values), linear_eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFYFswUgJ9Bu"
   },
   "source": [
    "**PART 3.2 a, b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14_ZK1IPUe8K",
    "outputId": "e2a64393-153d-4b0b-c6ec-5695d3f1f244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using pca : 0.6097560975609756\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_pca = myclassifier(transformed_pca_train, y_train.values, transformed_pca_test)\n",
    "\n",
    "# Calculate accuracy using calculate_accuracy function\n",
    "accuracy_pca = calculate_accuracy(y_test.values, predicted_labels_pca)\n",
    "print(f'Accuracy using pca :', accuracy_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "knOm8pAiJzef",
    "outputId": "5319f2ec-30df-483a-938b-7106ffff47d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using rbf_kcpa : 0.5609756097560976\n"
     ]
    }
   ],
   "source": [
    "# Now you can use transformed_rbf_train and transformed_rbf_test for classification\n",
    "rbf_predicted_labels = myclassifier(transformed_rbf_train, y_train.values, transformed_rbf_test)\n",
    "\n",
    "# Calculate accuracy using calculate_accuracy function\n",
    "accuracy_rbf_kpca = calculate_accuracy(y_test.values, rbf_predicted_labels)\n",
    "print(f'Accuracy using rbf_kcpa :', accuracy_rbf_kpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCZ-J0bWJzfk",
    "outputId": "0baf41fd-2ad9-488c-d2db-84d0c1be730c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using polynomial_kcpa of degree 3: 0.6097560975609756\n"
     ]
    }
   ],
   "source": [
    "# Now you can use transformed_poly_train and transformed_poly_test for classification\n",
    "predicted_labels_poly = myclassifier(transformed_poly_train, y_train.values, transformed_poly_test)\n",
    "\n",
    "# Calculate accuracy using calculate_accuracy function\n",
    "accuracy_poly_kpca = calculate_accuracy(y_test.values, predicted_labels_poly)\n",
    "print(f'Accuracy using polynomial_kcpa of degree {degree_poly}:', accuracy_poly_kpca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSQAmJHJJzg7",
    "outputId": "b127a94a-cb5a-4bff-96de-e6a596f4d7ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using linear_kcpa for : 0.6341463414634146\n"
     ]
    }
   ],
   "source": [
    "# Now you can use transformed_linear_train and transformed_test_kpca_linear for classification\n",
    "predicted_labels_linear = myclassifier(transformed_linear_train, y_train.values, transformed_linear_test)\n",
    "\n",
    "# Calculate accuracy using calculate_accuracy function\n",
    "accuracy_linear_kpca = calculate_accuracy(y_test.values, predicted_labels_linear)\n",
    "print(f'Accuracy using linear_kcpa for :', accuracy_linear_kpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_ELYe58YRXh"
   },
   "source": [
    "# Key Findings :\n",
    "\n",
    "## Comparing accuracies :\n",
    "\n",
    "- Accuracy using pca : 0.6097\n",
    "- Accuracy using KPCA with rbf kernel : 0.5365\n",
    "- Accuracy using KPCA with poly kernel : 0.4634\n",
    "- Accuracy using KPCA with linear kernel : 0.6097\n",
    "\n",
    "\n",
    "Accuracy obtained by using the reduced data with pca (without any kernel) and using linear kernel produced better results than compared to other methods. simple PCA and linear_kpca achieved an accuracy of 0.6097 While poly kernel performed poorly with 0.4634 accuracy\n",
    "\n",
    "Given train data has around 10 columns (features), but we have reduced the data to 5 principal components and yet we can achieve accuracies as high as 0.6097.\n",
    "\n",
    "## Impact on classification performance :\n",
    "- Although we have reduced train data using differnet kernels or without kernel into same number of principal components, there is high difference in the accuracy (classification accuracy)\n",
    " - The data whcih is reduced using PCA, and using KPCA with linear kernel performed better than rest of all and achieved an accuracy of 60.97%\n",
    " - While the data whcih is reduced using KPCA with rbf_kernel performed poorly and achieved an accuracy of 46.34%\n",
    " - So, the reduction technique that we used to reduce the data, greatly impacts the classification performance\n",
    "\n",
    "\n",
    "## Advantages and Disadvantages of using codes written from scratch over sklearn's functions :\n",
    "\n",
    "- Advantage is that if we implement codes form scractch, we can customize the code for our requirements. We can learn and apply these learnings in other areas as well, whcih we cannot do if we have directly used sklearns functions. And also in this case, which is a lower dimensional data, functions implemented from scratch performed faster than those of sklearn's functions.\n",
    "\n",
    "- Disadvantage is that, as the size and shape (dimensions) of the data increases, then sklearn's functions might perform well, and faster.\n",
    "- And also sklearn's inbuilt functions, comes with many optimisations for calculations whcih we are difficult to implement from scratch. Which will play crucial role in faster calculations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WGfxwVbYO4G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tNFerS5gDV_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
