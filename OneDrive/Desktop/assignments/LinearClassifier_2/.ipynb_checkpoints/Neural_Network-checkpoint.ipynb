{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zIjSNRfyBcUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Pytorch functions"
      ],
      "metadata": {
        "id": "JL8SEX0LBdXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple neural network with 2 fully connected layers\n",
        "\n",
        "\n",
        "# Load the CIFAR-10 dataset and transform it to tensors\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "-r1uaGosszRr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a9eed3-f49a-412b-d647-bf3ea2690df4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, 128)  # 3 channels, 32x32 image size\n",
        "        self.fc2 = nn.Linear(128, 10)  # Output layer with 10 classes (CIFAR-10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3 * 32 * 32)  # Flattening the input\n",
        "        x = F.relu(self.fc1(x))  # Applying ReLU activation to the first layer\n",
        "        x = self.fc2(x)  # Output layer\n",
        "        return x\n",
        "net = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(2):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero'ing the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM2FH-6t9LbG",
        "outputId": "25422137-62b0-46e8-8cf6-ba5d8668e55c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,  2000] loss: 2.094\n",
            "[1,  4000] loss: 1.896\n",
            "[1,  6000] loss: 1.809\n",
            "[1,  8000] loss: 1.755\n",
            "[1, 10000] loss: 1.711\n",
            "[1, 12000] loss: 1.709\n",
            "[2,  2000] loss: 1.652\n",
            "[2,  4000] loss: 1.628\n",
            "[2,  6000] loss: 1.628\n",
            "[2,  8000] loss: 1.607\n",
            "[2, 10000] loss: 1.589\n",
            "[2, 12000] loss: 1.588\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 44 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lxnh3n5EBh8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-coded version of 2 layer NN (Without Regularisation)"
      ],
      "metadata": {
        "id": "Z5bySjDUBiSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "def convert_to_numpy(data_loader):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for inputs, targets in data_loader:\n",
        "        images.append(inputs.numpy())\n",
        "        labels.append(targets.numpy())\n",
        "    images = np.vstack(images)\n",
        "    labels = np.hstack(labels)\n",
        "    return images, labels\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize each channel with mean 0.5 and std deviation 0.5\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "input_size = 32 * 32 * 3\n",
        "hidden_size = 128\n",
        "output_size = 10\n",
        "\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def forward(X, W1, b1, W2, b2):\n",
        "    # Input to hidden layer\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "\n",
        "    # Hidden to output layer\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = softmax(Z2)\n",
        "\n",
        "    return A1, A2\n",
        "\n",
        "#cross-entropy loss\n",
        "def compute_loss(Y, Y_pred):\n",
        "    m = Y.shape[0]\n",
        "    loss = -np.sum(Y * np.log(Y_pred)) / m\n",
        "    return loss\n",
        "\n",
        "def backward(X, Y, A1, A2, W1, W2, b1, b2, learning_rate):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Gradient of loss w.r.t. Z2\n",
        "    dZ2 = A2 - Y\n",
        "\n",
        "    # Gradient of loss w.r.t. W2 and b2\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Gradient of loss w.r.t. Z1\n",
        "    dZ1 = np.dot(dZ2, W2.T) * A1 * (1 - A1)\n",
        "\n",
        "    # Gradient of loss w.r.t. W1 and b1\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Updating weights and bias\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "train_images, train_labels = convert_to_numpy(trainloader)\n",
        "train_images = train_images.reshape(-1, 3072)\n",
        "test_images, test_labels = convert_to_numpy(testloader)\n",
        "test_images = test_images.reshape(-1,  3072)\n",
        "\n",
        "learning_rate = 0.001\n",
        "epochs = 30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    A1, A2 = forward(train_images, W1, b1, W2, b2)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = compute_loss(np.eye(output_size)[train_labels], A2)\n",
        "\n",
        "    # Backward pass\n",
        "    W1, b1, W2, b2 = backward(train_images, np.eye(output_size)[train_labels], A1, A2, W1, W2, b1, b2, learning_rate)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss}')\n",
        "\n",
        "_, test_pred = forward(test_images, W1, b1, W2, b2)\n",
        "predictions = np.argmax(test_pred, axis=1)\n",
        "accuracy = np.mean(predictions == test_labels) * 100\n",
        "print(f'Test Accuracy: {accuracy}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbRbeCSg9jc5",
        "outputId": "6b9a18b0-ec5a-48e0-b775-61264591245b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/30, Loss: 2.303537521439136\n",
            "Epoch 2/30, Loss: 2.303521147232998\n",
            "Epoch 3/30, Loss: 2.3035048255147474\n",
            "Epoch 4/30, Loss: 2.3034885559478173\n",
            "Epoch 5/30, Loss: 2.3034723381976905\n",
            "Epoch 6/30, Loss: 2.3034561719318947\n",
            "Epoch 7/30, Loss: 2.3034400568199875\n",
            "Epoch 8/30, Loss: 2.3034239925335442\n",
            "Epoch 9/30, Loss: 2.3034079787461477\n",
            "Epoch 10/30, Loss: 2.3033920151333716\n",
            "Epoch 11/30, Loss: 2.3033761013727747\n",
            "Epoch 12/30, Loss: 2.3033602371438815\n",
            "Epoch 13/30, Loss: 2.3033444221281782\n",
            "Epoch 14/30, Loss: 2.3033286560090964\n",
            "Epoch 15/30, Loss: 2.303312938472001\n",
            "Epoch 16/30, Loss: 2.30329726920418\n",
            "Epoch 17/30, Loss: 2.303281647894835\n",
            "Epoch 18/30, Loss: 2.3032660742350637\n",
            "Epoch 19/30, Loss: 2.3032505479178553\n",
            "Epoch 20/30, Loss: 2.3032350686380743\n",
            "Epoch 21/30, Loss: 2.3032196360924497\n",
            "Epoch 22/30, Loss: 2.303204249979569\n",
            "Epoch 23/30, Loss: 2.303188909999856\n",
            "Epoch 24/30, Loss: 2.303173615855576\n",
            "Epoch 25/30, Loss: 2.303158367250805\n",
            "Epoch 26/30, Loss: 2.3031431638914364\n",
            "Epoch 27/30, Loss: 2.303128005485159\n",
            "Epoch 28/30, Loss: 2.30311289174145\n",
            "Epoch 29/30, Loss: 2.303097822371568\n",
            "Epoch 30/30, Loss: 2.30308279708853\n",
            "Test Accuracy: 9.8%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4U47GuOuBmHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improved 2-Layer NN with hyperparameter tuning and L2 Regularisation"
      ],
      "metadata": {
        "id": "NeHoSTyRBmWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "def convert_to_numpy(data_loader):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for inputs, targets in data_loader:\n",
        "        images.append(inputs.numpy())\n",
        "        labels.append(targets.numpy())\n",
        "    images = np.vstack(images)\n",
        "    labels = np.hstack(labels)\n",
        "    return images, labels\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# Define the forward pass\n",
        "def forward(X, W1, b1, W2, b2):\n",
        "    # Input to hidden layer\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "\n",
        "    # Hidden to output layer\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = softmax(Z2)\n",
        "\n",
        "    return A1, A2\n",
        "\n",
        "def compute_loss(Y, Y_pred):\n",
        "    m = Y.shape[0]\n",
        "    loss = -np.sum(Y * np.log(Y_pred)) / m\n",
        "    return loss\n",
        "\n",
        "# backward pass with L2 regularization\n",
        "def backward_with_regularization(X, Y, A1, A2, W1, W2, b1, b2, learning_rate, reg_coeff):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Gradient of loss w.r.t. Z2\n",
        "    dZ2 = A2 - Y\n",
        "\n",
        "    # Gradient of loss w.r.t. W2 and b2 with L2 regularization\n",
        "    dW2 = (np.dot(A1.T, dZ2) + reg_coeff * W2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Gradient of loss w.r.t. Z1\n",
        "    dZ1 = np.dot(dZ2, W2.T) * A1 * (1 - A1)\n",
        "\n",
        "    # Gradient of loss w.r.t. W1 and b1 with L2 regularization\n",
        "    dW1 = (np.dot(X.T, dZ1) + reg_coeff * W1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Updating weights and biases\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "\n",
        "def convert_to_numpy(data_loader):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for inputs, targets in data_loader:\n",
        "        images.append(inputs.numpy())\n",
        "        labels.append(targets.numpy())\n",
        "    images = np.vstack(images)\n",
        "    labels = np.hstack(labels)\n",
        "    return images, labels\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize each channel with mean 0.5 and std deviation 0.5\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "input_size = 32 * 32 * 3\n",
        "output_size = 10\n",
        "\n",
        "train_images, train_labels = convert_to_numpy(trainloader)\n",
        "train_images = train_images.reshape(-1, 3072)\n",
        "test_images, test_labels = convert_to_numpy(testloader)\n",
        "test_images = test_images.reshape(-1,  3072)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujzkcJpd_Dos",
        "outputId": "0fde48a8-b1f1-43fa-f296-fed2a57722a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_sizes = [64, 128]\n",
        "regularization_coeffs = [0.00001, 0.0001, 0.001]\n",
        "learning_rates = [1, 1.1, 1.25]\n",
        "epochs_list = [30, 50]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "# Training loop with L2 regularization\n",
        "for hidden_size in hidden_sizes:\n",
        "    for reg_coeff in regularization_coeffs:\n",
        "        for learning_rate in learning_rates:\n",
        "            for epochs in epochs_list:\n",
        "                print(f\"Training with hidden_size={hidden_size}, reg_coeff={reg_coeff}, learning_rate={learning_rate}, epochs={epochs}...\")\n",
        "\n",
        "                # Initialize weights and biases\n",
        "                np.random.seed(42)\n",
        "                W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "                b1 = np.zeros((1, hidden_size))\n",
        "                W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "                b2 = np.zeros((1, output_size))\n",
        "\n",
        "                # Training loop\n",
        "                for epoch in range(epochs):\n",
        "                    # Forward pass\n",
        "                    A1, A2 = forward(train_images, W1, b1, W2, b2)\n",
        "\n",
        "                    # Compute train loss with L2 regularization\n",
        "                    train_loss = compute_loss(np.eye(output_size)[train_labels], A2) + (reg_coeff / (2 * train_images.shape[0])) * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
        "\n",
        "                    # Backward pass with L2 regularization\n",
        "                    W1, b1, W2, b2 = backward_with_regularization(train_images, np.eye(output_size)[train_labels], A1, A2, W1, W2, b1, b2, learning_rate, reg_coeff)\n",
        "\n",
        "                    # Print train loss every epoch\n",
        "                    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss}')\n",
        "\n",
        "                # Validate after training\n",
        "                _, test_pred = forward(test_images, W1, b1, W2, b2)\n",
        "                predictions = np.argmax(test_pred, axis=1)\n",
        "                accuracy = np.mean(predictions == test_labels) * 100\n",
        "                print(f'Test Accuracy: {accuracy}%')\n",
        "\n",
        "                # Update best accuracy and parameters if current accuracy is better\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_params = {\n",
        "                        'hidden_size': hidden_size,\n",
        "                        'reg_coeff': reg_coeff,\n",
        "                        'learning_rate': learning_rate,\n",
        "                        'epochs': epochs\n",
        "                    }\n",
        "                    print(\"Best parameters updated:\", best_params)\n",
        "\n",
        "                print(\"Training complete.\\n\")\n",
        "\n",
        "print(\"Grid search complete.\")\n",
        "print(\"Best parameters found:\", best_params)\n",
        "print(\"Best test accuracy:\", best_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKhY8uCvE82J",
        "outputId": "346d9287-780b-4d5a-d244-90bfb248dce2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with hidden_size=64, reg_coeff=1e-05, learning_rate=1, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.304810586826439\n",
            "Epoch 2/30, Train Loss: 2.3000220413287753\n",
            "Epoch 3/30, Train Loss: 2.2947722977383385\n",
            "Epoch 4/30, Train Loss: 2.2866488938876315\n",
            "Epoch 5/30, Train Loss: 2.2734199165548414\n",
            "Epoch 6/30, Train Loss: 2.2538450530469896\n",
            "Epoch 7/30, Train Loss: 2.22910617478603\n",
            "Epoch 8/30, Train Loss: 2.202659833616347\n",
            "Epoch 9/30, Train Loss: 2.177582726316821\n",
            "Epoch 10/30, Train Loss: 2.1547538310636045\n",
            "Epoch 11/30, Train Loss: 2.1336818120452996\n",
            "Epoch 12/30, Train Loss: 2.113937271507474\n",
            "Epoch 13/30, Train Loss: 2.0955154674667194\n",
            "Epoch 14/30, Train Loss: 2.078528026635388\n",
            "Epoch 15/30, Train Loss: 2.062952525177478\n",
            "Epoch 16/30, Train Loss: 2.048684355215331\n",
            "Epoch 17/30, Train Loss: 2.035612911338649\n",
            "Epoch 18/30, Train Loss: 2.023636278933894\n",
            "Epoch 19/30, Train Loss: 2.0126518353737093\n",
            "Epoch 20/30, Train Loss: 2.0025548329115965\n",
            "Epoch 21/30, Train Loss: 1.9932405465982548\n",
            "Epoch 22/30, Train Loss: 1.9846099412298788\n",
            "Epoch 23/30, Train Loss: 1.9765736624701324\n",
            "Epoch 24/30, Train Loss: 1.9690548040452398\n",
            "Epoch 25/30, Train Loss: 1.9619891302689219\n",
            "Epoch 26/30, Train Loss: 1.955324094103499\n",
            "Epoch 27/30, Train Loss: 1.9490167438423796\n",
            "Epoch 28/30, Train Loss: 1.9430314957713064\n",
            "Epoch 29/30, Train Loss: 1.93733792399077\n",
            "Epoch 30/30, Train Loss: 1.9319090018704597\n",
            "Test Accuracy: 31.369999999999997%\n",
            "Best parameters updated: {'hidden_size': 64, 'reg_coeff': 1e-05, 'learning_rate': 1, 'epochs': 30}\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=1e-05, learning_rate=1, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.304810586826439\n",
            "Epoch 2/50, Train Loss: 2.3000220413287753\n",
            "Epoch 3/50, Train Loss: 2.2947722977383385\n",
            "Epoch 4/50, Train Loss: 2.2866488938876315\n",
            "Epoch 5/50, Train Loss: 2.2734199165548414\n",
            "Epoch 6/50, Train Loss: 2.2538450530469896\n",
            "Epoch 7/50, Train Loss: 2.22910617478603\n",
            "Epoch 8/50, Train Loss: 2.202659833616347\n",
            "Epoch 9/50, Train Loss: 2.177582726316821\n",
            "Epoch 10/50, Train Loss: 2.1547538310636045\n",
            "Epoch 11/50, Train Loss: 2.1336818120452996\n",
            "Epoch 12/50, Train Loss: 2.113937271507474\n",
            "Epoch 13/50, Train Loss: 2.0955154674667194\n",
            "Epoch 14/50, Train Loss: 2.078528026635388\n",
            "Epoch 15/50, Train Loss: 2.062952525177478\n",
            "Epoch 16/50, Train Loss: 2.048684355215331\n",
            "Epoch 17/50, Train Loss: 2.035612911338649\n",
            "Epoch 18/50, Train Loss: 2.023636278933894\n",
            "Epoch 19/50, Train Loss: 2.0126518353737093\n",
            "Epoch 20/50, Train Loss: 2.0025548329115965\n",
            "Epoch 21/50, Train Loss: 1.9932405465982548\n",
            "Epoch 22/50, Train Loss: 1.9846099412298788\n",
            "Epoch 23/50, Train Loss: 1.9765736624701324\n",
            "Epoch 24/50, Train Loss: 1.9690548040452398\n",
            "Epoch 25/50, Train Loss: 1.9619891302689219\n",
            "Epoch 26/50, Train Loss: 1.955324094103499\n",
            "Epoch 27/50, Train Loss: 1.9490167438423796\n",
            "Epoch 28/50, Train Loss: 1.9430314957713064\n",
            "Epoch 29/50, Train Loss: 1.93733792399077\n",
            "Epoch 30/50, Train Loss: 1.9319090018704597\n",
            "Epoch 31/50, Train Loss: 1.9267197868498727\n",
            "Epoch 32/50, Train Loss: 1.921746667145242\n",
            "Epoch 33/50, Train Loss: 1.9169670250277564\n",
            "Epoch 34/50, Train Loss: 1.9123592838023504\n",
            "Epoch 35/50, Train Loss: 1.907903113346449\n",
            "Epoch 36/50, Train Loss: 1.9035797636021914\n",
            "Epoch 37/50, Train Loss: 1.8993723140088186\n",
            "Epoch 38/50, Train Loss: 1.8952659148323607\n",
            "Epoch 39/50, Train Loss: 1.8912478412047535\n",
            "Epoch 40/50, Train Loss: 1.8873075425015189\n",
            "Epoch 41/50, Train Loss: 1.8834364937853363\n",
            "Epoch 42/50, Train Loss: 1.8796281069355847\n",
            "Epoch 43/50, Train Loss: 1.8758774536441631\n",
            "Epoch 44/50, Train Loss: 1.8721811218054512\n",
            "Epoch 45/50, Train Loss: 1.8685368820287407\n",
            "Epoch 46/50, Train Loss: 1.8649435579749496\n",
            "Epoch 47/50, Train Loss: 1.8614006830500154\n",
            "Epoch 48/50, Train Loss: 1.8579084335375715\n",
            "Epoch 49/50, Train Loss: 1.8544672937682036\n",
            "Epoch 50/50, Train Loss: 1.8510780885924385\n",
            "Test Accuracy: 34.849999999999994%\n",
            "Best parameters updated: {'hidden_size': 64, 'reg_coeff': 1e-05, 'learning_rate': 1, 'epochs': 50}\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=1e-05, learning_rate=1.1, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.304810586826439\n",
            "Epoch 2/30, Train Loss: 2.2999630980371544\n",
            "Epoch 3/30, Train Loss: 2.2941145228509505\n",
            "Epoch 4/30, Train Loss: 2.284317225724224\n",
            "Epoch 5/30, Train Loss: 2.267877479387482\n",
            "Epoch 6/30, Train Loss: 2.2439748397575716\n",
            "Epoch 7/30, Train Loss: 2.2154067887184854\n",
            "Epoch 8/30, Train Loss: 2.186936488194078\n",
            "Epoch 9/30, Train Loss: 2.161025294272193\n",
            "Epoch 10/30, Train Loss: 2.1374831214013432\n",
            "Epoch 11/30, Train Loss: 2.1155700566852156\n",
            "Epoch 12/30, Train Loss: 2.095251284256988\n",
            "Epoch 13/30, Train Loss: 2.076648247668937\n",
            "Epoch 14/30, Train Loss: 2.059758084227957\n",
            "Epoch 15/30, Train Loss: 2.044409956531588\n",
            "Epoch 16/30, Train Loss: 2.0304813570098235\n",
            "Epoch 17/30, Train Loss: 2.0178216845345025\n",
            "Epoch 18/30, Train Loss: 2.0063070806827334\n",
            "Epoch 19/30, Train Loss: 1.9957858223045024\n",
            "Epoch 20/30, Train Loss: 1.9861324126902657\n",
            "Epoch 21/30, Train Loss: 1.9772161267314354\n",
            "Epoch 22/30, Train Loss: 1.968938817046288\n",
            "Epoch 23/30, Train Loss: 1.9612098360930703\n",
            "Epoch 24/30, Train Loss: 1.9539656096325846\n",
            "Epoch 25/30, Train Loss: 1.947150561732949\n",
            "Epoch 26/30, Train Loss: 1.9407272443959531\n",
            "Epoch 27/30, Train Loss: 1.9346630525292872\n",
            "Epoch 28/30, Train Loss: 1.9289433936817422\n",
            "Epoch 29/30, Train Loss: 1.9235548228259018\n",
            "Epoch 30/30, Train Loss: 1.9185320594979534\n",
            "Test Accuracy: 31.619999999999997%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=1e-05, learning_rate=1.1, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.304810586826439\n",
            "Epoch 2/50, Train Loss: 2.2999630980371544\n",
            "Epoch 3/50, Train Loss: 2.2941145228509505\n",
            "Epoch 4/50, Train Loss: 2.284317225724224\n",
            "Epoch 5/50, Train Loss: 2.267877479387482\n",
            "Epoch 6/50, Train Loss: 2.2439748397575716\n",
            "Epoch 7/50, Train Loss: 2.2154067887184854\n",
            "Epoch 8/50, Train Loss: 2.186936488194078\n",
            "Epoch 9/50, Train Loss: 2.161025294272193\n",
            "Epoch 10/50, Train Loss: 2.1374831214013432\n",
            "Epoch 11/50, Train Loss: 2.1155700566852156\n",
            "Epoch 12/50, Train Loss: 2.095251284256988\n",
            "Epoch 13/50, Train Loss: 2.076648247668937\n",
            "Epoch 14/50, Train Loss: 2.059758084227957\n",
            "Epoch 15/50, Train Loss: 2.044409956531588\n",
            "Epoch 16/50, Train Loss: 2.0304813570098235\n",
            "Epoch 17/50, Train Loss: 2.0178216845345025\n",
            "Epoch 18/50, Train Loss: 2.0063070806827334\n",
            "Epoch 19/50, Train Loss: 1.9957858223045024\n",
            "Epoch 20/50, Train Loss: 1.9861324126902657\n",
            "Epoch 21/50, Train Loss: 1.9772161267314354\n",
            "Epoch 22/50, Train Loss: 1.968938817046288\n",
            "Epoch 23/50, Train Loss: 1.9612098360930703\n",
            "Epoch 24/50, Train Loss: 1.9539656096325846\n",
            "Epoch 25/50, Train Loss: 1.947150561732949\n",
            "Epoch 26/50, Train Loss: 1.9407272443959531\n",
            "Epoch 27/50, Train Loss: 1.9346630525292872\n",
            "Epoch 28/50, Train Loss: 1.9289433936817422\n",
            "Epoch 29/50, Train Loss: 1.9235548228259018\n",
            "Epoch 30/50, Train Loss: 1.9185320594979534\n",
            "Epoch 31/50, Train Loss: 1.9138924250762261\n",
            "Epoch 32/50, Train Loss: 1.909843225986066\n",
            "Epoch 33/50, Train Loss: 1.9064340768908987\n",
            "Epoch 34/50, Train Loss: 1.9045193724597986\n",
            "Epoch 35/50, Train Loss: 1.9038356797061182\n",
            "Epoch 36/50, Train Loss: 1.907523312600093\n",
            "Epoch 37/50, Train Loss: 1.9117679918645876\n",
            "Epoch 38/50, Train Loss: 1.9263840119055122\n",
            "Epoch 39/50, Train Loss: 1.9297028691020184\n",
            "Epoch 40/50, Train Loss: 1.9500298500386217\n",
            "Epoch 41/50, Train Loss: 1.9363747611130389\n",
            "Epoch 42/50, Train Loss: 1.948572894610693\n",
            "Epoch 43/50, Train Loss: 1.926899953658083\n",
            "Epoch 44/50, Train Loss: 1.9318451032006763\n",
            "Epoch 45/50, Train Loss: 1.9110361904162854\n",
            "Epoch 46/50, Train Loss: 1.9129788324482677\n",
            "Epoch 47/50, Train Loss: 1.8948212091925978\n",
            "Epoch 48/50, Train Loss: 1.8960176642672208\n",
            "Epoch 49/50, Train Loss: 1.8806559460237502\n",
            "Epoch 50/50, Train Loss: 1.882060325078969\n",
            "Test Accuracy: 33.08%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=1e-05, learning_rate=1.25, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.304810586826439\n",
            "Epoch 2/30, Train Loss: 2.300016161860373\n",
            "Epoch 3/30, Train Loss: 2.2936987901803643\n",
            "Epoch 4/30, Train Loss: 2.2818019830895704\n",
            "Epoch 5/30, Train Loss: 2.2608692153856493\n",
            "Epoch 6/30, Train Loss: 2.231736874441696\n",
            "Epoch 7/30, Train Loss: 2.1999061740720305\n",
            "Epoch 8/30, Train Loss: 2.1709604822231583\n",
            "Epoch 9/30, Train Loss: 2.1448184678953446\n",
            "Epoch 10/30, Train Loss: 2.121209974525079\n",
            "Epoch 11/30, Train Loss: 2.0995232740576593\n",
            "Epoch 12/30, Train Loss: 2.0813767556410103\n",
            "Epoch 13/30, Train Loss: 2.064834190970018\n",
            "Epoch 14/30, Train Loss: 2.0533412958143535\n",
            "Epoch 15/30, Train Loss: 2.040385976388644\n",
            "Epoch 16/30, Train Loss: 2.0359549585791643\n",
            "Epoch 17/30, Train Loss: 2.0241100672533605\n",
            "Epoch 18/30, Train Loss: 2.026077970080524\n",
            "Epoch 19/30, Train Loss: 2.016100284353738\n",
            "Epoch 20/30, Train Loss: 2.0242892228965608\n",
            "Epoch 21/30, Train Loss: 2.023506454068416\n",
            "Epoch 22/30, Train Loss: 2.038453554631165\n",
            "Epoch 23/30, Train Loss: 2.048968169742595\n",
            "Epoch 24/30, Train Loss: 2.064919236356172\n",
            "Epoch 25/30, Train Loss: 2.0493299470824393\n",
            "Epoch 26/30, Train Loss: 2.055876794102878\n",
            "Epoch 27/30, Train Loss: 2.018261145771551\n",
            "Epoch 28/30, Train Loss: 2.0191385608664385\n",
            "Epoch 29/30, Train Loss: 1.9880638503365562\n",
            "Epoch 30/30, Train Loss: 1.9894278108645662\n",
            "Test Accuracy: 28.79%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=1e-05, learning_rate=1.25, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.304810586826439\n",
            "Epoch 2/50, Train Loss: 2.300016161860373\n",
            "Epoch 3/50, Train Loss: 2.2936987901803643\n",
            "Epoch 4/50, Train Loss: 2.2818019830895704\n",
            "Epoch 5/50, Train Loss: 2.2608692153856493\n",
            "Epoch 6/50, Train Loss: 2.231736874441696\n",
            "Epoch 7/50, Train Loss: 2.1999061740720305\n",
            "Epoch 8/50, Train Loss: 2.1709604822231583\n",
            "Epoch 9/50, Train Loss: 2.1448184678953446\n",
            "Epoch 10/50, Train Loss: 2.121209974525079\n",
            "Epoch 11/50, Train Loss: 2.0995232740576593\n",
            "Epoch 12/50, Train Loss: 2.0813767556410103\n",
            "Epoch 13/50, Train Loss: 2.064834190970018\n",
            "Epoch 14/50, Train Loss: 2.0533412958143535\n",
            "Epoch 15/50, Train Loss: 2.040385976388644\n",
            "Epoch 16/50, Train Loss: 2.0359549585791643\n",
            "Epoch 17/50, Train Loss: 2.0241100672533605\n",
            "Epoch 18/50, Train Loss: 2.026077970080524\n",
            "Epoch 19/50, Train Loss: 2.016100284353738\n",
            "Epoch 20/50, Train Loss: 2.0242892228965608\n",
            "Epoch 21/50, Train Loss: 2.023506454068416\n",
            "Epoch 22/50, Train Loss: 2.038453554631165\n",
            "Epoch 23/50, Train Loss: 2.048968169742595\n",
            "Epoch 24/50, Train Loss: 2.064919236356172\n",
            "Epoch 25/50, Train Loss: 2.0493299470824393\n",
            "Epoch 26/50, Train Loss: 2.055876794102878\n",
            "Epoch 27/50, Train Loss: 2.018261145771551\n",
            "Epoch 28/50, Train Loss: 2.0191385608664385\n",
            "Epoch 29/50, Train Loss: 1.9880638503365562\n",
            "Epoch 30/50, Train Loss: 1.9894278108645662\n",
            "Epoch 31/50, Train Loss: 1.9631978767340563\n",
            "Epoch 32/50, Train Loss: 1.9671022309328536\n",
            "Epoch 33/50, Train Loss: 1.94371026233862\n",
            "Epoch 34/50, Train Loss: 1.9510109416140986\n",
            "Epoch 35/50, Train Loss: 1.928320228101038\n",
            "Epoch 36/50, Train Loss: 1.9394339041104716\n",
            "Epoch 37/50, Train Loss: 1.9158668556255587\n",
            "Epoch 38/50, Train Loss: 1.9309078199573964\n",
            "Epoch 39/50, Train Loss: 1.9055919069821952\n",
            "Epoch 40/50, Train Loss: 1.924273174753135\n",
            "Epoch 41/50, Train Loss: 1.8972551546160956\n",
            "Epoch 42/50, Train Loss: 1.9190588921918375\n",
            "Epoch 43/50, Train Loss: 1.891650489935059\n",
            "Epoch 44/50, Train Loss: 1.9163629922985603\n",
            "Epoch 45/50, Train Loss: 1.892494957525372\n",
            "Epoch 46/50, Train Loss: 1.9202329698272782\n",
            "Epoch 47/50, Train Loss: 1.909260194254664\n",
            "Epoch 48/50, Train Loss: 1.9345186817854116\n",
            "Epoch 49/50, Train Loss: 1.9373622592491835\n",
            "Epoch 50/50, Train Loss: 1.9322712784651819\n",
            "Test Accuracy: 30.95%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=0.0001, learning_rate=1, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.304810604568723\n",
            "Epoch 2/30, Train Loss: 2.3000220590776874\n",
            "Epoch 3/30, Train Loss: 2.294772315534101\n",
            "Epoch 4/30, Train Loss: 2.2866489117885416\n",
            "Epoch 5/30, Train Loss: 2.2734199346519026\n",
            "Epoch 6/30, Train Loss: 2.253845071447385\n",
            "Epoch 7/30, Train Loss: 2.2291061935653826\n",
            "Epoch 8/30, Train Loss: 2.202659852792674\n",
            "Epoch 9/30, Train Loss: 2.1775827458867423\n",
            "Epoch 10/30, Train Loss: 2.154753851042972\n",
            "Epoch 11/30, Train Loss: 2.1336818324656583\n",
            "Epoch 12/30, Train Loss: 2.113937292393764\n",
            "Epoch 13/30, Train Loss: 2.0955154888268597\n",
            "Epoch 14/30, Train Loss: 2.078528048466454\n",
            "Epoch 15/30, Train Loss: 2.0629525474735084\n",
            "Epoch 16/30, Train Loss: 2.048684377968546\n",
            "Epoch 17/30, Train Loss: 2.0356129345392056\n",
            "Epoch 18/30, Train Loss: 2.0236363025707567\n",
            "Epoch 19/30, Train Loss: 2.012651859435794\n",
            "Epoch 20/30, Train Loss: 2.0025548573886467\n",
            "Epoch 21/30, Train Loss: 1.993240571481283\n",
            "Epoch 22/30, Train Loss: 1.9846099665112649\n",
            "Epoch 23/30, Train Loss: 1.976573688143473\n",
            "Epoch 24/30, Train Loss: 1.9690548301051145\n",
            "Epoch 25/30, Train Loss: 1.961989156710636\n",
            "Epoch 26/30, Train Loss: 1.9553241209228847\n",
            "Epoch 27/30, Train Loss: 1.9490167710356663\n",
            "Epoch 28/30, Train Loss: 1.9430315233350752\n",
            "Epoch 29/30, Train Loss: 1.9373379519219907\n",
            "Epoch 30/30, Train Loss: 1.9319090301665833\n",
            "Test Accuracy: 31.369999999999997%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=0.0001, learning_rate=1, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.304810604568723\n",
            "Epoch 2/50, Train Loss: 2.3000220590776874\n",
            "Epoch 3/50, Train Loss: 2.294772315534101\n",
            "Epoch 4/50, Train Loss: 2.2866489117885416\n",
            "Epoch 5/50, Train Loss: 2.2734199346519026\n",
            "Epoch 6/50, Train Loss: 2.253845071447385\n",
            "Epoch 7/50, Train Loss: 2.2291061935653826\n",
            "Epoch 8/50, Train Loss: 2.202659852792674\n",
            "Epoch 9/50, Train Loss: 2.1775827458867423\n",
            "Epoch 10/50, Train Loss: 2.154753851042972\n",
            "Epoch 11/50, Train Loss: 2.1336818324656583\n",
            "Epoch 12/50, Train Loss: 2.113937292393764\n",
            "Epoch 13/50, Train Loss: 2.0955154888268597\n",
            "Epoch 14/50, Train Loss: 2.078528048466454\n",
            "Epoch 15/50, Train Loss: 2.0629525474735084\n",
            "Epoch 16/50, Train Loss: 2.048684377968546\n",
            "Epoch 17/50, Train Loss: 2.0356129345392056\n",
            "Epoch 18/50, Train Loss: 2.0236363025707567\n",
            "Epoch 19/50, Train Loss: 2.012651859435794\n",
            "Epoch 20/50, Train Loss: 2.0025548573886467\n",
            "Epoch 21/50, Train Loss: 1.993240571481283\n",
            "Epoch 22/50, Train Loss: 1.9846099665112649\n",
            "Epoch 23/50, Train Loss: 1.976573688143473\n",
            "Epoch 24/50, Train Loss: 1.9690548301051145\n",
            "Epoch 25/50, Train Loss: 1.961989156710636\n",
            "Epoch 26/50, Train Loss: 1.9553241209228847\n",
            "Epoch 27/50, Train Loss: 1.9490167710356663\n",
            "Epoch 28/50, Train Loss: 1.9430315233350752\n",
            "Epoch 29/50, Train Loss: 1.9373379519219907\n",
            "Epoch 30/50, Train Loss: 1.9319090301665833\n",
            "Epoch 31/50, Train Loss: 1.9267198155089673\n",
            "Epoch 32/50, Train Loss: 1.9217466961661211\n",
            "Epoch 33/50, Train Loss: 1.9169670544100723\n",
            "Epoch 34/50, Train Loss: 1.9123593135466146\n",
            "Epoch 35/50, Train Loss: 1.9079031434539904\n",
            "Epoch 36/50, Train Loss: 1.903579794075039\n",
            "Epoch 37/50, Train Loss: 1.8993723448495476\n",
            "Epoch 38/50, Train Loss: 1.8952659460438892\n",
            "Epoch 39/50, Train Loss: 1.891247872790158\n",
            "Epoch 40/50, Train Loss: 1.8873075744638401\n",
            "Epoch 41/50, Train Loss: 1.88343652612742\n",
            "Epoch 42/50, Train Loss: 1.8796281396599497\n",
            "Epoch 43/50, Train Loss: 1.8758774867529233\n",
            "Epoch 44/50, Train Loss: 1.8721811553002492\n",
            "Epoch 45/50, Train Loss: 1.8685369159107523\n",
            "Epoch 46/50, Train Loss: 1.8649435922448827\n",
            "Epoch 47/50, Train Loss: 1.8614007177081688\n",
            "Epoch 48/50, Train Loss: 1.8579084685838638\n",
            "Epoch 49/50, Train Loss: 1.8544673292022598\n",
            "Epoch 50/50, Train Loss: 1.8510781244136112\n",
            "Test Accuracy: 34.849999999999994%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=0.0001, learning_rate=1.1, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.304810604568723\n",
            "Epoch 2/30, Train Loss: 2.2999631157891915\n",
            "Epoch 3/30, Train Loss: 2.2941145406627346\n",
            "Epoch 4/30, Train Loss: 2.284317243669575\n",
            "Epoch 5/30, Train Loss: 2.2678774975808627\n",
            "Epoch 6/30, Train Loss: 2.243974858320088\n",
            "Epoch 7/30, Train Loss: 2.215406807711811\n",
            "Epoch 8/30, Train Loss: 2.1869365076191927\n",
            "Epoch 9/30, Train Loss: 2.1610253141363316\n",
            "Epoch 10/30, Train Loss: 2.1374831417394926\n",
            "Epoch 11/30, Train Loss: 2.1155700775312836\n",
            "Epoch 12/30, Train Loss: 2.0952513056229067\n",
            "Epoch 13/30, Train Loss: 2.076648269551687\n",
            "Epoch 14/30, Train Loss: 2.059758106620515\n",
            "Epoch 15/30, Train Loss: 2.0444099794243202\n",
            "Epoch 16/30, Train Loss: 2.03048138039048\n",
            "Epoch 17/30, Train Loss: 2.0178217083895973\n",
            "Epoch 18/30, Train Loss: 2.006307104999299\n",
            "Epoch 19/30, Train Loss: 1.9957858470709169\n",
            "Epoch 20/30, Train Loss: 1.9861324378967864\n",
            "Epoch 21/30, Train Loss: 1.9772161523698826\n",
            "Epoch 22/30, Train Loss: 1.9689388431099262\n",
            "Epoch 23/30, Train Loss: 1.9612098625759684\n",
            "Epoch 24/30, Train Loss: 1.9539656365294769\n",
            "Epoch 25/30, Train Loss: 1.9471505890387344\n",
            "Epoch 26/30, Train Loss: 1.9407272721055084\n",
            "Epoch 27/30, Train Loss: 1.934663080637136\n",
            "Epoch 28/30, Train Loss: 1.928943422180826\n",
            "Epoch 29/30, Train Loss: 1.923554851707375\n",
            "Epoch 30/30, Train Loss: 1.9185320887444117\n",
            "Test Accuracy: 31.619999999999997%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=0.0001, learning_rate=1.1, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.304810604568723\n",
            "Epoch 2/50, Train Loss: 2.2999631157891915\n",
            "Epoch 3/50, Train Loss: 2.2941145406627346\n",
            "Epoch 4/50, Train Loss: 2.284317243669575\n",
            "Epoch 5/50, Train Loss: 2.2678774975808627\n",
            "Epoch 6/50, Train Loss: 2.243974858320088\n",
            "Epoch 7/50, Train Loss: 2.215406807711811\n",
            "Epoch 8/50, Train Loss: 2.1869365076191927\n",
            "Epoch 9/50, Train Loss: 2.1610253141363316\n",
            "Epoch 10/50, Train Loss: 2.1374831417394926\n",
            "Epoch 11/50, Train Loss: 2.1155700775312836\n",
            "Epoch 12/50, Train Loss: 2.0952513056229067\n",
            "Epoch 13/50, Train Loss: 2.076648269551687\n",
            "Epoch 14/50, Train Loss: 2.059758106620515\n",
            "Epoch 15/50, Train Loss: 2.0444099794243202\n",
            "Epoch 16/50, Train Loss: 2.03048138039048\n",
            "Epoch 17/50, Train Loss: 2.0178217083895973\n",
            "Epoch 18/50, Train Loss: 2.006307104999299\n",
            "Epoch 19/50, Train Loss: 1.9957858470709169\n",
            "Epoch 20/50, Train Loss: 1.9861324378967864\n",
            "Epoch 21/50, Train Loss: 1.9772161523698826\n",
            "Epoch 22/50, Train Loss: 1.9689388431099262\n",
            "Epoch 23/50, Train Loss: 1.9612098625759684\n",
            "Epoch 24/50, Train Loss: 1.9539656365294769\n",
            "Epoch 25/50, Train Loss: 1.9471505890387344\n",
            "Epoch 26/50, Train Loss: 1.9407272721055084\n",
            "Epoch 27/50, Train Loss: 1.934663080637136\n",
            "Epoch 28/50, Train Loss: 1.928943422180826\n",
            "Epoch 29/50, Train Loss: 1.923554851707375\n",
            "Epoch 30/50, Train Loss: 1.9185320887444117\n",
            "Epoch 31/50, Train Loss: 1.913892454663723\n",
            "Epoch 32/50, Train Loss: 1.9098432558510066\n",
            "Epoch 33/50, Train Loss: 1.9064341069586068\n",
            "Epoch 34/50, Train Loss: 1.9045194024851735\n",
            "Epoch 35/50, Train Loss: 1.903835709534711\n",
            "Epoch 36/50, Train Loss: 1.90752334144038\n",
            "Epoch 37/50, Train Loss: 1.911768020124837\n",
            "Epoch 38/50, Train Loss: 1.9263840380877197\n",
            "Epoch 39/50, Train Loss: 1.929702897145931\n",
            "Epoch 40/50, Train Loss: 1.9500298778815253\n",
            "Epoch 41/50, Train Loss: 1.9363747932716868\n",
            "Epoch 42/50, Train Loss: 1.9485729279808144\n",
            "Epoch 43/50, Train Loss: 1.9268999887970775\n",
            "Epoch 44/50, Train Loss: 1.9318451388165485\n",
            "Epoch 45/50, Train Loss: 1.9110362270596986\n",
            "Epoch 46/50, Train Loss: 1.9129788690893665\n",
            "Epoch 47/50, Train Loss: 1.894821246622497\n",
            "Epoch 48/50, Train Loss: 1.8960177014931179\n",
            "Epoch 49/50, Train Loss: 1.8806559840469697\n",
            "Epoch 50/50, Train Loss: 1.8820603627504577\n",
            "Test Accuracy: 33.08%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=0.0001, learning_rate=1.25, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.304810604568723\n",
            "Epoch 2/30, Train Loss: 2.300016179617922\n",
            "Epoch 3/30, Train Loss: 2.293698808023474\n",
            "Epoch 4/30, Train Loss: 2.281802001127617\n",
            "Epoch 5/30, Train Loss: 2.260869233778992\n",
            "Epoch 6/30, Train Loss: 2.2317368933296384\n",
            "Epoch 7/30, Train Loss: 2.1999061934923296\n",
            "Epoch 8/30, Train Loss: 2.170960502181082\n",
            "Epoch 9/30, Train Loss: 2.144818488430943\n",
            "Epoch 10/30, Train Loss: 2.1212099956737576\n",
            "Epoch 11/30, Train Loss: 2.0995232958398025\n",
            "Epoch 12/30, Train Loss: 2.0813767780453154\n",
            "Epoch 13/30, Train Loss: 2.0648342139887763\n",
            "Epoch 14/30, Train Loss: 2.0533413194622225\n",
            "Epoch 15/30, Train Loss: 2.0403860005462247\n",
            "Epoch 16/30, Train Loss: 2.035954983329707\n",
            "Epoch 17/30, Train Loss: 2.0241100922449733\n",
            "Epoch 18/30, Train Loss: 2.0260779954551436\n",
            "Epoch 19/30, Train Loss: 2.016100309538973\n",
            "Epoch 20/30, Train Loss: 2.0242892480041323\n",
            "Epoch 21/30, Train Loss: 2.0235064781808165\n",
            "Epoch 22/30, Train Loss: 2.0384535780445003\n",
            "Epoch 23/30, Train Loss: 2.0489681928494994\n",
            "Epoch 24/30, Train Loss: 2.0649192590949426\n",
            "Epoch 25/30, Train Loss: 2.0493299751308127\n",
            "Epoch 26/30, Train Loss: 2.0558768233446876\n",
            "Epoch 27/30, Train Loss: 2.018261176173477\n",
            "Epoch 28/30, Train Loss: 2.0191385919098264\n",
            "Epoch 29/30, Train Loss: 1.9880638821544385\n",
            "Epoch 30/30, Train Loss: 1.989427842964193\n",
            "Test Accuracy: 28.79%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=0.0001, learning_rate=1.25, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.304810604568723\n",
            "Epoch 2/50, Train Loss: 2.300016179617922\n",
            "Epoch 3/50, Train Loss: 2.293698808023474\n",
            "Epoch 4/50, Train Loss: 2.281802001127617\n",
            "Epoch 5/50, Train Loss: 2.260869233778992\n",
            "Epoch 6/50, Train Loss: 2.2317368933296384\n",
            "Epoch 7/50, Train Loss: 2.1999061934923296\n",
            "Epoch 8/50, Train Loss: 2.170960502181082\n",
            "Epoch 9/50, Train Loss: 2.144818488430943\n",
            "Epoch 10/50, Train Loss: 2.1212099956737576\n",
            "Epoch 11/50, Train Loss: 2.0995232958398025\n",
            "Epoch 12/50, Train Loss: 2.0813767780453154\n",
            "Epoch 13/50, Train Loss: 2.0648342139887763\n",
            "Epoch 14/50, Train Loss: 2.0533413194622225\n",
            "Epoch 15/50, Train Loss: 2.0403860005462247\n",
            "Epoch 16/50, Train Loss: 2.035954983329707\n",
            "Epoch 17/50, Train Loss: 2.0241100922449733\n",
            "Epoch 18/50, Train Loss: 2.0260779954551436\n",
            "Epoch 19/50, Train Loss: 2.016100309538973\n",
            "Epoch 20/50, Train Loss: 2.0242892480041323\n",
            "Epoch 21/50, Train Loss: 2.0235064781808165\n",
            "Epoch 22/50, Train Loss: 2.0384535780445003\n",
            "Epoch 23/50, Train Loss: 2.0489681928494994\n",
            "Epoch 24/50, Train Loss: 2.0649192590949426\n",
            "Epoch 25/50, Train Loss: 2.0493299751308127\n",
            "Epoch 26/50, Train Loss: 2.0558768233446876\n",
            "Epoch 27/50, Train Loss: 2.018261176173477\n",
            "Epoch 28/50, Train Loss: 2.0191385919098264\n",
            "Epoch 29/50, Train Loss: 1.9880638821544385\n",
            "Epoch 30/50, Train Loss: 1.989427842964193\n",
            "Epoch 31/50, Train Loss: 1.9631979095624177\n",
            "Epoch 32/50, Train Loss: 1.9671022636999091\n",
            "Epoch 33/50, Train Loss: 1.9437102960477377\n",
            "Epoch 34/50, Train Loss: 1.951010974893827\n",
            "Epoch 35/50, Train Loss: 1.928320262601304\n",
            "Epoch 36/50, Train Loss: 1.939433937769243\n",
            "Epoch 37/50, Train Loss: 1.9158668907702123\n",
            "Epoch 38/50, Train Loss: 1.930907853899317\n",
            "Epoch 39/50, Train Loss: 1.9055919425093517\n",
            "Epoch 40/50, Train Loss: 1.9242732088911323\n",
            "Epoch 41/50, Train Loss: 1.897255189969137\n",
            "Epoch 42/50, Train Loss: 1.9190589260960293\n",
            "Epoch 43/50, Train Loss: 1.8916505235311714\n",
            "Epoch 44/50, Train Loss: 1.9163630242614436\n",
            "Epoch 45/50, Train Loss: 1.8924949846546522\n",
            "Epoch 46/50, Train Loss: 1.9202329959743865\n",
            "Epoch 47/50, Train Loss: 1.9092602081188628\n",
            "Epoch 48/50, Train Loss: 1.9345187058439641\n",
            "Epoch 49/50, Train Loss: 1.9373622863817797\n",
            "Epoch 50/50, Train Loss: 1.9322713378212737\n",
            "Test Accuracy: 30.95%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=0.001, learning_rate=1, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.304810781991564\n",
            "Epoch 2/30, Train Loss: 2.300022236566804\n",
            "Epoch 3/30, Train Loss: 2.2947724934917106\n",
            "Epoch 4/30, Train Loss: 2.2866490907976247\n",
            "Epoch 5/30, Train Loss: 2.2734201156224842\n",
            "Epoch 6/30, Train Loss: 2.2538452554513024\n",
            "Epoch 7/30, Train Loss: 2.229106381358855\n",
            "Epoch 8/30, Train Loss: 2.2026600445558984\n",
            "Epoch 9/30, Train Loss: 2.177582941585905\n",
            "Epoch 10/30, Train Loss: 2.1547540508365954\n",
            "Epoch 11/30, Train Loss: 2.1336820366691676\n",
            "Epoch 12/30, Train Loss: 2.113937501256586\n",
            "Epoch 13/30, Train Loss: 2.095515702428188\n",
            "Epoch 14/30, Train Loss: 2.0785282667770026\n",
            "Epoch 15/30, Train Loss: 2.0629527704337103\n",
            "Epoch 16/30, Train Loss: 2.0486846055005827\n",
            "Epoch 17/30, Train Loss: 2.035613166544652\n",
            "Epoch 18/30, Train Loss: 2.02363653893925\n",
            "Epoch 19/30, Train Loss: 2.0126521000565067\n",
            "Epoch 20/30, Train Loss: 2.0025551021589973\n",
            "Epoch 21/30, Train Loss: 1.9932408203114163\n",
            "Epoch 22/30, Train Loss: 1.9846102193249553\n",
            "Epoch 23/30, Train Loss: 1.976573944876713\n",
            "Epoch 24/30, Train Loss: 1.9690550907036837\n",
            "Epoch 25/30, Train Loss: 1.9619894211275941\n",
            "Epoch 26/30, Train Loss: 1.955324389116549\n",
            "Epoch 27/30, Train Loss: 1.949017042968332\n",
            "Epoch 28/30, Train Loss: 1.943031798972558\n",
            "Epoch 29/30, Train Loss: 1.9373382312339718\n",
            "Epoch 30/30, Train Loss: 1.9319093131275855\n",
            "Test Accuracy: 31.369999999999997%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=0.001, learning_rate=1, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.304810781991564\n",
            "Epoch 2/50, Train Loss: 2.300022236566804\n",
            "Epoch 3/50, Train Loss: 2.2947724934917106\n",
            "Epoch 4/50, Train Loss: 2.2866490907976247\n",
            "Epoch 5/50, Train Loss: 2.2734201156224842\n",
            "Epoch 6/50, Train Loss: 2.2538452554513024\n",
            "Epoch 7/50, Train Loss: 2.229106381358855\n",
            "Epoch 8/50, Train Loss: 2.2026600445558984\n",
            "Epoch 9/50, Train Loss: 2.177582941585905\n",
            "Epoch 10/50, Train Loss: 2.1547540508365954\n",
            "Epoch 11/50, Train Loss: 2.1336820366691676\n",
            "Epoch 12/50, Train Loss: 2.113937501256586\n",
            "Epoch 13/50, Train Loss: 2.095515702428188\n",
            "Epoch 14/50, Train Loss: 2.0785282667770026\n",
            "Epoch 15/50, Train Loss: 2.0629527704337103\n",
            "Epoch 16/50, Train Loss: 2.0486846055005827\n",
            "Epoch 17/50, Train Loss: 2.035613166544652\n",
            "Epoch 18/50, Train Loss: 2.02363653893925\n",
            "Epoch 19/50, Train Loss: 2.0126521000565067\n",
            "Epoch 20/50, Train Loss: 2.0025551021589973\n",
            "Epoch 21/50, Train Loss: 1.9932408203114163\n",
            "Epoch 22/50, Train Loss: 1.9846102193249553\n",
            "Epoch 23/50, Train Loss: 1.976573944876713\n",
            "Epoch 24/50, Train Loss: 1.9690550907036837\n",
            "Epoch 25/50, Train Loss: 1.9619894211275941\n",
            "Epoch 26/50, Train Loss: 1.955324389116549\n",
            "Epoch 27/50, Train Loss: 1.949017042968332\n",
            "Epoch 28/50, Train Loss: 1.943031798972558\n",
            "Epoch 29/50, Train Loss: 1.9373382312339718\n",
            "Epoch 30/50, Train Loss: 1.9319093131275855\n",
            "Epoch 31/50, Train Loss: 1.9267201020996791\n",
            "Epoch 32/50, Train Loss: 1.921746986374678\n",
            "Epoch 33/50, Train Loss: 1.9169673482329799\n",
            "Epoch 34/50, Train Loss: 1.912359610988992\n",
            "Epoch 35/50, Train Loss: 1.9079034445291274\n",
            "Epoch 36/50, Train Loss: 1.9035800988032434\n",
            "Epoch 37/50, Train Loss: 1.8993726532565438\n",
            "Epoch 38/50, Train Loss: 1.8952662581588822\n",
            "Epoch 39/50, Train Loss: 1.8912481886439065\n",
            "Epoch 40/50, Train Loss: 1.887307894086732\n",
            "Epoch 41/50, Train Loss: 1.8834368495479292\n",
            "Epoch 42/50, Train Loss: 1.8796284669032677\n",
            "Epoch 43/50, Train Loss: 1.8758778178401718\n",
            "Epoch 44/50, Train Loss: 1.8721814902478673\n",
            "Epoch 45/50, Train Loss: 1.8685372547304986\n",
            "Epoch 46/50, Train Loss: 1.8649439349438366\n",
            "Epoch 47/50, Train Loss: 1.8614010642893177\n",
            "Epoch 48/50, Train Loss: 1.85790881904639\n",
            "Epoch 49/50, Train Loss: 1.8544676835424203\n",
            "Epoch 50/50, Train Loss: 1.8510784826249318\n",
            "Test Accuracy: 34.849999999999994%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=0.001, learning_rate=1.1, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.304810781991564\n",
            "Epoch 2/30, Train Loss: 2.299963293309553\n",
            "Epoch 3/30, Train Loss: 2.2941147187805657\n",
            "Epoch 4/30, Train Loss: 2.2843174231230625\n",
            "Epoch 5/30, Train Loss: 2.2678776795146383\n",
            "Epoch 6/30, Train Loss: 2.2439750439452038\n",
            "Epoch 7/30, Train Loss: 2.2154069976450272\n",
            "Epoch 8/30, Train Loss: 2.1869367018702843\n",
            "Epoch 9/30, Train Loss: 2.161025512777658\n",
            "Epoch 10/30, Train Loss: 2.13748334512091\n",
            "Epoch 11/30, Train Loss: 2.1155702859918892\n",
            "Epoch 12/30, Train Loss: 2.095251519282013\n",
            "Epoch 13/30, Train Loss: 2.0766484883790937\n",
            "Epoch 14/30, Train Loss: 2.059758330545976\n",
            "Epoch 15/30, Train Loss: 2.044410208351531\n",
            "Epoch 16/30, Train Loss: 2.0304816141969186\n",
            "Epoch 17/30, Train Loss: 2.0178219469404075\n",
            "Epoch 18/30, Train Loss: 2.0063073481648153\n",
            "Epoch 19/30, Train Loss: 1.995786094734904\n",
            "Epoch 20/30, Train Loss: 1.9861326899618246\n",
            "Epoch 21/30, Train Loss: 1.977216408754199\n",
            "Epoch 22/30, Train Loss: 1.9689391037461237\n",
            "Epoch 23/30, Train Loss: 1.9612101274047564\n",
            "Epoch 24/30, Train Loss: 1.9539659054981942\n",
            "Epoch 25/30, Train Loss: 1.9471508620963802\n",
            "Epoch 26/30, Train Loss: 1.9407275492008453\n",
            "Epoch 27/30, Train Loss: 1.9346633617153959\n",
            "Epoch 28/30, Train Loss: 1.9289437071714262\n",
            "Epoch 29/30, Train Loss: 1.9235551405218623\n",
            "Epoch 30/30, Train Loss: 1.9185323812087387\n",
            "Test Accuracy: 31.619999999999997%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=0.001, learning_rate=1.1, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.304810781991564\n",
            "Epoch 2/50, Train Loss: 2.299963293309553\n",
            "Epoch 3/50, Train Loss: 2.2941147187805657\n",
            "Epoch 4/50, Train Loss: 2.2843174231230625\n",
            "Epoch 5/50, Train Loss: 2.2678776795146383\n",
            "Epoch 6/50, Train Loss: 2.2439750439452038\n",
            "Epoch 7/50, Train Loss: 2.2154069976450272\n",
            "Epoch 8/50, Train Loss: 2.1869367018702843\n",
            "Epoch 9/50, Train Loss: 2.161025512777658\n",
            "Epoch 10/50, Train Loss: 2.13748334512091\n",
            "Epoch 11/50, Train Loss: 2.1155702859918892\n",
            "Epoch 12/50, Train Loss: 2.095251519282013\n",
            "Epoch 13/50, Train Loss: 2.0766484883790937\n",
            "Epoch 14/50, Train Loss: 2.059758330545976\n",
            "Epoch 15/50, Train Loss: 2.044410208351531\n",
            "Epoch 16/50, Train Loss: 2.0304816141969186\n",
            "Epoch 17/50, Train Loss: 2.0178219469404075\n",
            "Epoch 18/50, Train Loss: 2.0063073481648153\n",
            "Epoch 19/50, Train Loss: 1.995786094734904\n",
            "Epoch 20/50, Train Loss: 1.9861326899618246\n",
            "Epoch 21/50, Train Loss: 1.977216408754199\n",
            "Epoch 22/50, Train Loss: 1.9689391037461237\n",
            "Epoch 23/50, Train Loss: 1.9612101274047564\n",
            "Epoch 24/50, Train Loss: 1.9539659054981942\n",
            "Epoch 25/50, Train Loss: 1.9471508620963802\n",
            "Epoch 26/50, Train Loss: 1.9407275492008453\n",
            "Epoch 27/50, Train Loss: 1.9346633617153959\n",
            "Epoch 28/50, Train Loss: 1.9289437071714262\n",
            "Epoch 29/50, Train Loss: 1.9235551405218623\n",
            "Epoch 30/50, Train Loss: 1.9185323812087387\n",
            "Epoch 31/50, Train Loss: 1.9138927505384333\n",
            "Epoch 32/50, Train Loss: 1.9098435545001249\n",
            "Epoch 33/50, Train Loss: 1.9064344076354143\n",
            "Epoch 34/50, Train Loss: 1.9045197027386356\n",
            "Epoch 35/50, Train Loss: 1.9038360078203422\n",
            "Epoch 36/50, Train Loss: 1.9075236298429543\n",
            "Epoch 37/50, Train Loss: 1.9117683027270274\n",
            "Epoch 38/50, Train Loss: 1.9263842999094747\n",
            "Epoch 39/50, Train Loss: 1.9297031775846751\n",
            "Epoch 40/50, Train Loss: 1.9500301563101587\n",
            "Epoch 41/50, Train Loss: 1.9363751148577586\n",
            "Epoch 42/50, Train Loss: 1.948573261681631\n",
            "Epoch 43/50, Train Loss: 1.9269003401866167\n",
            "Epoch 44/50, Train Loss: 1.9318454949748765\n",
            "Epoch 45/50, Train Loss: 1.9110365934934321\n",
            "Epoch 46/50, Train Loss: 1.9129792354999373\n",
            "Epoch 47/50, Train Loss: 1.8948216209210456\n",
            "Epoch 48/50, Train Loss: 1.8960180737516505\n",
            "Epoch 49/50, Train Loss: 1.8806563642787202\n",
            "Epoch 50/50, Train Loss: 1.8820607394649063\n",
            "Test Accuracy: 33.08%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=0.001, learning_rate=1.25, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.304810781991564\n",
            "Epoch 2/30, Train Loss: 2.3000163571934156\n",
            "Epoch 3/30, Train Loss: 2.293698986454551\n",
            "Epoch 4/30, Train Loss: 2.281802181508062\n",
            "Epoch 5/30, Train Loss: 2.2608694177123883\n",
            "Epoch 6/30, Train Loss: 2.2317370822090195\n",
            "Epoch 7/30, Train Loss: 2.199906387695264\n",
            "Epoch 8/30, Train Loss: 2.1709607017602677\n",
            "Epoch 9/30, Train Loss: 2.1448186937868643\n",
            "Epoch 10/30, Train Loss: 2.1212102071604675\n",
            "Epoch 11/30, Train Loss: 2.0995235136611354\n",
            "Epoch 12/30, Train Loss: 2.0813770020882694\n",
            "Epoch 13/30, Train Loss: 2.0648344441762494\n",
            "Epoch 14/30, Train Loss: 2.0533415559407957\n",
            "Epoch 15/30, Train Loss: 2.0403862421219037\n",
            "Epoch 16/30, Train Loss: 2.0359552308349937\n",
            "Epoch 17/30, Train Loss: 2.024110342160959\n",
            "Epoch 18/30, Train Loss: 2.026078249201179\n",
            "Epoch 19/30, Train Loss: 2.016100561391155\n",
            "Epoch 20/30, Train Loss: 2.0242894990796514\n",
            "Epoch 21/30, Train Loss: 2.023506719304636\n",
            "Epoch 22/30, Train Loss: 2.0384538121776683\n",
            "Epoch 23/30, Train Loss: 2.0489684239183252\n",
            "Epoch 24/30, Train Loss: 2.0649194864824154\n",
            "Epoch 25/30, Train Loss: 2.0493302556142963\n",
            "Epoch 26/30, Train Loss: 2.0558771157625144\n",
            "Epoch 27/30, Train Loss: 2.018261480192481\n",
            "Epoch 28/30, Train Loss: 2.0191389023434327\n",
            "Epoch 29/30, Train Loss: 1.988064200332983\n",
            "Epoch 30/30, Train Loss: 1.9894281639601685\n",
            "Test Accuracy: 28.79%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=64, reg_coeff=0.001, learning_rate=1.25, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.304810781991564\n",
            "Epoch 2/50, Train Loss: 2.3000163571934156\n",
            "Epoch 3/50, Train Loss: 2.293698986454551\n",
            "Epoch 4/50, Train Loss: 2.281802181508062\n",
            "Epoch 5/50, Train Loss: 2.2608694177123883\n",
            "Epoch 6/50, Train Loss: 2.2317370822090195\n",
            "Epoch 7/50, Train Loss: 2.199906387695264\n",
            "Epoch 8/50, Train Loss: 2.1709607017602677\n",
            "Epoch 9/50, Train Loss: 2.1448186937868643\n",
            "Epoch 10/50, Train Loss: 2.1212102071604675\n",
            "Epoch 11/50, Train Loss: 2.0995235136611354\n",
            "Epoch 12/50, Train Loss: 2.0813770020882694\n",
            "Epoch 13/50, Train Loss: 2.0648344441762494\n",
            "Epoch 14/50, Train Loss: 2.0533415559407957\n",
            "Epoch 15/50, Train Loss: 2.0403862421219037\n",
            "Epoch 16/50, Train Loss: 2.0359552308349937\n",
            "Epoch 17/50, Train Loss: 2.024110342160959\n",
            "Epoch 18/50, Train Loss: 2.026078249201179\n",
            "Epoch 19/50, Train Loss: 2.016100561391155\n",
            "Epoch 20/50, Train Loss: 2.0242894990796514\n",
            "Epoch 21/50, Train Loss: 2.023506719304636\n",
            "Epoch 22/50, Train Loss: 2.0384538121776683\n",
            "Epoch 23/50, Train Loss: 2.0489684239183252\n",
            "Epoch 24/50, Train Loss: 2.0649194864824154\n",
            "Epoch 25/50, Train Loss: 2.0493302556142963\n",
            "Epoch 26/50, Train Loss: 2.0558771157625144\n",
            "Epoch 27/50, Train Loss: 2.018261480192481\n",
            "Epoch 28/50, Train Loss: 2.0191389023434327\n",
            "Epoch 29/50, Train Loss: 1.988064200332983\n",
            "Epoch 30/50, Train Loss: 1.9894281639601685\n",
            "Epoch 31/50, Train Loss: 1.9631982378457165\n",
            "Epoch 32/50, Train Loss: 1.9671025913701508\n",
            "Epoch 33/50, Train Loss: 1.943710633138592\n",
            "Epoch 34/50, Train Loss: 1.9510113076907796\n",
            "Epoch 35/50, Train Loss: 1.9283206076036112\n",
            "Epoch 36/50, Train Loss: 1.9394342743566089\n",
            "Epoch 37/50, Train Loss: 1.9158672422163878\n",
            "Epoch 38/50, Train Loss: 1.930908193318149\n",
            "Epoch 39/50, Train Loss: 1.905592297780548\n",
            "Epoch 40/50, Train Loss: 1.92427355027072\n",
            "Epoch 41/50, Train Loss: 1.8972555434991834\n",
            "Epoch 42/50, Train Loss: 1.919059265137575\n",
            "Epoch 43/50, Train Loss: 1.891650859492011\n",
            "Epoch 44/50, Train Loss: 1.9163633438899608\n",
            "Epoch 45/50, Train Loss: 1.8924952559473598\n",
            "Epoch 46/50, Train Loss: 1.9202332574452088\n",
            "Epoch 47/50, Train Loss: 1.9092603467605405\n",
            "Epoch 48/50, Train Loss: 1.9345189464282946\n",
            "Epoch 49/50, Train Loss: 1.937362557705226\n",
            "Epoch 50/50, Train Loss: 1.9322719313801984\n",
            "Test Accuracy: 30.95%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=1e-05, learning_rate=1, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.303537525385555\n",
            "Epoch 2/30, Train Loss: 2.300713619333988\n",
            "Epoch 3/30, Train Loss: 2.3184681539983756\n",
            "Epoch 4/30, Train Loss: 2.4192723092047825\n",
            "Epoch 5/30, Train Loss: 2.7862082778257804\n",
            "Epoch 6/30, Train Loss: 2.648006431249222\n",
            "Epoch 7/30, Train Loss: 2.8767349724039635\n",
            "Epoch 8/30, Train Loss: 2.4884911401437906\n",
            "Epoch 9/30, Train Loss: 2.669344333726746\n",
            "Epoch 10/30, Train Loss: 2.315857065269301\n",
            "Epoch 11/30, Train Loss: 2.4895833584611324\n",
            "Epoch 12/30, Train Loss: 2.3116118913332153\n",
            "Epoch 13/30, Train Loss: 2.3925682937864625\n",
            "Epoch 14/30, Train Loss: 2.2716826085503214\n",
            "Epoch 15/30, Train Loss: 2.2583864661819106\n",
            "Epoch 16/30, Train Loss: 2.3330880096355036\n",
            "Epoch 17/30, Train Loss: 2.237297963462741\n",
            "Epoch 18/30, Train Loss: 2.208055197831296\n",
            "Epoch 19/30, Train Loss: 2.2056636784347727\n",
            "Epoch 20/30, Train Loss: 2.3562903996287083\n",
            "Epoch 21/30, Train Loss: 2.2346303245280157\n",
            "Epoch 22/30, Train Loss: 2.1476843121218\n",
            "Epoch 23/30, Train Loss: 2.1552875937737848\n",
            "Epoch 24/30, Train Loss: 2.1610424126600023\n",
            "Epoch 25/30, Train Loss: 2.2862511489654818\n",
            "Epoch 26/30, Train Loss: 2.1904965929372264\n",
            "Epoch 27/30, Train Loss: 2.114215404398184\n",
            "Epoch 28/30, Train Loss: 2.1216270226152685\n",
            "Epoch 29/30, Train Loss: 2.21153251942215\n",
            "Epoch 30/30, Train Loss: 2.153207680680813\n",
            "Test Accuracy: 27.450000000000003%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=1e-05, learning_rate=1, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.303537525385555\n",
            "Epoch 2/50, Train Loss: 2.300713619333988\n",
            "Epoch 3/50, Train Loss: 2.3184681539983756\n",
            "Epoch 4/50, Train Loss: 2.4192723092047825\n",
            "Epoch 5/50, Train Loss: 2.7862082778257804\n",
            "Epoch 6/50, Train Loss: 2.648006431249222\n",
            "Epoch 7/50, Train Loss: 2.8767349724039635\n",
            "Epoch 8/50, Train Loss: 2.4884911401437906\n",
            "Epoch 9/50, Train Loss: 2.669344333726746\n",
            "Epoch 10/50, Train Loss: 2.315857065269301\n",
            "Epoch 11/50, Train Loss: 2.4895833584611324\n",
            "Epoch 12/50, Train Loss: 2.3116118913332153\n",
            "Epoch 13/50, Train Loss: 2.3925682937864625\n",
            "Epoch 14/50, Train Loss: 2.2716826085503214\n",
            "Epoch 15/50, Train Loss: 2.2583864661819106\n",
            "Epoch 16/50, Train Loss: 2.3330880096355036\n",
            "Epoch 17/50, Train Loss: 2.237297963462741\n",
            "Epoch 18/50, Train Loss: 2.208055197831296\n",
            "Epoch 19/50, Train Loss: 2.2056636784347727\n",
            "Epoch 20/50, Train Loss: 2.3562903996287083\n",
            "Epoch 21/50, Train Loss: 2.2346303245280157\n",
            "Epoch 22/50, Train Loss: 2.1476843121218\n",
            "Epoch 23/50, Train Loss: 2.1552875937737848\n",
            "Epoch 24/50, Train Loss: 2.1610424126600023\n",
            "Epoch 25/50, Train Loss: 2.2862511489654818\n",
            "Epoch 26/50, Train Loss: 2.1904965929372264\n",
            "Epoch 27/50, Train Loss: 2.114215404398184\n",
            "Epoch 28/50, Train Loss: 2.1216270226152685\n",
            "Epoch 29/50, Train Loss: 2.21153251942215\n",
            "Epoch 30/50, Train Loss: 2.153207680680813\n",
            "Epoch 31/50, Train Loss: 2.0997576892719976\n",
            "Epoch 32/50, Train Loss: 2.087661493280638\n",
            "Epoch 33/50, Train Loss: 2.1863664605571698\n",
            "Epoch 34/50, Train Loss: 2.0992559276767495\n",
            "Epoch 35/50, Train Loss: 2.085824601077178\n",
            "Epoch 36/50, Train Loss: 2.0612899887493654\n",
            "Epoch 37/50, Train Loss: 2.1068943741183683\n",
            "Epoch 38/50, Train Loss: 2.0631786536778107\n",
            "Epoch 39/50, Train Loss: 2.0766065475742583\n",
            "Epoch 40/50, Train Loss: 2.0698976264250453\n",
            "Epoch 41/50, Train Loss: 2.088832129774676\n",
            "Epoch 42/50, Train Loss: 2.02895822738314\n",
            "Epoch 43/50, Train Loss: 2.0585713641305734\n",
            "Epoch 44/50, Train Loss: 2.0907889840622853\n",
            "Epoch 45/50, Train Loss: 2.060630332000576\n",
            "Epoch 46/50, Train Loss: 2.08642434109794\n",
            "Epoch 47/50, Train Loss: 2.060218068979705\n",
            "Epoch 48/50, Train Loss: 2.1640781680181527\n",
            "Epoch 49/50, Train Loss: 2.0141808228768388\n",
            "Epoch 50/50, Train Loss: 2.0768946964809083\n",
            "Test Accuracy: 28.389999999999997%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=1e-05, learning_rate=1.1, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.303537525385555\n",
            "Epoch 2/30, Train Loss: 2.3019659375799435\n",
            "Epoch 3/30, Train Loss: 2.3398407317611642\n",
            "Epoch 4/30, Train Loss: 2.565225427880403\n",
            "Epoch 5/30, Train Loss: 3.215048640036267\n",
            "Epoch 6/30, Train Loss: 2.679365029898705\n",
            "Epoch 7/30, Train Loss: 3.1497533680498355\n",
            "Epoch 8/30, Train Loss: 2.8155027332491818\n",
            "Epoch 9/30, Train Loss: 2.9164861148452004\n",
            "Epoch 10/30, Train Loss: 2.423471466123043\n",
            "Epoch 11/30, Train Loss: 2.581362716999808\n",
            "Epoch 12/30, Train Loss: 2.5463991129345045\n",
            "Epoch 13/30, Train Loss: 2.422953507632918\n",
            "Epoch 14/30, Train Loss: 2.3884886338568903\n",
            "Epoch 15/30, Train Loss: 2.4376428810546007\n",
            "Epoch 16/30, Train Loss: 2.232630561031479\n",
            "Epoch 17/30, Train Loss: 2.3740372754540573\n",
            "Epoch 18/30, Train Loss: 2.154385486721669\n",
            "Epoch 19/30, Train Loss: 2.374852875122785\n",
            "Epoch 20/30, Train Loss: 2.122489632558462\n",
            "Epoch 21/30, Train Loss: 2.2459554593285667\n",
            "Epoch 22/30, Train Loss: 2.192579011149296\n",
            "Epoch 23/30, Train Loss: 2.3762564893520985\n",
            "Epoch 24/30, Train Loss: 2.0787051968372667\n",
            "Epoch 25/30, Train Loss: 2.0573490739154177\n",
            "Epoch 26/30, Train Loss: 2.0301904245075666\n",
            "Epoch 27/30, Train Loss: 2.0733600026988497\n",
            "Epoch 28/30, Train Loss: 2.149836081878983\n",
            "Epoch 29/30, Train Loss: 2.182896487943474\n",
            "Epoch 30/30, Train Loss: 2.2931797248118233\n",
            "Test Accuracy: 24.95%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=1e-05, learning_rate=1.1, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.303537525385555\n",
            "Epoch 2/50, Train Loss: 2.3019659375799435\n",
            "Epoch 3/50, Train Loss: 2.3398407317611642\n",
            "Epoch 4/50, Train Loss: 2.565225427880403\n",
            "Epoch 5/50, Train Loss: 3.215048640036267\n",
            "Epoch 6/50, Train Loss: 2.679365029898705\n",
            "Epoch 7/50, Train Loss: 3.1497533680498355\n",
            "Epoch 8/50, Train Loss: 2.8155027332491818\n",
            "Epoch 9/50, Train Loss: 2.9164861148452004\n",
            "Epoch 10/50, Train Loss: 2.423471466123043\n",
            "Epoch 11/50, Train Loss: 2.581362716999808\n",
            "Epoch 12/50, Train Loss: 2.5463991129345045\n",
            "Epoch 13/50, Train Loss: 2.422953507632918\n",
            "Epoch 14/50, Train Loss: 2.3884886338568903\n",
            "Epoch 15/50, Train Loss: 2.4376428810546007\n",
            "Epoch 16/50, Train Loss: 2.232630561031479\n",
            "Epoch 17/50, Train Loss: 2.3740372754540573\n",
            "Epoch 18/50, Train Loss: 2.154385486721669\n",
            "Epoch 19/50, Train Loss: 2.374852875122785\n",
            "Epoch 20/50, Train Loss: 2.122489632558462\n",
            "Epoch 21/50, Train Loss: 2.2459554593285667\n",
            "Epoch 22/50, Train Loss: 2.192579011149296\n",
            "Epoch 23/50, Train Loss: 2.3762564893520985\n",
            "Epoch 24/50, Train Loss: 2.0787051968372667\n",
            "Epoch 25/50, Train Loss: 2.0573490739154177\n",
            "Epoch 26/50, Train Loss: 2.0301904245075666\n",
            "Epoch 27/50, Train Loss: 2.0733600026988497\n",
            "Epoch 28/50, Train Loss: 2.149836081878983\n",
            "Epoch 29/50, Train Loss: 2.182896487943474\n",
            "Epoch 30/50, Train Loss: 2.2931797248118233\n",
            "Epoch 31/50, Train Loss: 2.264848151626909\n",
            "Epoch 32/50, Train Loss: 2.133450287665745\n",
            "Epoch 33/50, Train Loss: 2.1803437589265857\n",
            "Epoch 34/50, Train Loss: 2.064616456474163\n",
            "Epoch 35/50, Train Loss: 2.1445939460705743\n",
            "Epoch 36/50, Train Loss: 2.190148348286945\n",
            "Epoch 37/50, Train Loss: 2.1809277021219016\n",
            "Epoch 38/50, Train Loss: 2.0667671448267533\n",
            "Epoch 39/50, Train Loss: 2.1121159183022575\n",
            "Epoch 40/50, Train Loss: 2.1447645381767266\n",
            "Epoch 41/50, Train Loss: 2.2205709230739283\n",
            "Epoch 42/50, Train Loss: 2.0955046352361664\n",
            "Epoch 43/50, Train Loss: 2.112318758197175\n",
            "Epoch 44/50, Train Loss: 2.0894113619518384\n",
            "Epoch 45/50, Train Loss: 2.159246260470123\n",
            "Epoch 46/50, Train Loss: 2.0455299305786596\n",
            "Epoch 47/50, Train Loss: 2.0094531575462486\n",
            "Epoch 48/50, Train Loss: 2.1042102100939237\n",
            "Epoch 49/50, Train Loss: 2.1635342492743153\n",
            "Epoch 50/50, Train Loss: 2.0833661907382237\n",
            "Test Accuracy: 27.88%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=1e-05, learning_rate=1.25, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.303537525385555\n",
            "Epoch 2/30, Train Loss: 2.3043814423873785\n",
            "Epoch 3/30, Train Loss: 2.390203451591841\n",
            "Epoch 4/30, Train Loss: 2.916052409728408\n",
            "Epoch 5/30, Train Loss: 3.832968402102852\n",
            "Epoch 6/30, Train Loss: 2.9592142696821693\n",
            "Epoch 7/30, Train Loss: 3.599111054155112\n",
            "Epoch 8/30, Train Loss: 3.1796199331201604\n",
            "Epoch 9/30, Train Loss: 3.293477873458069\n",
            "Epoch 10/30, Train Loss: 3.361078170751436\n",
            "Epoch 11/30, Train Loss: 2.620395028202512\n",
            "Epoch 12/30, Train Loss: 2.7776076689273053\n",
            "Epoch 13/30, Train Loss: 2.8254211952372814\n",
            "Epoch 14/30, Train Loss: 2.344489536080947\n",
            "Epoch 15/30, Train Loss: 2.4951253179065143\n",
            "Epoch 16/30, Train Loss: 2.1435545906086526\n",
            "Epoch 17/30, Train Loss: 2.313658649086854\n",
            "Epoch 18/30, Train Loss: 2.229430685574196\n",
            "Epoch 19/30, Train Loss: 2.442872688157439\n",
            "Epoch 20/30, Train Loss: 2.3900478338010425\n",
            "Epoch 21/30, Train Loss: 2.1471315902891277\n",
            "Epoch 22/30, Train Loss: 2.2357812500878405\n",
            "Epoch 23/30, Train Loss: 2.1337749437325435\n",
            "Epoch 24/30, Train Loss: 2.329070906910494\n",
            "Epoch 25/30, Train Loss: 2.2123289962716837\n",
            "Epoch 26/30, Train Loss: 2.2558881660319012\n",
            "Epoch 27/30, Train Loss: 2.356691705848179\n",
            "Epoch 28/30, Train Loss: 2.1585257299270673\n",
            "Epoch 29/30, Train Loss: 2.2853548539111177\n",
            "Epoch 30/30, Train Loss: 2.0111760176754876\n",
            "Test Accuracy: 26.22%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=1e-05, learning_rate=1.25, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.303537525385555\n",
            "Epoch 2/50, Train Loss: 2.3043814423873785\n",
            "Epoch 3/50, Train Loss: 2.390203451591841\n",
            "Epoch 4/50, Train Loss: 2.916052409728408\n",
            "Epoch 5/50, Train Loss: 3.832968402102852\n",
            "Epoch 6/50, Train Loss: 2.9592142696821693\n",
            "Epoch 7/50, Train Loss: 3.599111054155112\n",
            "Epoch 8/50, Train Loss: 3.1796199331201604\n",
            "Epoch 9/50, Train Loss: 3.293477873458069\n",
            "Epoch 10/50, Train Loss: 3.361078170751436\n",
            "Epoch 11/50, Train Loss: 2.620395028202512\n",
            "Epoch 12/50, Train Loss: 2.7776076689273053\n",
            "Epoch 13/50, Train Loss: 2.8254211952372814\n",
            "Epoch 14/50, Train Loss: 2.344489536080947\n",
            "Epoch 15/50, Train Loss: 2.4951253179065143\n",
            "Epoch 16/50, Train Loss: 2.1435545906086526\n",
            "Epoch 17/50, Train Loss: 2.313658649086854\n",
            "Epoch 18/50, Train Loss: 2.229430685574196\n",
            "Epoch 19/50, Train Loss: 2.442872688157439\n",
            "Epoch 20/50, Train Loss: 2.3900478338010425\n",
            "Epoch 21/50, Train Loss: 2.1471315902891277\n",
            "Epoch 22/50, Train Loss: 2.2357812500878405\n",
            "Epoch 23/50, Train Loss: 2.1337749437325435\n",
            "Epoch 24/50, Train Loss: 2.329070906910494\n",
            "Epoch 25/50, Train Loss: 2.2123289962716837\n",
            "Epoch 26/50, Train Loss: 2.2558881660319012\n",
            "Epoch 27/50, Train Loss: 2.356691705848179\n",
            "Epoch 28/50, Train Loss: 2.1585257299270673\n",
            "Epoch 29/50, Train Loss: 2.2853548539111177\n",
            "Epoch 30/50, Train Loss: 2.0111760176754876\n",
            "Epoch 31/50, Train Loss: 2.164160946080889\n",
            "Epoch 32/50, Train Loss: 2.2752208019447826\n",
            "Epoch 33/50, Train Loss: 2.2723480575870276\n",
            "Epoch 34/50, Train Loss: 2.101956730319926\n",
            "Epoch 35/50, Train Loss: 2.1729970312215134\n",
            "Epoch 36/50, Train Loss: 2.061193217157414\n",
            "Epoch 37/50, Train Loss: 2.2290841577073475\n",
            "Epoch 38/50, Train Loss: 2.1180081744297152\n",
            "Epoch 39/50, Train Loss: 2.3231522356001904\n",
            "Epoch 40/50, Train Loss: 2.1327728182925108\n",
            "Epoch 41/50, Train Loss: 1.9586245053108522\n",
            "Epoch 42/50, Train Loss: 2.0216065738732416\n",
            "Epoch 43/50, Train Loss: 2.0636160924081297\n",
            "Epoch 44/50, Train Loss: 2.1654298305979203\n",
            "Epoch 45/50, Train Loss: 2.090291848823863\n",
            "Epoch 46/50, Train Loss: 2.2759659585860663\n",
            "Epoch 47/50, Train Loss: 2.0214275352780176\n",
            "Epoch 48/50, Train Loss: 2.115546817177246\n",
            "Epoch 49/50, Train Loss: 2.1545574013904423\n",
            "Epoch 50/50, Train Loss: 2.2375253701844096\n",
            "Test Accuracy: 29.330000000000002%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=0.0001, learning_rate=1, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.3035375609033357\n",
            "Epoch 2/30, Train Loss: 2.3007136548882583\n",
            "Epoch 3/30, Train Loss: 2.3184681897699013\n",
            "Epoch 4/30, Train Loss: 2.419272345806149\n",
            "Epoch 5/30, Train Loss: 2.7862083167684126\n",
            "Epoch 6/30, Train Loss: 2.6480064690105616\n",
            "Epoch 7/30, Train Loss: 2.876735011255171\n",
            "Epoch 8/30, Train Loss: 2.488491179239934\n",
            "Epoch 9/30, Train Loss: 2.6693443739741034\n",
            "Epoch 10/30, Train Loss: 2.315857103312278\n",
            "Epoch 11/30, Train Loss: 2.4895834017317955\n",
            "Epoch 12/30, Train Loss: 2.3116119309623944\n",
            "Epoch 13/30, Train Loss: 2.3925683283930907\n",
            "Epoch 14/30, Train Loss: 2.27168266340263\n",
            "Epoch 15/30, Train Loss: 2.25838651527749\n",
            "Epoch 16/30, Train Loss: 2.3330880484049716\n",
            "Epoch 17/30, Train Loss: 2.2372979944670424\n",
            "Epoch 18/30, Train Loss: 2.208055260281383\n",
            "Epoch 19/30, Train Loss: 2.205663739524557\n",
            "Epoch 20/30, Train Loss: 2.3562904551390367\n",
            "Epoch 21/30, Train Loss: 2.2346303630670845\n",
            "Epoch 22/30, Train Loss: 2.147684363572808\n",
            "Epoch 23/30, Train Loss: 2.155287645200288\n",
            "Epoch 24/30, Train Loss: 2.1610424535420862\n",
            "Epoch 25/30, Train Loss: 2.286251190468492\n",
            "Epoch 26/30, Train Loss: 2.190496645966095\n",
            "Epoch 27/30, Train Loss: 2.1142154642161635\n",
            "Epoch 28/30, Train Loss: 2.1216270629195586\n",
            "Epoch 29/30, Train Loss: 2.2115325534684525\n",
            "Epoch 30/30, Train Loss: 2.153207742302917\n",
            "Test Accuracy: 27.450000000000003%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=0.0001, learning_rate=1, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.3035375609033357\n",
            "Epoch 2/50, Train Loss: 2.3007136548882583\n",
            "Epoch 3/50, Train Loss: 2.3184681897699013\n",
            "Epoch 4/50, Train Loss: 2.419272345806149\n",
            "Epoch 5/50, Train Loss: 2.7862083167684126\n",
            "Epoch 6/50, Train Loss: 2.6480064690105616\n",
            "Epoch 7/50, Train Loss: 2.876735011255171\n",
            "Epoch 8/50, Train Loss: 2.488491179239934\n",
            "Epoch 9/50, Train Loss: 2.6693443739741034\n",
            "Epoch 10/50, Train Loss: 2.315857103312278\n",
            "Epoch 11/50, Train Loss: 2.4895834017317955\n",
            "Epoch 12/50, Train Loss: 2.3116119309623944\n",
            "Epoch 13/50, Train Loss: 2.3925683283930907\n",
            "Epoch 14/50, Train Loss: 2.27168266340263\n",
            "Epoch 15/50, Train Loss: 2.25838651527749\n",
            "Epoch 16/50, Train Loss: 2.3330880484049716\n",
            "Epoch 17/50, Train Loss: 2.2372979944670424\n",
            "Epoch 18/50, Train Loss: 2.208055260281383\n",
            "Epoch 19/50, Train Loss: 2.205663739524557\n",
            "Epoch 20/50, Train Loss: 2.3562904551390367\n",
            "Epoch 21/50, Train Loss: 2.2346303630670845\n",
            "Epoch 22/50, Train Loss: 2.147684363572808\n",
            "Epoch 23/50, Train Loss: 2.155287645200288\n",
            "Epoch 24/50, Train Loss: 2.1610424535420862\n",
            "Epoch 25/50, Train Loss: 2.286251190468492\n",
            "Epoch 26/50, Train Loss: 2.190496645966095\n",
            "Epoch 27/50, Train Loss: 2.1142154642161635\n",
            "Epoch 28/50, Train Loss: 2.1216270629195586\n",
            "Epoch 29/50, Train Loss: 2.2115325534684525\n",
            "Epoch 30/50, Train Loss: 2.153207742302917\n",
            "Epoch 31/50, Train Loss: 2.0997577574182986\n",
            "Epoch 32/50, Train Loss: 2.0876615431871586\n",
            "Epoch 33/50, Train Loss: 2.1863665054851764\n",
            "Epoch 34/50, Train Loss: 2.0992559908385213\n",
            "Epoch 35/50, Train Loss: 2.0858246517767087\n",
            "Epoch 36/50, Train Loss: 2.061290055360626\n",
            "Epoch 37/50, Train Loss: 2.1068944775690213\n",
            "Epoch 38/50, Train Loss: 2.0631786922369657\n",
            "Epoch 39/50, Train Loss: 2.0766065910605467\n",
            "Epoch 40/50, Train Loss: 2.069897674177731\n",
            "Epoch 41/50, Train Loss: 2.0888321764846496\n",
            "Epoch 42/50, Train Loss: 2.028958269126158\n",
            "Epoch 43/50, Train Loss: 2.0585714090772234\n",
            "Epoch 44/50, Train Loss: 2.0907890011200325\n",
            "Epoch 45/50, Train Loss: 2.060630384147442\n",
            "Epoch 46/50, Train Loss: 2.086424348937581\n",
            "Epoch 47/50, Train Loss: 2.060218136071789\n",
            "Epoch 48/50, Train Loss: 2.1640782543133152\n",
            "Epoch 49/50, Train Loss: 2.0141808936528154\n",
            "Epoch 50/50, Train Loss: 2.0768948019575255\n",
            "Test Accuracy: 28.389999999999997%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=0.0001, learning_rate=1.1, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.3035375609033357\n",
            "Epoch 2/30, Train Loss: 2.301965973142685\n",
            "Epoch 3/30, Train Loss: 2.3398407676359128\n",
            "Epoch 4/30, Train Loss: 2.565225465173942\n",
            "Epoch 5/30, Train Loss: 3.2150486804894696\n",
            "Epoch 6/30, Train Loss: 2.679365067966037\n",
            "Epoch 7/30, Train Loss: 3.149753409451947\n",
            "Epoch 8/30, Train Loss: 2.8155027742036873\n",
            "Epoch 9/30, Train Loss: 2.9164861578032264\n",
            "Epoch 10/30, Train Loss: 2.423471511452651\n",
            "Epoch 11/30, Train Loss: 2.5813627625414948\n",
            "Epoch 12/30, Train Loss: 2.5463991411855917\n",
            "Epoch 13/30, Train Loss: 2.422953532298462\n",
            "Epoch 14/30, Train Loss: 2.3884886814781177\n",
            "Epoch 15/30, Train Loss: 2.437642882341186\n",
            "Epoch 16/30, Train Loss: 2.2326306531938793\n",
            "Epoch 17/30, Train Loss: 2.3740372987420653\n",
            "Epoch 18/30, Train Loss: 2.1543854719154436\n",
            "Epoch 19/30, Train Loss: 2.374852726902607\n",
            "Epoch 20/30, Train Loss: 2.1224894879067246\n",
            "Epoch 21/30, Train Loss: 2.2459555633356616\n",
            "Epoch 22/30, Train Loss: 2.192579436238063\n",
            "Epoch 23/30, Train Loss: 2.3762568953816094\n",
            "Epoch 24/30, Train Loss: 2.078705091492875\n",
            "Epoch 25/30, Train Loss: 2.057349317322906\n",
            "Epoch 26/30, Train Loss: 2.03019046781791\n",
            "Epoch 27/30, Train Loss: 2.0733600860766237\n",
            "Epoch 28/30, Train Loss: 2.1498345550909264\n",
            "Epoch 29/30, Train Loss: 2.1828956743007524\n",
            "Epoch 30/30, Train Loss: 2.293177714905724\n",
            "Test Accuracy: 24.95%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=0.0001, learning_rate=1.1, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.3035375609033357\n",
            "Epoch 2/50, Train Loss: 2.301965973142685\n",
            "Epoch 3/50, Train Loss: 2.3398407676359128\n",
            "Epoch 4/50, Train Loss: 2.565225465173942\n",
            "Epoch 5/50, Train Loss: 3.2150486804894696\n",
            "Epoch 6/50, Train Loss: 2.679365067966037\n",
            "Epoch 7/50, Train Loss: 3.149753409451947\n",
            "Epoch 8/50, Train Loss: 2.8155027742036873\n",
            "Epoch 9/50, Train Loss: 2.9164861578032264\n",
            "Epoch 10/50, Train Loss: 2.423471511452651\n",
            "Epoch 11/50, Train Loss: 2.5813627625414948\n",
            "Epoch 12/50, Train Loss: 2.5463991411855917\n",
            "Epoch 13/50, Train Loss: 2.422953532298462\n",
            "Epoch 14/50, Train Loss: 2.3884886814781177\n",
            "Epoch 15/50, Train Loss: 2.437642882341186\n",
            "Epoch 16/50, Train Loss: 2.2326306531938793\n",
            "Epoch 17/50, Train Loss: 2.3740372987420653\n",
            "Epoch 18/50, Train Loss: 2.1543854719154436\n",
            "Epoch 19/50, Train Loss: 2.374852726902607\n",
            "Epoch 20/50, Train Loss: 2.1224894879067246\n",
            "Epoch 21/50, Train Loss: 2.2459555633356616\n",
            "Epoch 22/50, Train Loss: 2.192579436238063\n",
            "Epoch 23/50, Train Loss: 2.3762568953816094\n",
            "Epoch 24/50, Train Loss: 2.078705091492875\n",
            "Epoch 25/50, Train Loss: 2.057349317322906\n",
            "Epoch 26/50, Train Loss: 2.03019046781791\n",
            "Epoch 27/50, Train Loss: 2.0733600860766237\n",
            "Epoch 28/50, Train Loss: 2.1498345550909264\n",
            "Epoch 29/50, Train Loss: 2.1828956743007524\n",
            "Epoch 30/50, Train Loss: 2.293177714905724\n",
            "Epoch 31/50, Train Loss: 2.264847956315132\n",
            "Epoch 32/50, Train Loss: 2.133447786831103\n",
            "Epoch 33/50, Train Loss: 2.1803414176605136\n",
            "Epoch 34/50, Train Loss: 2.064618979905572\n",
            "Epoch 35/50, Train Loss: 2.144594542401745\n",
            "Epoch 36/50, Train Loss: 2.1901539189976442\n",
            "Epoch 37/50, Train Loss: 2.1809268279164113\n",
            "Epoch 38/50, Train Loss: 2.0667811627096504\n",
            "Epoch 39/50, Train Loss: 2.112136759144616\n",
            "Epoch 40/50, Train Loss: 2.1447582437722255\n",
            "Epoch 41/50, Train Loss: 2.220578305266876\n",
            "Epoch 42/50, Train Loss: 2.095483556421291\n",
            "Epoch 43/50, Train Loss: 2.1123011462089716\n",
            "Epoch 44/50, Train Loss: 2.0893908605873373\n",
            "Epoch 45/50, Train Loss: 2.159197911388564\n",
            "Epoch 46/50, Train Loss: 2.045579946793688\n",
            "Epoch 47/50, Train Loss: 2.0095047836323885\n",
            "Epoch 48/50, Train Loss: 2.10422725852555\n",
            "Epoch 49/50, Train Loss: 2.1635387155785115\n",
            "Epoch 50/50, Train Loss: 2.0833489764238036\n",
            "Test Accuracy: 27.88%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=0.0001, learning_rate=1.25, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.3035375609033357\n",
            "Epoch 2/30, Train Loss: 2.30438147796448\n",
            "Epoch 3/30, Train Loss: 2.390203487677812\n",
            "Epoch 4/30, Train Loss: 2.916052448373872\n",
            "Epoch 5/30, Train Loss: 3.832968444365659\n",
            "Epoch 6/30, Train Loss: 2.9592143079654187\n",
            "Epoch 7/30, Train Loss: 3.5991110995555253\n",
            "Epoch 8/30, Train Loss: 3.1796199737369695\n",
            "Epoch 9/30, Train Loss: 3.293477913685146\n",
            "Epoch 10/30, Train Loss: 3.361078209783996\n",
            "Epoch 11/30, Train Loss: 2.6203950597238332\n",
            "Epoch 12/30, Train Loss: 2.7776076996463175\n",
            "Epoch 13/30, Train Loss: 2.8254212287784983\n",
            "Epoch 14/30, Train Loss: 2.3444895814241375\n",
            "Epoch 15/30, Train Loss: 2.4951253403198366\n",
            "Epoch 16/30, Train Loss: 2.1435546228536917\n",
            "Epoch 17/30, Train Loss: 2.3136586837171365\n",
            "Epoch 18/30, Train Loss: 2.2294311020605986\n",
            "Epoch 19/30, Train Loss: 2.4428735429116086\n",
            "Epoch 20/30, Train Loss: 2.3900476276245564\n",
            "Epoch 21/30, Train Loss: 2.147132457721916\n",
            "Epoch 22/30, Train Loss: 2.235781112373321\n",
            "Epoch 23/30, Train Loss: 2.133775723114787\n",
            "Epoch 24/30, Train Loss: 2.3290735618768577\n",
            "Epoch 25/30, Train Loss: 2.2123263701038485\n",
            "Epoch 26/30, Train Loss: 2.2558838316701286\n",
            "Epoch 27/30, Train Loss: 2.3567013154327223\n",
            "Epoch 28/30, Train Loss: 2.158532172889234\n",
            "Epoch 29/30, Train Loss: 2.285361776204236\n",
            "Epoch 30/30, Train Loss: 2.0111505968620578\n",
            "Test Accuracy: 26.22%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=0.0001, learning_rate=1.25, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.3035375609033357\n",
            "Epoch 2/50, Train Loss: 2.30438147796448\n",
            "Epoch 3/50, Train Loss: 2.390203487677812\n",
            "Epoch 4/50, Train Loss: 2.916052448373872\n",
            "Epoch 5/50, Train Loss: 3.832968444365659\n",
            "Epoch 6/50, Train Loss: 2.9592143079654187\n",
            "Epoch 7/50, Train Loss: 3.5991110995555253\n",
            "Epoch 8/50, Train Loss: 3.1796199737369695\n",
            "Epoch 9/50, Train Loss: 3.293477913685146\n",
            "Epoch 10/50, Train Loss: 3.361078209783996\n",
            "Epoch 11/50, Train Loss: 2.6203950597238332\n",
            "Epoch 12/50, Train Loss: 2.7776076996463175\n",
            "Epoch 13/50, Train Loss: 2.8254212287784983\n",
            "Epoch 14/50, Train Loss: 2.3444895814241375\n",
            "Epoch 15/50, Train Loss: 2.4951253403198366\n",
            "Epoch 16/50, Train Loss: 2.1435546228536917\n",
            "Epoch 17/50, Train Loss: 2.3136586837171365\n",
            "Epoch 18/50, Train Loss: 2.2294311020605986\n",
            "Epoch 19/50, Train Loss: 2.4428735429116086\n",
            "Epoch 20/50, Train Loss: 2.3900476276245564\n",
            "Epoch 21/50, Train Loss: 2.147132457721916\n",
            "Epoch 22/50, Train Loss: 2.235781112373321\n",
            "Epoch 23/50, Train Loss: 2.133775723114787\n",
            "Epoch 24/50, Train Loss: 2.3290735618768577\n",
            "Epoch 25/50, Train Loss: 2.2123263701038485\n",
            "Epoch 26/50, Train Loss: 2.2558838316701286\n",
            "Epoch 27/50, Train Loss: 2.3567013154327223\n",
            "Epoch 28/50, Train Loss: 2.158532172889234\n",
            "Epoch 29/50, Train Loss: 2.285361776204236\n",
            "Epoch 30/50, Train Loss: 2.0111505968620578\n",
            "Epoch 31/50, Train Loss: 2.164089706806842\n",
            "Epoch 32/50, Train Loss: 2.2752774513685528\n",
            "Epoch 33/50, Train Loss: 2.272450313082222\n",
            "Epoch 34/50, Train Loss: 2.1019468436249777\n",
            "Epoch 35/50, Train Loss: 2.1731283248677813\n",
            "Epoch 36/50, Train Loss: 2.0613646614469605\n",
            "Epoch 37/50, Train Loss: 2.2303620484383715\n",
            "Epoch 38/50, Train Loss: 2.118646430977962\n",
            "Epoch 39/50, Train Loss: 2.320241182497339\n",
            "Epoch 40/50, Train Loss: 2.1296769856072935\n",
            "Epoch 41/50, Train Loss: 1.9587591487525653\n",
            "Epoch 42/50, Train Loss: 2.0219896390326255\n",
            "Epoch 43/50, Train Loss: 2.067899258754341\n",
            "Epoch 44/50, Train Loss: 2.1915737433701805\n",
            "Epoch 45/50, Train Loss: 2.083036363334002\n",
            "Epoch 46/50, Train Loss: 2.2411655016806717\n",
            "Epoch 47/50, Train Loss: 2.039140385428287\n",
            "Epoch 48/50, Train Loss: 2.1618720658547788\n",
            "Epoch 49/50, Train Loss: 2.104822095793176\n",
            "Epoch 50/50, Train Loss: 2.1719331007462874\n",
            "Test Accuracy: 30.03%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=0.001, learning_rate=1, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.3035379160811438\n",
            "Epoch 2/30, Train Loss: 2.300714010430944\n",
            "Epoch 3/30, Train Loss: 2.3184685474851308\n",
            "Epoch 4/30, Train Loss: 2.41927271181977\n",
            "Epoch 5/30, Train Loss: 2.7862087061946905\n",
            "Epoch 6/30, Train Loss: 2.6480068466238786\n",
            "Epoch 7/30, Train Loss: 2.876735399767174\n",
            "Epoch 8/30, Train Loss: 2.4884915702012496\n",
            "Epoch 9/30, Train Loss: 2.66934477644758\n",
            "Epoch 10/30, Train Loss: 2.3158574837419277\n",
            "Epoch 11/30, Train Loss: 2.489583834438292\n",
            "Epoch 12/30, Train Loss: 2.3116123272540277\n",
            "Epoch 13/30, Train Loss: 2.3925686744592283\n",
            "Epoch 14/30, Train Loss: 2.2716832119255668\n",
            "Epoch 15/30, Train Loss: 2.2583870062331735\n",
            "Epoch 16/30, Train Loss: 2.3330884360990196\n",
            "Epoch 17/30, Train Loss: 2.237298304509512\n",
            "Epoch 18/30, Train Loss: 2.2080558847825382\n",
            "Epoch 19/30, Train Loss: 2.205664350422841\n",
            "Epoch 20/30, Train Loss: 2.3562910102411654\n",
            "Epoch 21/30, Train Loss: 2.2346307484556958\n",
            "Epoch 22/30, Train Loss: 2.147684878083861\n",
            "Epoch 23/30, Train Loss: 2.155288159468005\n",
            "Epoch 24/30, Train Loss: 2.1610428623618763\n",
            "Epoch 25/30, Train Loss: 2.2862516054962247\n",
            "Epoch 26/30, Train Loss: 2.1904971762564847\n",
            "Epoch 27/30, Train Loss: 2.114216062399351\n",
            "Epoch 28/30, Train Loss: 2.1216274659580048\n",
            "Epoch 29/30, Train Loss: 2.211532893923563\n",
            "Epoch 30/30, Train Loss: 2.1532083585276958\n",
            "Test Accuracy: 27.450000000000003%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=0.001, learning_rate=1, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.3035379160811438\n",
            "Epoch 2/50, Train Loss: 2.300714010430944\n",
            "Epoch 3/50, Train Loss: 2.3184685474851308\n",
            "Epoch 4/50, Train Loss: 2.41927271181977\n",
            "Epoch 5/50, Train Loss: 2.7862087061946905\n",
            "Epoch 6/50, Train Loss: 2.6480068466238786\n",
            "Epoch 7/50, Train Loss: 2.876735399767174\n",
            "Epoch 8/50, Train Loss: 2.4884915702012496\n",
            "Epoch 9/50, Train Loss: 2.66934477644758\n",
            "Epoch 10/50, Train Loss: 2.3158574837419277\n",
            "Epoch 11/50, Train Loss: 2.489583834438292\n",
            "Epoch 12/50, Train Loss: 2.3116123272540277\n",
            "Epoch 13/50, Train Loss: 2.3925686744592283\n",
            "Epoch 14/50, Train Loss: 2.2716832119255668\n",
            "Epoch 15/50, Train Loss: 2.2583870062331735\n",
            "Epoch 16/50, Train Loss: 2.3330884360990196\n",
            "Epoch 17/50, Train Loss: 2.237298304509512\n",
            "Epoch 18/50, Train Loss: 2.2080558847825382\n",
            "Epoch 19/50, Train Loss: 2.205664350422841\n",
            "Epoch 20/50, Train Loss: 2.3562910102411654\n",
            "Epoch 21/50, Train Loss: 2.2346307484556958\n",
            "Epoch 22/50, Train Loss: 2.147684878083861\n",
            "Epoch 23/50, Train Loss: 2.155288159468005\n",
            "Epoch 24/50, Train Loss: 2.1610428623618763\n",
            "Epoch 25/50, Train Loss: 2.2862516054962247\n",
            "Epoch 26/50, Train Loss: 2.1904971762564847\n",
            "Epoch 27/50, Train Loss: 2.114216062399351\n",
            "Epoch 28/50, Train Loss: 2.1216274659580048\n",
            "Epoch 29/50, Train Loss: 2.211532893923563\n",
            "Epoch 30/50, Train Loss: 2.1532083585276958\n",
            "Epoch 31/50, Train Loss: 2.0997584388881125\n",
            "Epoch 32/50, Train Loss: 2.087662042249286\n",
            "Epoch 33/50, Train Loss: 2.1863669547593974\n",
            "Epoch 34/50, Train Loss: 2.0992566224599325\n",
            "Epoch 35/50, Train Loss: 2.085825158770719\n",
            "Epoch 36/50, Train Loss: 2.061290721480364\n",
            "Epoch 37/50, Train Loss: 2.1068955120949266\n",
            "Epoch 38/50, Train Loss: 2.063179077821263\n",
            "Epoch 39/50, Train Loss: 2.076607025916975\n",
            "Epoch 40/50, Train Loss: 2.069898151700296\n",
            "Epoch 41/50, Train Loss: 2.0888326435736295\n",
            "Epoch 42/50, Train Loss: 2.0289586865589118\n",
            "Epoch 43/50, Train Loss: 2.0585718585419017\n",
            "Epoch 44/50, Train Loss: 2.090789171700336\n",
            "Epoch 45/50, Train Loss: 2.060630905613666\n",
            "Epoch 46/50, Train Loss: 2.0864244273357544\n",
            "Epoch 47/50, Train Loss: 2.0602188069911156\n",
            "Epoch 48/50, Train Loss: 2.164079117261777\n",
            "Epoch 49/50, Train Loss: 2.0141816014130582\n",
            "Epoch 50/50, Train Loss: 2.076895856725853\n",
            "Test Accuracy: 28.389999999999997%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=0.001, learning_rate=1.1, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.3035379160811438\n",
            "Epoch 2/30, Train Loss: 2.3019663287700873\n",
            "Epoch 3/30, Train Loss: 2.339841126383367\n",
            "Epoch 4/30, Train Loss: 2.565225838109271\n",
            "Epoch 5/30, Train Loss: 3.2150490850214455\n",
            "Epoch 6/30, Train Loss: 2.6793654486392735\n",
            "Epoch 7/30, Train Loss: 3.1497538234729587\n",
            "Epoch 8/30, Train Loss: 2.8155031837486293\n",
            "Epoch 9/30, Train Loss: 2.9164865873833197\n",
            "Epoch 10/30, Train Loss: 2.4234719647485674\n",
            "Epoch 11/30, Train Loss: 2.581363217958231\n",
            "Epoch 12/30, Train Loss: 2.5463994236963807\n",
            "Epoch 13/30, Train Loss: 2.422953778953947\n",
            "Epoch 14/30, Train Loss: 2.388489157689984\n",
            "Epoch 15/30, Train Loss: 2.437642895207306\n",
            "Epoch 16/30, Train Loss: 2.2326315748194703\n",
            "Epoch 17/30, Train Loss: 2.37403753162071\n",
            "Epoch 18/30, Train Loss: 2.1543853238665758\n",
            "Epoch 19/30, Train Loss: 2.3748512446880055\n",
            "Epoch 20/30, Train Loss: 2.1224880413803073\n",
            "Epoch 21/30, Train Loss: 2.245956603384436\n",
            "Epoch 22/30, Train Loss: 2.1925836870447046\n",
            "Epoch 23/30, Train Loss: 2.3762609556318064\n",
            "Epoch 24/30, Train Loss: 2.078704038127134\n",
            "Epoch 25/30, Train Loss: 2.057351751283443\n",
            "Epoch 26/30, Train Loss: 2.0301909010924066\n",
            "Epoch 27/30, Train Loss: 2.073360920450593\n",
            "Epoch 28/30, Train Loss: 2.149819289769629\n",
            "Epoch 29/30, Train Loss: 2.182887540311293\n",
            "Epoch 30/30, Train Loss: 2.293157587249915\n",
            "Test Accuracy: 24.95%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=0.001, learning_rate=1.1, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.3035379160811438\n",
            "Epoch 2/50, Train Loss: 2.3019663287700873\n",
            "Epoch 3/50, Train Loss: 2.339841126383367\n",
            "Epoch 4/50, Train Loss: 2.565225838109271\n",
            "Epoch 5/50, Train Loss: 3.2150490850214455\n",
            "Epoch 6/50, Train Loss: 2.6793654486392735\n",
            "Epoch 7/50, Train Loss: 3.1497538234729587\n",
            "Epoch 8/50, Train Loss: 2.8155031837486293\n",
            "Epoch 9/50, Train Loss: 2.9164865873833197\n",
            "Epoch 10/50, Train Loss: 2.4234719647485674\n",
            "Epoch 11/50, Train Loss: 2.581363217958231\n",
            "Epoch 12/50, Train Loss: 2.5463994236963807\n",
            "Epoch 13/50, Train Loss: 2.422953778953947\n",
            "Epoch 14/50, Train Loss: 2.388489157689984\n",
            "Epoch 15/50, Train Loss: 2.437642895207306\n",
            "Epoch 16/50, Train Loss: 2.2326315748194703\n",
            "Epoch 17/50, Train Loss: 2.37403753162071\n",
            "Epoch 18/50, Train Loss: 2.1543853238665758\n",
            "Epoch 19/50, Train Loss: 2.3748512446880055\n",
            "Epoch 20/50, Train Loss: 2.1224880413803073\n",
            "Epoch 21/50, Train Loss: 2.245956603384436\n",
            "Epoch 22/50, Train Loss: 2.1925836870447046\n",
            "Epoch 23/50, Train Loss: 2.3762609556318064\n",
            "Epoch 24/50, Train Loss: 2.078704038127134\n",
            "Epoch 25/50, Train Loss: 2.057351751283443\n",
            "Epoch 26/50, Train Loss: 2.0301909010924066\n",
            "Epoch 27/50, Train Loss: 2.073360920450593\n",
            "Epoch 28/50, Train Loss: 2.149819289769629\n",
            "Epoch 29/50, Train Loss: 2.182887540311293\n",
            "Epoch 30/50, Train Loss: 2.293157587249915\n",
            "Epoch 31/50, Train Loss: 2.2648459839528137\n",
            "Epoch 32/50, Train Loss: 2.1334227822565515\n",
            "Epoch 33/50, Train Loss: 2.180317993874955\n",
            "Epoch 34/50, Train Loss: 2.0646443199017974\n",
            "Epoch 35/50, Train Loss: 2.1446006674677047\n",
            "Epoch 36/50, Train Loss: 2.1902095103774517\n",
            "Epoch 37/50, Train Loss: 2.1809178957224336\n",
            "Epoch 38/50, Train Loss: 2.066921432197212\n",
            "Epoch 39/50, Train Loss: 2.1123451766149777\n",
            "Epoch 40/50, Train Loss: 2.1446959997245427\n",
            "Epoch 41/50, Train Loss: 2.220652901585285\n",
            "Epoch 42/50, Train Loss: 2.095271231204384\n",
            "Epoch 43/50, Train Loss: 2.1121232721369676\n",
            "Epoch 44/50, Train Loss: 2.0891859278640994\n",
            "Epoch 45/50, Train Loss: 2.158712177970218\n",
            "Epoch 46/50, Train Loss: 2.046085397177801\n",
            "Epoch 47/50, Train Loss: 2.0100280096115477\n",
            "Epoch 48/50, Train Loss: 2.104393926194328\n",
            "Epoch 49/50, Train Loss: 2.1635795417824455\n",
            "Epoch 50/50, Train Loss: 2.0831794859813284\n",
            "Test Accuracy: 27.860000000000003%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=0.001, learning_rate=1.25, epochs=30...\n",
            "Epoch 1/30, Train Loss: 2.3035379160811438\n",
            "Epoch 2/30, Train Loss: 2.3043818337354764\n",
            "Epoch 3/30, Train Loss: 2.390203848537489\n",
            "Epoch 4/30, Train Loss: 2.9160528348284753\n",
            "Epoch 5/30, Train Loss: 3.8329688669936606\n",
            "Epoch 6/30, Train Loss: 2.959214690797821\n",
            "Epoch 7/30, Train Loss: 3.5991115535595606\n",
            "Epoch 8/30, Train Loss: 3.179620379904932\n",
            "Epoch 9/30, Train Loss: 3.293478315955841\n",
            "Epoch 10/30, Train Loss: 3.361078600109569\n",
            "Epoch 11/30, Train Loss: 2.62039537493677\n",
            "Epoch 12/30, Train Loss: 2.777608006835598\n",
            "Epoch 13/30, Train Loss: 2.825421564190015\n",
            "Epoch 14/30, Train Loss: 2.34449003485633\n",
            "Epoch 15/30, Train Loss: 2.495125564453367\n",
            "Epoch 16/30, Train Loss: 2.1435549453093916\n",
            "Epoch 17/30, Train Loss: 2.3136590300334916\n",
            "Epoch 18/30, Train Loss: 2.229435266955455\n",
            "Epoch 19/30, Train Loss: 2.4428820904582795\n",
            "Epoch 20/30, Train Loss: 2.3900455663614903\n",
            "Epoch 21/30, Train Loss: 2.147141132848912\n",
            "Epoch 22/30, Train Loss: 2.2357797394872816\n",
            "Epoch 23/30, Train Loss: 2.133783495336513\n",
            "Epoch 24/30, Train Loss: 2.3291000587564676\n",
            "Epoch 25/30, Train Loss: 2.2123000739106895\n",
            "Epoch 26/30, Train Loss: 2.2558404916673354\n",
            "Epoch 27/30, Train Loss: 2.3567973486877105\n",
            "Epoch 28/30, Train Loss: 2.1585965704843004\n",
            "Epoch 29/30, Train Loss: 2.2854310234220616\n",
            "Epoch 30/30, Train Loss: 2.010896893113462\n",
            "Test Accuracy: 26.229999999999997%\n",
            "Training complete.\n",
            "\n",
            "Training with hidden_size=128, reg_coeff=0.001, learning_rate=1.25, epochs=50...\n",
            "Epoch 1/50, Train Loss: 2.3035379160811438\n",
            "Epoch 2/50, Train Loss: 2.3043818337354764\n",
            "Epoch 3/50, Train Loss: 2.390203848537489\n",
            "Epoch 4/50, Train Loss: 2.9160528348284753\n",
            "Epoch 5/50, Train Loss: 3.8329688669936606\n",
            "Epoch 6/50, Train Loss: 2.959214690797821\n",
            "Epoch 7/50, Train Loss: 3.5991115535595606\n",
            "Epoch 8/50, Train Loss: 3.179620379904932\n",
            "Epoch 9/50, Train Loss: 3.293478315955841\n",
            "Epoch 10/50, Train Loss: 3.361078600109569\n",
            "Epoch 11/50, Train Loss: 2.62039537493677\n",
            "Epoch 12/50, Train Loss: 2.777608006835598\n",
            "Epoch 13/50, Train Loss: 2.825421564190015\n",
            "Epoch 14/50, Train Loss: 2.34449003485633\n",
            "Epoch 15/50, Train Loss: 2.495125564453367\n",
            "Epoch 16/50, Train Loss: 2.1435549453093916\n",
            "Epoch 17/50, Train Loss: 2.3136590300334916\n",
            "Epoch 18/50, Train Loss: 2.229435266955455\n",
            "Epoch 19/50, Train Loss: 2.4428820904582795\n",
            "Epoch 20/50, Train Loss: 2.3900455663614903\n",
            "Epoch 21/50, Train Loss: 2.147141132848912\n",
            "Epoch 22/50, Train Loss: 2.2357797394872816\n",
            "Epoch 23/50, Train Loss: 2.133783495336513\n",
            "Epoch 24/50, Train Loss: 2.3291000587564676\n",
            "Epoch 25/50, Train Loss: 2.2123000739106895\n",
            "Epoch 26/50, Train Loss: 2.2558404916673354\n",
            "Epoch 27/50, Train Loss: 2.3567973486877105\n",
            "Epoch 28/50, Train Loss: 2.1585965704843004\n",
            "Epoch 29/50, Train Loss: 2.2854310234220616\n",
            "Epoch 30/50, Train Loss: 2.010896893113462\n",
            "Epoch 31/50, Train Loss: 2.1633775171009613\n",
            "Epoch 32/50, Train Loss: 2.2758428161036215\n",
            "Epoch 33/50, Train Loss: 2.2734715447168043\n",
            "Epoch 34/50, Train Loss: 2.1018519550909716\n",
            "Epoch 35/50, Train Loss: 2.174449862613114\n",
            "Epoch 36/50, Train Loss: 2.063147336317498\n",
            "Epoch 37/50, Train Loss: 2.243102147423254\n",
            "Epoch 38/50, Train Loss: 2.1246775632311476\n",
            "Epoch 39/50, Train Loss: 2.2907883993622256\n",
            "Epoch 40/50, Train Loss: 2.0997348360257764\n",
            "Epoch 41/50, Train Loss: 1.9637536706461225\n",
            "Epoch 42/50, Train Loss: 2.038090989282162\n",
            "Epoch 43/50, Train Loss: 2.1053018534163095\n",
            "Epoch 44/50, Train Loss: 2.3441984198154158\n",
            "Epoch 45/50, Train Loss: 2.0808038780200646\n",
            "Epoch 46/50, Train Loss: 2.1310145301526604\n",
            "Epoch 47/50, Train Loss: 2.07915654051197\n",
            "Epoch 48/50, Train Loss: 2.252599189024248\n",
            "Epoch 49/50, Train Loss: 1.9938959657720128\n",
            "Epoch 50/50, Train Loss: 1.9064224834386387\n",
            "Test Accuracy: 31.830000000000002%\n",
            "Training complete.\n",
            "\n",
            "Grid search complete.\n",
            "Best parameters found: {'hidden_size': 64, 'reg_coeff': 1e-05, 'learning_rate': 1, 'epochs': 50}\n",
            "Best test accuracy: 34.849999999999994\n"
          ]
        }
      ]
    }
  ]
}