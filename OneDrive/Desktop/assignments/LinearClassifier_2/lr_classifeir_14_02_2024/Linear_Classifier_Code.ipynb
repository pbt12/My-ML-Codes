{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCDl5IlXCth9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n7FWNWb_hH3q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cuqlSATphVcx",
    "outputId": "3808452f-205c-42e0-94a7-9bad575b69d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:04<00:00, 39520815.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "def to_numpy(dataset):\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "    data = next(iter(data_loader))\n",
    "    images, labels = data\n",
    "    images = images.numpy()\n",
    "    labels = labels.numpy()\n",
    "    images = images.reshape(images.shape[0], -1)\n",
    "    return images, labels\n",
    "\n",
    "x_train, y_train = to_numpy(trainset)\n",
    "x_test, y_test = to_numpy(testset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mSVPXwAWROH0",
    "outputId": "503e0791-df4f-41e4-b70f-dde078ec9f6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.097077834816834\n",
      "Epoch 20, Loss: 2.024669673663025\n",
      "Epoch 30, Loss: 1.9822496868518868\n",
      "Epoch 40, Loss: 1.952951850514273\n",
      "Epoch 50, Loss: 1.9310895390070293\n",
      "Epoch 60, Loss: 1.9139715523094172\n",
      "Epoch 70, Loss: 1.9001053034117166\n",
      "Epoch 80, Loss: 1.8885785995851685\n",
      "Epoch 90, Loss: 1.878796574525763\n",
      "Epoch 100, Loss: 1.8703529027054155\n",
      "Training time taken:  191.73924279212952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36.64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def initialize_parameters(input_size, num_classes):\n",
    "    weights = np.random.randn(input_size, num_classes) * 0.0001\n",
    "    bias = np.zeros((1, num_classes))\n",
    "    return weights, bias\n",
    "\n",
    "def linear_forward(X, weights, bias):\n",
    "    return np.dot(X, weights) + bias\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    m = y_true.shape[0]\n",
    "    log_likelihood = -np.log(y_pred[range(m), y_true])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "def linear_backward(X, y_true, y_pred, weights):\n",
    "    m = y_true.shape[0]\n",
    "    grad_softmax = y_pred\n",
    "    grad_softmax[range(m), y_true] -= 1\n",
    "    grad_softmax /= m\n",
    "    grad_weights = np.dot(X.T, grad_softmax)\n",
    "    grad_bias = np.sum(grad_softmax, axis=0, keepdims=True)\n",
    "    return grad_weights, grad_bias\n",
    "\n",
    "def update_parameters(weights, bias, grad_weights, grad_bias, learning_rate):\n",
    "    # Update parameters using gradients\n",
    "    weights -= learning_rate * grad_weights\n",
    "    bias -= learning_rate * grad_bias\n",
    "    return weights, bias\n",
    "\n",
    "def train(x_train, y_train, weights, bias, learning_rate, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        output = linear_forward(x_train, weights, bias)\n",
    "        y_pred = softmax(output)\n",
    "\n",
    "        # Loss\n",
    "        loss = cross_entropy_loss(y_pred, y_train)\n",
    "\n",
    "        # Backward pass\n",
    "        grad_weights, grad_bias = linear_backward(x_train, y_train, y_pred, weights)\n",
    "\n",
    "        # Update parameters\n",
    "        weights, bias = update_parameters(weights, bias, grad_weights, grad_bias, learning_rate)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss}')\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "def test(x_test, y_test, weights, bias):\n",
    "    # Forward pass\n",
    "    output = linear_forward(x_test, weights, bias)\n",
    "    y_pred = softmax(output)\n",
    "\n",
    "    # Convert predictions to label indexes\n",
    "    predictions = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predictions == y_test) * 100\n",
    "    return accuracy\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "input_size = 32*32*3\n",
    "num_classes = 10\n",
    "weights, bias = initialize_parameters(input_size, num_classes)\n",
    "\n",
    "start_time = time.time()\n",
    "weights, bias = train(x_train, y_train, weights, bias, learning_rate, epochs)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print('Training time taken: ', training_time)\n",
    "accuracy = test(x_test, y_test, weights, bias)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYhpP-BhRPfk",
    "outputId": "9d14c035-80d2-48ac-a5a4-6dd6631f52c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.0972238556020617\n",
      "Epoch 20, Loss: 2.0250233011337837\n",
      "Epoch 30, Loss: 1.9828078353432657\n",
      "Epoch 40, Loss: 1.9537136518421627\n",
      "Epoch 50, Loss: 1.9320505710098728\n",
      "Epoch 60, Loss: 1.915125554402625\n",
      "Epoch 70, Loss: 1.9014453620981697\n",
      "Epoch 80, Loss: 1.8900977930988605\n",
      "Epoch 90, Loss: 1.8804882961465164\n",
      "Epoch 100, Loss: 1.872211005036776\n",
      "Training time taken:  183.57237124443054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36.64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_parameters(input_size, num_classes):\n",
    "    weights = np.random.randn(input_size, num_classes) * 0.0001\n",
    "    bias = np.zeros((1, num_classes))\n",
    "    return weights, bias\n",
    "\n",
    "def linear_forward(X, weights, bias):\n",
    "    return np.dot(X, weights) + bias\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true, weights, lambda_reg):\n",
    "    m = y_true.shape[0]\n",
    "    log_likelihood = -np.log(y_pred[range(m), y_true])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    reg_loss = 0.5 * lambda_reg * np.sum(weights ** 2)\n",
    "    loss += reg_loss\n",
    "    return loss\n",
    "\n",
    "def linear_backward(X, y_true, y_pred, weights, lambda_reg):\n",
    "    m = y_true.shape[0]\n",
    "    grad_softmax = y_pred\n",
    "    grad_softmax[range(m), y_true] -= 1\n",
    "    grad_softmax /= m\n",
    "    grad_weights = np.dot(X.T, grad_softmax) + lambda_reg * weights\n",
    "    grad_bias = np.sum(grad_softmax, axis=0, keepdims=True)\n",
    "    return grad_weights, grad_bias\n",
    "\n",
    "def update_parameters(weights, bias, grad_weights, grad_bias, learning_rate):\n",
    "    weights -= learning_rate * grad_weights\n",
    "    bias -= learning_rate * grad_bias\n",
    "    return weights, bias\n",
    "\n",
    "def train(x_train, y_train, weights, bias, learning_rate, epochs, lambda_reg):\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        output = linear_forward(x_train, weights, bias)\n",
    "        y_pred = softmax(output)\n",
    "        # Loss\n",
    "        loss = cross_entropy_loss(y_pred, y_train, weights, lambda_reg)\n",
    "        # Backward pass\n",
    "        grad_weights, grad_bias = linear_backward(x_train, y_train, y_pred, weights, lambda_reg)\n",
    "        # Update parameters\n",
    "        weights, bias = update_parameters(weights, bias, grad_weights, grad_bias, learning_rate)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss}')\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "def test(x_test, y_test, weights, bias):\n",
    "    output = linear_forward(x_test, weights, bias)\n",
    "    y_pred = softmax(output)\n",
    "\n",
    "    predictions = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predictions == y_test) * 100\n",
    "    return accuracy\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "lambda_reg = 0.01  # Regularization parameter\n",
    "\n",
    "weights, bias = initialize_parameters(input_size, num_classes)\n",
    "\n",
    "# Training the model with regularization\n",
    "start_time = time.time()\n",
    "weights, bias = train(x_train, y_train, weights, bias, learning_rate, epochs, lambda_reg)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print('Training time taken: ', training_time)\n",
    "accuracy = test(x_test, y_test, weights, bias)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1vpRGcbRWhEh",
    "outputId": "0ba408cf-71fa-4a20-dbf1-169062e5fe63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.7540510673050587, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.7252810206296785, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.9970611885793192, Learning Rate: 0.001\n",
      "Learning Rate: 0.01, Epochs: 30, Lambda_reg: 0.01, Batch Size: 32, Accuracy: 41.71, Training Time: 61.26 seconds\n",
      "Epoch 10, Loss: 1.9638081578174036, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.7585166276499296, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.7734362593204658, Learning Rate: 0.001\n",
      "Learning Rate: 0.01, Epochs: 30, Lambda_reg: 0.01, Batch Size: 64, Accuracy: 40.94, Training Time: 50.49 seconds\n",
      "Epoch 10, Loss: 1.3420674412039557, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.3988755777678303, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.4086585752406091, Learning Rate: 0.001\n",
      "Learning Rate: 0.01, Epochs: 30, Lambda_reg: 0.001, Batch Size: 32, Accuracy: 41.589999999999996, Training Time: 63.10 seconds\n",
      "Epoch 10, Loss: 1.7866076011985725, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 2.1745909025715715, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.5187411075125712, Learning Rate: 0.001\n",
      "Learning Rate: 0.01, Epochs: 30, Lambda_reg: 0.001, Batch Size: 64, Accuracy: 41.199999999999996, Training Time: 48.67 seconds\n",
      "Epoch 10, Loss: 2.032930459759261, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.4518052398204842, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.8830223536638833, Learning Rate: 0.001\n",
      "Learning Rate: 0.01, Epochs: 30, Lambda_reg: 0.0001, Batch Size: 32, Accuracy: 41.53, Training Time: 61.31 seconds\n",
      "Epoch 10, Loss: 1.9385634189955463, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.6585261692971784, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.7952009861251628, Learning Rate: 0.001\n",
      "Learning Rate: 0.01, Epochs: 30, Lambda_reg: 0.0001, Batch Size: 64, Accuracy: 41.33, Training Time: 50.07 seconds\n",
      "Epoch 10, Loss: 1.7531309803002637, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.4127098310637893, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.8775214462351668, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.500020485326307, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.909452698261512, Learning Rate: 0.0001\n",
      "Learning Rate: 0.01, Epochs: 50, Lambda_reg: 0.01, Batch Size: 32, Accuracy: 41.65, Training Time: 103.20 seconds\n",
      "Epoch 10, Loss: 1.8584586321715144, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.7250943528083575, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.4535159436738825, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.7843058885393677, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.4937012696222174, Learning Rate: 0.0001\n",
      "Learning Rate: 0.01, Epochs: 50, Lambda_reg: 0.01, Batch Size: 64, Accuracy: 41.23, Training Time: 82.89 seconds\n",
      "Epoch 10, Loss: 1.8439100106757649, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.4167194053723555, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.2450325387042391, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.5169817780430697, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.9791808263957509, Learning Rate: 0.0001\n",
      "Learning Rate: 0.01, Epochs: 50, Lambda_reg: 0.001, Batch Size: 32, Accuracy: 41.67, Training Time: 103.37 seconds\n",
      "Epoch 10, Loss: 2.0238210204316425, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.5874376514911095, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.9933118092721804, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.5090684017501497, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.283532153227245, Learning Rate: 0.0001\n",
      "Learning Rate: 0.01, Epochs: 50, Lambda_reg: 0.001, Batch Size: 64, Accuracy: 41.33, Training Time: 84.20 seconds\n",
      "Epoch 10, Loss: 1.5366633037318411, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.8349150969675605, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 2.0024674638941815, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.5963579017070637, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.465474988735112, Learning Rate: 0.0001\n",
      "Learning Rate: 0.01, Epochs: 50, Lambda_reg: 0.0001, Batch Size: 32, Accuracy: 41.65, Training Time: 101.66 seconds\n",
      "Epoch 10, Loss: 1.5854505563358623, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.8127838723656462, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.887044472240821, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.690165540755411, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.7667677194133582, Learning Rate: 0.0001\n",
      "Learning Rate: 0.01, Epochs: 50, Lambda_reg: 0.0001, Batch Size: 64, Accuracy: 41.260000000000005, Training Time: 83.55 seconds\n",
      "Epoch 10, Loss: 1.6591595792062095, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.8100129832073253, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.627719316051464, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.809749217064391, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.6309173935048766, Learning Rate: 0.0001\n",
      "Epoch 60, Loss: 1.472221833142774, Learning Rate: 0.0001\n",
      "Epoch 70, Loss: 1.5631399803461985, Learning Rate: 1e-05\n",
      "Epoch 80, Loss: 2.0223523658419853, Learning Rate: 1e-05\n",
      "Epoch 90, Loss: 1.622765261154215, Learning Rate: 1e-05\n",
      "Epoch 100, Loss: 1.8066190104833917, Learning Rate: 1.0000000000000002e-06\n",
      "Learning Rate: 0.01, Epochs: 100, Lambda_reg: 0.01, Batch Size: 32, Accuracy: 41.68, Training Time: 205.49 seconds\n",
      "Epoch 10, Loss: 1.5243167450321393, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.557371244290864, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.6166284669075168, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.7552648144708707, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.5311627121423375, Learning Rate: 0.0001\n",
      "Epoch 60, Loss: 1.4231192005280293, Learning Rate: 0.0001\n",
      "Epoch 70, Loss: 1.4839037781860465, Learning Rate: 1e-05\n",
      "Epoch 80, Loss: 1.6197607186893166, Learning Rate: 1e-05\n",
      "Epoch 90, Loss: 1.9216993065500199, Learning Rate: 1e-05\n",
      "Epoch 100, Loss: 1.8949106549084969, Learning Rate: 1.0000000000000002e-06\n",
      "Learning Rate: 0.01, Epochs: 100, Lambda_reg: 0.01, Batch Size: 64, Accuracy: 41.33, Training Time: 169.67 seconds\n",
      "Epoch 10, Loss: 1.8119373569227868, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.8894946370618202, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.5169797826731453, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.5396596384089198, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.5976471027738455, Learning Rate: 0.0001\n",
      "Epoch 60, Loss: 1.6016580386116266, Learning Rate: 0.0001\n",
      "Epoch 70, Loss: 1.6325356581117165, Learning Rate: 1e-05\n",
      "Epoch 80, Loss: 1.6490867779508804, Learning Rate: 1e-05\n",
      "Epoch 90, Loss: 1.4400765816507337, Learning Rate: 1e-05\n",
      "Epoch 100, Loss: 1.8297353654606796, Learning Rate: 1.0000000000000002e-06\n",
      "Learning Rate: 0.01, Epochs: 100, Lambda_reg: 0.001, Batch Size: 32, Accuracy: 41.72, Training Time: 204.61 seconds\n",
      "Epoch 10, Loss: 1.9240453654558904, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.413924716852406, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.711288399997565, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.8221710671534972, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.7005372615405694, Learning Rate: 0.0001\n",
      "Epoch 60, Loss: 1.797182052709316, Learning Rate: 0.0001\n",
      "Epoch 70, Loss: 1.267291442747868, Learning Rate: 1e-05\n",
      "Epoch 80, Loss: 1.4037049062850677, Learning Rate: 1e-05\n",
      "Epoch 90, Loss: 1.6673114824987136, Learning Rate: 1e-05\n",
      "Epoch 100, Loss: 1.811681651591305, Learning Rate: 1.0000000000000002e-06\n",
      "Learning Rate: 0.01, Epochs: 100, Lambda_reg: 0.001, Batch Size: 64, Accuracy: 41.3, Training Time: 169.57 seconds\n",
      "Epoch 10, Loss: 1.793835923034668, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.9219483312484746, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.3282398903029213, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.3470021125570397, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.9526294622979528, Learning Rate: 0.0001\n",
      "Epoch 60, Loss: 1.450841709537054, Learning Rate: 0.0001\n",
      "Epoch 70, Loss: 1.3588603558342174, Learning Rate: 1e-05\n",
      "Epoch 80, Loss: 1.7431800529503805, Learning Rate: 1e-05\n",
      "Epoch 90, Loss: 1.6037280906516367, Learning Rate: 1e-05\n",
      "Epoch 100, Loss: 1.7128302774180613, Learning Rate: 1.0000000000000002e-06\n",
      "Learning Rate: 0.01, Epochs: 100, Lambda_reg: 0.0001, Batch Size: 32, Accuracy: 41.69, Training Time: 203.63 seconds\n",
      "Epoch 10, Loss: 2.029778128555819, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.4314415384733807, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.749024640825537, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.550250428147209, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.4773276628248408, Learning Rate: 0.0001\n",
      "Epoch 60, Loss: 1.550526001899538, Learning Rate: 0.0001\n",
      "Epoch 70, Loss: 1.7380148665005504, Learning Rate: 1e-05\n",
      "Epoch 80, Loss: 1.6037376170455073, Learning Rate: 1e-05\n",
      "Epoch 90, Loss: 1.9251289205364297, Learning Rate: 1e-05\n",
      "Epoch 100, Loss: 1.1311695864199476, Learning Rate: 1.0000000000000002e-06\n",
      "Learning Rate: 0.01, Epochs: 100, Lambda_reg: 0.0001, Batch Size: 64, Accuracy: 41.27, Training Time: 166.09 seconds\n",
      "Epoch 10, Loss: 1.8607749228945956, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.4798979168510003, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.4322377867062772, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.7834179734903057, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.5166449450947594, Learning Rate: 0.0001\n",
      "Epoch 60, Loss: 1.6407158279415521, Learning Rate: 0.0001\n",
      "Epoch 70, Loss: 1.4624792815763121, Learning Rate: 1e-05\n",
      "Epoch 80, Loss: 1.6335713144391273, Learning Rate: 1e-05\n",
      "Epoch 90, Loss: 1.6213298337730413, Learning Rate: 1e-05\n",
      "Epoch 100, Loss: 1.5940085396496801, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 110, Loss: 1.8310685389484382, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 120, Loss: 1.6367870418568, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 130, Loss: 1.9985893394964587, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 140, Loss: 1.6861295186265877, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 150, Loss: 1.6438102865398956, Learning Rate: 1.0000000000000002e-07\n",
      "Learning Rate: 0.01, Epochs: 150, Lambda_reg: 0.01, Batch Size: 32, Accuracy: 41.68, Training Time: 308.87 seconds\n",
      "Epoch 10, Loss: 1.6296172055235774, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.9227184343497359, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.68461656831904, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.595028066118491, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.5461740047364352, Learning Rate: 0.0001\n",
      "Epoch 60, Loss: 1.2118039847201167, Learning Rate: 0.0001\n",
      "Epoch 70, Loss: 1.7077360260520233, Learning Rate: 1e-05\n",
      "Epoch 80, Loss: 1.6923714650266173, Learning Rate: 1e-05\n",
      "Epoch 90, Loss: 1.7543712890464558, Learning Rate: 1e-05\n",
      "Epoch 100, Loss: 1.723471106513563, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 110, Loss: 1.8100724148947056, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 120, Loss: 1.5950512512175044, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 130, Loss: 1.6730440281326446, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 140, Loss: 2.4149891687772675, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 150, Loss: 1.8027395995130049, Learning Rate: 1.0000000000000002e-07\n",
      "Learning Rate: 0.01, Epochs: 150, Lambda_reg: 0.01, Batch Size: 64, Accuracy: 41.3, Training Time: 250.02 seconds\n",
      "Epoch 10, Loss: 1.2908823307166393, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.6304261098954551, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.636327176256262, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.9028747523738476, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.9507346341571214, Learning Rate: 0.0001\n",
      "Epoch 60, Loss: 1.617374377954962, Learning Rate: 0.0001\n",
      "Epoch 70, Loss: 1.76496112009714, Learning Rate: 1e-05\n",
      "Epoch 80, Loss: 1.9345610787420136, Learning Rate: 1e-05\n",
      "Epoch 90, Loss: 1.9008115975260171, Learning Rate: 1e-05\n",
      "Epoch 100, Loss: 1.6548243393721969, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 110, Loss: 1.9348543265076508, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 120, Loss: 1.4465381906805574, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 130, Loss: 1.6720217903173793, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 140, Loss: 1.6379047607958765, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 150, Loss: 1.9016258933623704, Learning Rate: 1.0000000000000002e-07\n",
      "Learning Rate: 0.01, Epochs: 150, Lambda_reg: 0.001, Batch Size: 32, Accuracy: 41.72, Training Time: 304.95 seconds\n",
      "Epoch 10, Loss: 1.9003565712875696, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.6484722777893406, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.4125179762319362, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.881421260920127, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.475996404871337, Learning Rate: 0.0001\n",
      "Epoch 60, Loss: 1.4123794828941447, Learning Rate: 0.0001\n",
      "Epoch 70, Loss: 1.7785922930112235, Learning Rate: 1e-05\n",
      "Epoch 80, Loss: 1.7161743118962391, Learning Rate: 1e-05\n",
      "Epoch 90, Loss: 1.4146900056713743, Learning Rate: 1e-05\n",
      "Epoch 100, Loss: 1.7341378352167778, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 110, Loss: 1.9830691662430764, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 120, Loss: 1.7014405794848604, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 130, Loss: 1.678572748185607, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 140, Loss: 2.125718610420328, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 150, Loss: 1.736771620507513, Learning Rate: 1.0000000000000002e-07\n",
      "Learning Rate: 0.01, Epochs: 150, Lambda_reg: 0.001, Batch Size: 64, Accuracy: 41.29, Training Time: 249.58 seconds\n",
      "Epoch 10, Loss: 2.0443722340661497, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.4996747988582604, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 2.295072007463445, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.8642675519428493, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.4071411454853986, Learning Rate: 0.0001\n",
      "Epoch 60, Loss: 1.3314699250862658, Learning Rate: 0.0001\n",
      "Epoch 70, Loss: 1.5648778926861613, Learning Rate: 1e-05\n",
      "Epoch 80, Loss: 1.2026921832724558, Learning Rate: 1e-05\n",
      "Epoch 90, Loss: 2.1185886253069515, Learning Rate: 1e-05\n",
      "Epoch 100, Loss: 1.6548512172003897, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 110, Loss: 1.2941590835758117, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 120, Loss: 1.28214787510033, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 130, Loss: 1.9181157121168875, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 140, Loss: 1.4582622279308888, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 150, Loss: 1.6265457451643732, Learning Rate: 1.0000000000000002e-07\n",
      "Learning Rate: 0.01, Epochs: 150, Lambda_reg: 0.0001, Batch Size: 32, Accuracy: 41.67, Training Time: 305.63 seconds\n",
      "Epoch 10, Loss: 2.046201795267871, Learning Rate: 0.001\n",
      "Epoch 20, Loss: 1.9070230165101099, Learning Rate: 0.001\n",
      "Epoch 30, Loss: 1.5706817229591357, Learning Rate: 0.001\n",
      "Epoch 40, Loss: 1.5757156097060443, Learning Rate: 0.0001\n",
      "Epoch 50, Loss: 1.8494656548601869, Learning Rate: 0.0001\n",
      "Epoch 60, Loss: 1.376801387024135, Learning Rate: 0.0001\n",
      "Epoch 70, Loss: 1.4538421925547553, Learning Rate: 1e-05\n",
      "Epoch 80, Loss: 1.8794191121963286, Learning Rate: 1e-05\n",
      "Epoch 90, Loss: 1.6829202758044073, Learning Rate: 1e-05\n",
      "Epoch 100, Loss: 1.61275284909454, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 110, Loss: 1.4831763651106273, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 120, Loss: 1.6536928946084266, Learning Rate: 1.0000000000000002e-06\n",
      "Epoch 130, Loss: 1.5412833343377446, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 140, Loss: 1.6288381374409673, Learning Rate: 1.0000000000000002e-07\n",
      "Epoch 150, Loss: 1.272621025393303, Learning Rate: 1.0000000000000002e-07\n",
      "Learning Rate: 0.01, Epochs: 150, Lambda_reg: 0.0001, Batch Size: 64, Accuracy: 41.199999999999996, Training Time: 249.34 seconds\n",
      "Best Hyperparameters:\n",
      "{'learning_rate': 0.01, 'epochs': 100, 'lambda_reg': 0.001, 'batch_size': 32}\n",
      "Best Accuracy: 41.72%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def initialize_parameters(input_size, num_classes):\n",
    "    # Initialize weights and biases\n",
    "    weights = np.random.randn(input_size, num_classes) * 0.0001\n",
    "    bias = np.zeros((1, num_classes))\n",
    "    return weights, bias\n",
    "\n",
    "def linear_forward(X, weights, bias):\n",
    "    # Linear forward computation\n",
    "    return np.dot(X, weights) + bias\n",
    "\n",
    "def softmax(z):\n",
    "    # Softmax function for multi-class classification\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true, weights, lambda_reg):\n",
    "\n",
    "    # Cross-entropy loss function with L2 regularization\n",
    "    m = y_true.shape[0]\n",
    "    log_likelihood = -np.log(y_pred[range(m), y_true])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "\n",
    "    # L2 regularization term\n",
    "    reg_loss = 0.5 * lambda_reg * np.sum(weights ** 2)\n",
    "\n",
    "    loss += reg_loss\n",
    "    return loss\n",
    "\n",
    "def linear_backward(X, y_true, y_pred, weights, lambda_reg):\n",
    "    m = y_true.shape[0]\n",
    "    grad_softmax = y_pred\n",
    "    grad_softmax[range(m), y_true] -= 1\n",
    "    grad_softmax /= m\n",
    "    grad_weights = np.dot(X.T, grad_softmax) + lambda_reg * weights\n",
    "    grad_bias = np.sum(grad_softmax, axis=0, keepdims=True)\n",
    "    return grad_weights, grad_bias\n",
    "\n",
    "def update_parameters(weights, bias, grad_weights, grad_bias, learning_rate):\n",
    "    weights -= learning_rate * grad_weights\n",
    "    bias -= learning_rate * grad_bias\n",
    "    return weights, bias\n",
    "\n",
    "def adjust_learning_rate(learning_rate, epoch):\n",
    "    if epoch % 30 == 0:\n",
    "        learning_rate *= 0.1\n",
    "    return learning_rate\n",
    "\n",
    "def train(x_train, y_train, weights, bias, learning_rate, epochs, lambda_reg, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        learning_rate = adjust_learning_rate(learning_rate, epoch)\n",
    "        indices = np.arange(x_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        x_train_shuffled = x_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "\n",
    "        # Mini-batch training\n",
    "        for i in range(0, x_train.shape[0], batch_size):\n",
    "            x_batch = x_train_shuffled[i:i+batch_size]\n",
    "            y_batch = y_train_shuffled[i:i+batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            output = linear_forward(x_batch, weights, bias)\n",
    "            y_pred = softmax(output)\n",
    "\n",
    "            # Loss\n",
    "            loss = cross_entropy_loss(y_pred, y_batch, weights, lambda_reg)\n",
    "\n",
    "            # Backward pass\n",
    "            grad_weights, grad_bias = linear_backward(x_batch, y_batch, y_pred, weights, lambda_reg)\n",
    "\n",
    "            # Update parameters\n",
    "            weights, bias = update_parameters(weights, bias, grad_weights, grad_bias, learning_rate)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}, Loss: {loss}, Learning Rate: {learning_rate}')\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "def test(x_test, y_test, weights, bias):\n",
    "    output = linear_forward(x_test, weights, bias)\n",
    "    y_pred = softmax(output)\n",
    "\n",
    "    predictions = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predictions == y_test) * 100\n",
    "    return accuracy\n",
    "\n",
    "learning_rates = [0.01]\n",
    "epochs_list = [30, 50, 100, 150]\n",
    "lambda_regs = [0.01, 0.001, 0.0001]\n",
    "batch_sizes = [32, 64]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_hyperparameters = {}\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for epochs in epochs_list:\n",
    "        for lambda_reg in lambda_regs:\n",
    "            for batch_size in batch_sizes:\n",
    "                weights, bias = initialize_parameters(input_size, num_classes)\n",
    "                start_time = time.time()\n",
    "                weights, bias = train(x_train, y_train, weights, bias, learning_rate, epochs, lambda_reg, batch_size)\n",
    "                end_time = time.time()\n",
    "                training_time = end_time - start_time\n",
    "                accuracy = test(x_test, y_test, weights, bias)\n",
    "                print(f'Learning Rate: {learning_rate}, Epochs: {epochs}, Lambda_reg: {lambda_reg}, Batch Size: {batch_size}, Accuracy: {accuracy}, Training Time: {training_time:.2f} seconds')\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_hyperparameters = {'learning_rate': learning_rate, 'epochs': epochs, 'lambda_reg': lambda_reg, 'batch_size': batch_size}\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hyperparameters)\n",
    "print(f\"Best Accuracy: {best_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bU23pH_HHMbd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
