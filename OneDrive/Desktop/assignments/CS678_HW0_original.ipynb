{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SgNZTjrhcHa0"
   },
   "source": [
    "## Homework 0, CS678 Spring 2024\n",
    "\n",
    "### This is due on Feburary 2nd, 2024. Please read the report PDF for submission instruction.\n",
    "\n",
    "#### IMPORTANT: After copying this notebook to a Google Drive or One Drive, please paste a link to the PDF report (\"Your Notebook solution\"). To get a publicly-accessible link, hit the *Share* button at the top right, then click \"Get shareable link\" and copy over the result. If you fail to do this, you will receive no credit for this homework!\n",
    "\n",
    "---\n",
    "\n",
    "##### *How to do this problem set:*\n",
    "\n",
    "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.\n",
    " \n",
    "- This assignment is designed so that you can run all cells almost instantly. If it is taking longer than that, you have made a mistake in your code.\n",
    "\n",
    "- Note that there are more questions in the PDF than the ones present in this notebook (which only includes the ones requiring code).\n",
    "\n",
    "---\n",
    "\n",
    "##### *How to submit this problem set:*\n",
    "- After filling in the missing code, provide all the answers in LaTeX template released with the assignment. Once again, you should create a shareable link of your completed notebook and paste it to the LaTex report. The PDF report compiled from running the LaTex template should be submitted to Gradescope.\n",
    "  \n",
    "---\n",
    "\n",
    "##### *Academic honesty*\n",
    "\n",
    "- We will audit the notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. See the course page for honesty policies.\n",
    "\n",
    "- We will also run automatic checks of notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "r1_cCBOwfPxI"
   },
   "source": [
    "### Section 3 \n",
    "\n",
    "#### Question 9 (5 points)\n",
    "\n",
    "Let's switch over to coding! The below coding cell contains the opening paragraph of Daphne du Maurier's novel *Rebecca*. Write some code in this cell to compute the number of unique word **types** and total word **tokens** in this paragraph (watch the lecture videos if you're confused about what these terms mean!). Use a whitespace tokenizer to separate words (i.e., split the string on white space using Python's split function). Be sure that the cell's output is visible in the PDF file you turn in on Gradescope.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9Fm6AQJQDFa"
   },
   "outputs": [],
   "source": [
    "paragraph = '''Last night I dreamed I went to Manderley again. It seemed to me\n",
    "that I was passing through the iron gates that led to the driveway.\n",
    "The drive was just a narrow track now, its stony surface covered\n",
    "with grass and weeds. Sometimes, when I thought I had lost it, it\n",
    "would appear again, beneath a fallen tree or beyond a muddy pool \n",
    "formed by the winter rains. The trees had thrown out new\n",
    "low branches which stretched across my way. I came to the house\n",
    "suddenly, and stood there with my heart beating fast and tears\n",
    "filling my eyes.'''.lower() # lowercase normalization is often useful in NLP\n",
    "\n",
    "types = 0\n",
    "tokens = 0\n",
    "\n",
    "# YOUR CODE HERE! POPULATE THE types AND tokens VARIABLES WITH THE CORRECT VALUES!\n",
    "\n",
    "\n",
    "# DO NOT MODIFY THE BELOW LINE!\n",
    "print('Number of word types: %d, number of word tokens:%d' % (types, tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_5f5YpclYjbh"
   },
   "source": [
    "#### Question 10 (5 points)\n",
    "\n",
    "Now let's look at the most frequently used word **types** in this paragraph. Write some code in the below cell to print out the ten most frequently-occurring types. We have initialized a [Counter](https://docs.python.org/2/library/collections.html#collections.Counter) object that you should use for this purpose. In general, Counters are very useful for text processing in Python.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpjx2fGbh_tp"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter()\n",
    "\n",
    "# YOUR CODE HERE! Use the counter over the above paragraph\n",
    "\n",
    "\n",
    "# DO NOT MODIFY THE BELOW LINES!\n",
    "for word, count in c.most_common()[:10]:\n",
    "    print(word, count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "y9OxWy-CYlp4"
   },
   "source": [
    "### Section 4 \n",
    "\n",
    "#### Question 12 (10 points)\n",
    "In *neural* language models, we represent words with low-dimensional vectors also called *embeddings*. We use these embeddings to compute a vector representation $\\boldsymbol{x}$ of a given prefix, and then predict the probability of the next word conditioned on $\\boldsymbol{x}$. In the below cell, we use [PyTorch](https://pytorch.org), a machine learning framework, to explore this setup. We provide embeddings for the prefix \"Alice talked to\"; your job is to combine them into a single vector representation $\\boldsymbol{x}$ using [element-wise vector addition](https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#elementwise-operations). \n",
    "\n",
    "*TIP: if you're finding the PyTorch coding problems difficult, you may want to run through [the 60 minutes blitz tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)!*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Su_j1JY1QG5f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "prefix = 'Alice talked to'\n",
    "\n",
    "# spend some time understanding this code / reading relevant documentation! \n",
    "# this is a toy problem with a 5 word vocabulary and 10-d embeddings\n",
    "embeddings = torch.nn.Embedding(num_embeddings=5, embedding_dim=10)\n",
    "# we define the vocabulary by hand below (since this is a toy problem)\n",
    "vocab = {'Alice':0, 'talked':1, 'to':2, 'Bob':3, '.':4}\n",
    "\n",
    "# we need to encode our prefix as integer indices (not words) that index \n",
    "# into the embeddings matrix. the below line accomplishes this.\n",
    "# note that PyTorch inputs are always Tensor objects, so we need\n",
    "# to create a LongTensor out of our list of indices first.\n",
    "indices = torch.LongTensor([vocab[w] for w in prefix.split()])\n",
    "prefix_embs = embeddings(indices)\n",
    "print('prefix embedding tensor size: ', prefix_embs.size())\n",
    "\n",
    "# okay! we now have three embeddings corresponding to each of the three\n",
    "# words in the prefix. write some code that adds them element-wise to obtain\n",
    "# a representation of the prefix! store your answer in a variable named \"x\".\n",
    "\n",
    "### YOUR CODE HERE!\n",
    "x = torch.rand(10)\n",
    "\n",
    "### DO NOT MODIFY THE BELOW LINE\n",
    "print('embedding sum: ', x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "b-xGz2_cZVs7"
   },
   "source": [
    "#### Question 14 (10 points)\n",
    "One very important function in neural language models (and for basically every task we'll look at this semester) is the [softmax](https://pytorch.org/docs/master/nn.functional.html#softmax), which is defined over an $n$-dimensional vector $<x_1, x_2, \\dots, x_n>$ as $\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{1 \\leq j \\leq n} e^{x_j}}$. Let's say we have our prefix representation $\\boldsymbol{x}$ from before. We can use the softmax function, along with a linear projection using a matrix $W$, to go from $\\boldsymbol{x}$ to a probability distribution $p$ over the next word: $p = \\text{softmax}(W\\boldsymbol{x}). $Let's explore this in the code cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mClmHIeowL4V"
   },
   "outputs": [],
   "source": [
    "# remember, our goal is to produce a probability distribution over the \n",
    "# next word, conditioned on the prefix representation x. This distribution\n",
    "# is thus over the entire vocabulary (i.e., it is a 5-dimensional vector).\n",
    "# take a look at the dimensionality of x, and you'll notice that it is a \n",
    "# 10-dimensional vector. first, we need to **project** this representation\n",
    "# down to 5-d. We'll do this using the below matrix:\n",
    "\n",
    "W = torch.rand(10, 5)\n",
    "\n",
    "# use this matrix to project x to a 5-d space, and then\n",
    "# use the softmax function to convert it to a probability distribution.\n",
    "# this will involve using PyTorch to compute a matrix/vector product.\n",
    "# look through the documentation if you're confused (torch.nn.functional.softmax)\n",
    "# please store your final probability distribution in the \"probs\" variable.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "probs = torch.rand(5)\n",
    "\n",
    "### DO NOT MODIFY THE BELOW LINE!\n",
    "print('probability distribution', probs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UOlrqnSqZ3H8"
   },
   "source": [
    "#### Questions 15 and 16 (10 points)\n",
    "So far, we have looked at just a single prefix (\"Alice talked to\"). In practice, it is common for us to compute many prefixes in one computation, as this enables us to take advantage of GPU parallelism and also obtain better gradient approximations (we'll talk more about the latter point later). This is called *batching*, where each prefix is an example in a larger batch. Here, you'll redo the computations from the previous cells, but instead of having one prefix, you'll have a batch of two prefixes. The final output of this cell should be a 2x5 matrix that contains two probability distributions, one for each prefix. **NOTE: YOU WILL LOSE POINTS IF YOU USE ANY LOOPS IN YOUR ANSWER!** Your code should be completely vectorized (a few large computations is faster than many smaller ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZarWwkESM7-"
   },
   "outputs": [],
   "source": [
    "# for this problem, we'll just copy our old prefix over three times\n",
    "# to form a batch. in practice, each example in the batch \n",
    "# would be different.\n",
    "batch_indices = torch.cat(2 * [indices]).reshape((2, 3))\n",
    "batch_embs = embeddings(batch_indices)\n",
    "print('batch embedding tensor size: ', batch_embs.size())\n",
    "\n",
    "# now, follow the same procedure as before:\n",
    "# step 1: compose each example's embeddings into a \n",
    "# single representation using element-wise addition. \n",
    "# HINT: check out the \"dim\" or \"axis\" argument (depending on your pytorch version) of the torch.sum function!\n",
    "x = None\n",
    "\n",
    "# step 2: project each composed representation into \n",
    "# a 5-d space using matrix W\n",
    "# step 3: use the softmax function to obtain a 2x5 matrix \n",
    "# with the probability distributions.\n",
    "# Please store this probability matrix in the \"batch_probs\" variable.\n",
    "batch_probs = torch.rand(2,5)\n",
    "\n",
    "### DO NOT MODIFY THE BELOW LINE\n",
    "print(\"batch probability distributions:\", batch_probs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1h5FJw06nc3hHUq-6pmOKShV7d7I2KqUA",
     "timestamp": 1671719094865
    },
    {
     "file_id": "1cZRsCkGlDEJy1L5Rs80tr5eNonzwXVn0",
     "timestamp": 1643318874030
    },
    {
     "file_id": "1wgo33YMqyTmwPXBCgDYvDD39Hgz516zV",
     "timestamp": 1630356584355
    },
    {
     "file_id": "1ZNQQshRjVp-0vLNi6ZGRtXX102EWHRq4",
     "timestamp": 1598302241860
    },
    {
     "file_id": "1XOa--UHuAQpBRcdqYbFcb8QuUTvywsSk",
     "timestamp": 1568522504552
    },
    {
     "file_id": "1LShMg_-e2SzrjDMxSgVbnYyTAgwcJov0",
     "timestamp": 1568420694683
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
