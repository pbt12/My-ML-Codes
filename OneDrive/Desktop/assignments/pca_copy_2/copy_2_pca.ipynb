{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld9VWTmaffZr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-c3LpvaEFwTl"
      },
      "outputs": [],
      "source": [
        "# Importing neccessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Imoprting TrainData.csv and TestData.csv\n",
        "# TrainData.csv and TestData.csv are required to be in the same\n",
        "# folder where this current file is present\n",
        "train_data = pd.read_csv('./TrainData.csv')\n",
        "test_data = pd.read_csv('./TestData.csv')\n",
        "\n",
        "train_labels = train_data.iloc[:, -1]\n",
        "train_data = train_data.iloc[:, :-1]\n",
        "\n",
        "test_labels = test_data.iloc[:, -1]\n",
        "test_data = test_data.iloc[:, :-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GXFYlKblLWll"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate Euclidean distance between two points\n",
        "def dis(x1, x2):\n",
        "    \"\"\"\n",
        "    Calculate the Euclidean distance between two points.\n",
        "\n",
        "    Parameters:\n",
        "    x1 (numpy.ndarray): The first point.\n",
        "    x2 (numpy.ndarray): The second point.\n",
        "\n",
        "    Returns:\n",
        "    float: The Euclidean distance between x1 and x2.\n",
        "    \"\"\"\n",
        "    return np.linalg.norm(x1 - x2)\n",
        "\n",
        "# Function to perform classification using the K - Nearest Neighbor approach\n",
        "def myclassifier(Train, Trainlabel, Test, k=3):\n",
        "    \"\"\"\n",
        "    Train is the training data\n",
        "    Trainlabel is the training labels\n",
        "    Test is the testing data\n",
        "    k is the number of neighbors to consider (default is 3)\n",
        "    The k-NN classifier assigns a test point to the class label\n",
        "    that is most common among its k nearest neighbors in the training data.\n",
        "    You can specify the value of k as a parameter (default is 3)\n",
        "    \"\"\"\n",
        "    # Create a k-NN classifier\n",
        "    classifier = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "    # Fit the classifier to the training data\n",
        "    classifier.fit(Train, Trainlabel)\n",
        "\n",
        "    # Perform classification on the test data\n",
        "    predicted_labels = classifier.predict(Test)\n",
        "    return predicted_labels\n",
        "\n",
        "\n",
        "def calculate_accuracy(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Calculate the accuracy of predicted labels compared to true labels.\n",
        "\n",
        "    Parameters:\n",
        "    true_labels (list or numpy.ndarray): The true labels.\n",
        "    predicted_labels (list): The predicted labels to be evaluated.\n",
        "\n",
        "    Returns:\n",
        "    float: The accuracy of the predicted labels.\n",
        "    \"\"\"\n",
        "    # Ensure that the true labels and predicted labels have the same length\n",
        "    if len(true_labels) != len(predicted_labels):\n",
        "        raise ValueError(\"Length of true_labels and predicted_labels must be the same.\")\n",
        "\n",
        "    # Count the number of correct predictions\n",
        "    correct_predictions = sum(1 for true, predicted in zip(true_labels, predicted_labels) if true == predicted)\n",
        "\n",
        "    # Calculate accuracy as the ratio of correct predictions to total predictions\n",
        "    accuracy = correct_predictions / len(true_labels)\n",
        "\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEtPV4dwJw5l"
      },
      "source": [
        "**PART 1 - Principal Component Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqSMOOTeeMFP"
      },
      "source": [
        "**C**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EFDMA6bJd8-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38bf5083-afb6-4046-cb81-31612a9051a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of components to retain 95% of variance: 110\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def my_pca(data, k):\n",
        "    \"\"\"\n",
        "    Perform Principal Component Analysis (PCA) on the given data.\n",
        "\n",
        "    Parameters:\n",
        "    data (numpy.ndarray): The input data for PCA.\n",
        "    k (int): The number of principal components to retain.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Transformed data after PCA.\n",
        "    numpy.ndarray: Top k eigenvectors.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Calculate the mean of the data\n",
        "    mean = np.mean(data, axis=0)\n",
        "\n",
        "    # Step 2: Center the data by subtracting the mean\n",
        "    centered_data = data - mean\n",
        "\n",
        "    # Step 3: Calculate the covariance matrix\n",
        "    cov_matrix = np.cov(centered_data, rowvar=False)\n",
        "\n",
        "    # Step 4: Calculate eigenvalues and eigenvectors of the covariance matrix\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "    # Step 5: Sort eigenvalues and eigenvectors in descending order\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "    eigenvalues = eigenvalues[sorted_indices]\n",
        "    eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "    # Step 6: Choose the top k eigenvectors to retain variance\n",
        "    top_eigenvectors = eigenvectors[:, :k]\n",
        "\n",
        "    # Step 7: Project the data onto the top k eigenvectors\n",
        "    reduced_data = np.dot(centered_data, top_eigenvectors)\n",
        "\n",
        "    return reduced_data, top_eigenvectors\n",
        "\n",
        "# Apply your PCA implementation to the Train dataset\n",
        "k = 200  # Adjust the number of principal components as needed\n",
        "reduced_train_data, top_eigenvectors = my_pca(train_data, k)\n",
        "\n",
        "# Calculate the total variance\n",
        "total_variance = np.sum(np.var(reduced_train_data, axis=0))\n",
        "\n",
        "# Choose an appropriate number of principal components to retain a significant amount of variance\n",
        "threshold_variance = 0.95  # Adjust as needed\n",
        "cumulative_variance = np.cumsum(np.var(reduced_train_data, axis=0)) / total_variance\n",
        "selected_components = np.argmax(cumulative_variance >= threshold_variance) + 1\n",
        "\n",
        "print(f\"Number of components to retain {threshold_variance:.0%} of variance:\", selected_components)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 - a, b :**"
      ],
      "metadata": {
        "id": "B-khPOcIXsZ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "COXWvGfahR6i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2354124c-86ba-4779-b0e6-5b22199a3723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time: 957.51 seconds\n"
          ]
        }
      ],
      "source": [
        "# From above code we got that with k = 110 we can retain 95% of the variance\n",
        "# so, we will be using  k = 120 (10 value extra, to be on safe side)\n",
        "\n",
        "def my_pca(data, k=None):\n",
        "    \"\"\"\n",
        "    Perform Principal Component Analysis (PCA) on the given data.\n",
        "\n",
        "    Parameters:\n",
        "    data (numpy.ndarray): The input data for PCA.\n",
        "    k (int, optional): The number of principal components to retain. If None, retains all components.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: Transformed data after PCA.\n",
        "    numpy.ndarray: Eigenvalues corresponding to principal components.\n",
        "    numpy.ndarray: Eigenvectors representing the principal components.\n",
        "    numpy.ndarray: Mean of the input data.\n",
        "    numpy.ndarray: Standard deviation of the input data.\n",
        "    \"\"\"\n",
        "    # Start the timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Standardize the data (center and scale)\n",
        "    mean = np.mean(data, axis=0)\n",
        "    std_dev = np.std(data, axis=0)\n",
        "    standardized_data = (data - mean) / std_dev\n",
        "\n",
        "    # Compute the covariance matrix\n",
        "    covariance_matrix = np.cov(standardized_data, rowvar=False)\n",
        "\n",
        "    # Calculate eigenvalues and eigenvectors\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
        "\n",
        "    # Sort eigenvalues and corresponding eigenvectors in descending order\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "    eigenvalues = eigenvalues[sorted_indices]\n",
        "    eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "    # Select the top k eigenvectors if specified\n",
        "    if k is not None:\n",
        "        eigenvalues = eigenvalues[:k]\n",
        "        eigenvectors = eigenvectors[:, :k]\n",
        "\n",
        "    # Project the data onto the selected eigenvectors\n",
        "    transformed_data = np.dot(standardized_data, eigenvectors)\n",
        "\n",
        "    # End the timer\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate and print the execution time\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "\n",
        "    return transformed_data, eigenvalues, eigenvectors, mean, std_dev\n",
        "\n",
        "# Perform PCA on the training data with a specified number of components (e.g., k=120)\n",
        "transformed_pca_train, pca_eigenvalues, pca_eigenvectors, pca_mean, pca_std_dev = my_pca(train_data.values, k=120)\n",
        "real_transformed_pca_train = np.real(transformed_pca_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "SZJa-J0X7R1p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MIsoYkVd9O8"
      },
      "source": [
        "**1.2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ExNgO2Y6cPNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106c701f-afbd-478d-adf6-62910ab31e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution time: 1.19 seconds\n"
          ]
        }
      ],
      "source": [
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Step 1: Create a PCA instance with the desired number of components\n",
        "n_components = 120\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "# Step 2: Fit PCA on the training data to compute principal components\n",
        "pca.fit(train_data)\n",
        "\n",
        "# Step 3: Transform the training data into the new feature space defined by PCs\n",
        "transformed_train_data_pca_sk = pca.transform(train_data)\n",
        "\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the execution time\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Execution time: {execution_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key findings :\n",
        "### Sklearn PCA vs my_pca\n",
        "\n",
        "- Although both the PCA from sklearn library and my_pca does the same transformation, the time taken to transform the data varies highly\n",
        "- From the output of above code block we can see that\n",
        "    - Execution time for my_pca is 611.3 seconds,\n",
        "    - Whereas Exectution time for PCA function from sklearn library is 1.16 seconds whcih is 900 times faster\n",
        "\n",
        "**Conlusion : Using sklearn's inbuilt functions are way faster than using functions that we were written from scratch**"
      ],
      "metadata": {
        "id": "QdpxrfrKU2KX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7UgCpw7XcPQh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SKeZPcpoa-q5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY0lqKdSJ2GN"
      },
      "source": [
        "**PART 2 - Kernal PCA (KPCA)**\n",
        "\n",
        "**2.1 KPCA with rbf**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kpFzbXWnJzIe"
      },
      "outputs": [],
      "source": [
        "def rbf_kernel(X1, X2, gamma=1.0):\n",
        "\n",
        "    # Calculate the pairwise squared Euclidean distances\n",
        "    pairwise_distances = np.sum(X1**2, axis=1, keepdims=True) + np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T)\n",
        "\n",
        "    # Calculate the kernel matrix using the RBF kernel formula\n",
        "    kernel_matrix = np.exp(-gamma * pairwise_distances)\n",
        "\n",
        "    return kernel_matrix\n",
        "\n",
        "# Define the Kernel PCA function with RBF kernel\n",
        "def kpca_rbf(X, gamma=1.0, n_components=None):\n",
        "\n",
        "    n_samples, _ = X.shape\n",
        "\n",
        "    # Initialize the scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Standardize the data and fit the scaler\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    # Calculate the kernel matrix using the RBF kernel defined above\n",
        "    kernel_matrix = rbf_kernel(X, X, gamma=gamma)\n",
        "\n",
        "    # Center the kernel matrix\n",
        "    one_n = np.ones((n_samples, n_samples)) / n_samples\n",
        "    kernel_matrix_centered = kernel_matrix - np.dot(one_n, kernel_matrix) - np.dot(kernel_matrix, one_n) + np.dot(np.dot(one_n, kernel_matrix), one_n)\n",
        "\n",
        "\n",
        "    # Compute eigenvalues and eigenvectors of the centered kernel matrix\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(kernel_matrix_centered)\n",
        "\n",
        "    # Sort eigenvalues and corresponding eigenvectors in descending order\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "    eigenvalues = eigenvalues[sorted_indices]\n",
        "    eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "    # Select the top n_components eigenvectors if specified\n",
        "    if n_components is not None:\n",
        "        eigenvalues = eigenvalues[:n_components]\n",
        "        eigenvectors = eigenvectors[:, :n_components]\n",
        "\n",
        "    # Project the data onto the selected eigenvectors\n",
        "    transformed_data = np.dot(kernel_matrix_centered, eigenvectors)\n",
        "\n",
        "    return transformed_data, eigenvalues, eigenvectors, scaler\n",
        "\n",
        "# Perform Kernel PCA transformation on the training data\n",
        "transformed_rbf_train, rbf_eigenvalues, rbf_eigenvectors, rbf_scaler = kpca_rbf(train_data.values,0, n_components=120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOjAKwT9XN5I"
      },
      "source": [
        "**2.2 KPCA with Polynomial Kernel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cfpu4xSxJzKt"
      },
      "outputs": [],
      "source": [
        "# Define the Polynomial kernel function\n",
        "def polynomial_kernel(X1, X2, degree=3):\n",
        "    kernel_matrix = (1 + np.dot(X1, X2.T)) ** degree\n",
        "    return kernel_matrix\n",
        "\n",
        "# Define the Kernel PCA function with Polynomial kernel\n",
        "def kpca_poly(X, degree=2, n_components=None):\n",
        "    n_samples, _ = X.shape\n",
        "\n",
        "    # Initialize the scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Standardize the data and fit the scaler\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    # Calculate the kernel matrix using the Polynomial kernel\n",
        "\n",
        "    kernel_matrix = polynomial_kernel(X, X, degree=degree)\n",
        "\n",
        "    # Center the kernel matrix\n",
        "    one_n = np.ones((n_samples, n_samples)) / n_samples\n",
        "    kernel_matrix_centered = kernel_matrix - np.dot(one_n, kernel_matrix) - np.dot(kernel_matrix, one_n) + np.dot(np.dot(one_n, kernel_matrix), one_n)\n",
        "\n",
        "    # Compute eigenvalues and eigenvectors of the centered kernel matrix\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(kernel_matrix_centered)\n",
        "\n",
        "    # Sort eigenvalues and corresponding eigenvectors in descending order\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "    eigenvalues = eigenvalues[sorted_indices]\n",
        "    eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "    # Select the top n_components eigenvectors if specified\n",
        "    if n_components is not None:\n",
        "        eigenvalues = eigenvalues[:n_components]\n",
        "        eigenvectors = eigenvectors[:, :n_components]\n",
        "\n",
        "    # Project the data onto the selected eigenvectors\n",
        "    transformed_data = np.dot(kernel_matrix_centered, eigenvectors)\n",
        "\n",
        "    return transformed_data, eigenvalues, eigenvectors, scaler\n",
        "\n",
        "n_components = 120  # Adjust the number of components as needed\n",
        "degree_poly = 3  # Adjust the degree of the polynomial kernel\n",
        "\n",
        "# Perform Polynomial Kernel PCA transformation on the training data\n",
        "transformed_poly_train, poly_eigenvalues, poly_eigenvectors, poly_scaler = kpca_poly(train_data.values, degree=degree_poly, n_components=n_components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGJ4dLUmZqKT"
      },
      "source": [
        "**2.3 KPCA with Linear Kernel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B2-MlZs1JzNH"
      },
      "outputs": [],
      "source": [
        "# Define the Linear kernel function\n",
        "def linear_kernel(X1, X2):\n",
        "    return np.dot(X1, X2.T)\n",
        "\n",
        "# Define the Kernel PCA function with Linear kernel\n",
        "def kpca_linear(X, n_components=None):\n",
        "    n_samples, _ = X.shape\n",
        "\n",
        "    # Initialize the scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Standardize the data and fit the scaler\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "    # Calculate the kernel matrix using the Linear kernel\n",
        "    kernel_matrix = linear_kernel(X, X)\n",
        "\n",
        "    # Center the kernel matrix\n",
        "    one_n = np.ones((n_samples, n_samples)) / n_samples\n",
        "    kernel_matrix_centered = kernel_matrix - np.dot(one_n, kernel_matrix) - np.dot(kernel_matrix, one_n) + np.dot(np.dot(one_n, kernel_matrix), one_n)\n",
        "\n",
        "    # Compute eigenvalues and eigenvectors of the centered kernel matrix\n",
        "    eigenvalues, eigenvectors = np.linalg.eigh(kernel_matrix_centered)\n",
        "\n",
        "    # Sort eigenvalues and corresponding eigenvectors in descending order\n",
        "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "    eigenvalues = eigenvalues[sorted_indices]\n",
        "    eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "    # Select the top n_components eigenvectors if specified\n",
        "    if n_components is not None:\n",
        "        eigenvalues = eigenvalues[:n_components]\n",
        "        eigenvectors = eigenvectors[:, :n_components]\n",
        "\n",
        "    # Project the data onto the selected eigenvectors\n",
        "    transformed_data = np.dot(kernel_matrix_centered, eigenvectors)\n",
        "\n",
        "    return transformed_data, eigenvalues, eigenvectors, scaler\n",
        "\n",
        "\n",
        "# Set the number of components\n",
        "n_components = 120\n",
        "\n",
        "# Perform Kernel PCA transformation on the training data\n",
        "transformed_linear_train, linear_eigenvalues, linear_eigenvectors, linear_scaler = kpca_linear(train_data.values, n_components=n_components)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjQM_1qxJ5uP"
      },
      "source": [
        "**PART 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wGnW62YpJzW2"
      },
      "outputs": [],
      "source": [
        "# Ensure that the test data is standardized and centered with respect to the training data using the scaler\n",
        "# Use the same transformation on the test data\n",
        "# Ensure that the test data is standardized with respect to the training data (use mean and std_dev)\n",
        "transformed_pca_test = (test_data - pca_mean) / pca_std_dev\n",
        "transformed_pca_test = np.dot(transformed_pca_test, pca_eigenvectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TMuAcfu3JzYI"
      },
      "outputs": [],
      "source": [
        "# Ensure that the test data is standardized and centered with respect to the training data using the scaler\n",
        "# Use the same transformation on the test data\n",
        "# This way we can make sure that the transformed test data is consistent with transformed train data\n",
        "centered_rbf_test_data = rbf_scaler.transform(test_data.values)\n",
        "transformed_rbf_test = np.dot(rbf_kernel(centered_rbf_test_data, centered_rbf_test_data), rbf_eigenvectors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5v8vmpw4C7hp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NczXY3umJzZr"
      },
      "outputs": [],
      "source": [
        "# Ensure that the test data is standardized and centered with respect to the training data using the scaler\n",
        "# Use the same transformation on the test data\n",
        "# Ensure that the test data is standardized and centered with respect to the training data using the scaler\n",
        "centered_poly_test_data = poly_scaler.transform(test_data.values)\n",
        "transformed_poly_test = np.dot(polynomial_kernel(centered_poly_test_data, centered_poly_test_data, degree=degree_poly), poly_eigenvectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "r6MkzCjIJzbG"
      },
      "outputs": [],
      "source": [
        "# Ensure that the test data is standardized and centered with respect to the training data using the scaler\n",
        "# Use the same transformation on the test data\n",
        "# Ensure that the test data is standardized and centered with respect to the training data using the scaler\n",
        "centered_test_data = linear_scaler.transform(test_data.values)\n",
        "transformed_linear_test = np.dot(linear_kernel(centered_test_data, centered_test_data), linear_eigenvectors)\n",
        "real_transformed_pca_test = np.real(transformed_pca_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFYFswUgJ9Bu"
      },
      "source": [
        "**PART 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "14_ZK1IPUe8K",
        "outputId": "e2a64393-153d-4b0b-c6ec-5695d3f1f244",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using pca : 0.83\n"
          ]
        }
      ],
      "source": [
        "real_transformed_pca_test = np.real(transformed_pca_test)\n",
        "predicted_labels_pca = myclassifier(real_transformed_pca_train, train_labels.values, real_transformed_pca_test)\n",
        "\n",
        "# Calculate accuracy using sklearn's accuracy_score function\n",
        "accuracy_pca = calculate_accuracy(test_labels.values, predicted_labels_pca)\n",
        "print(f'Accuracy using pca :', accuracy_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "knOm8pAiJzef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5319f2ec-30df-483a-938b-7106ffff47d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using rbf_kcpa : 0.025\n"
          ]
        }
      ],
      "source": [
        "# Now you can use transformed_rbf_train and transformed_rbf_test for classification\n",
        "rbf_predicted_labels = myclassifier(transformed_rbf_train, train_labels.values, transformed_rbf_test)\n",
        "\n",
        "# Calculate accuracy using sklearn's accuracy_score function\n",
        "accuracy_rbf_kpca = calculate_accuracy(test_labels.values, rbf_predicted_labels)\n",
        "print(f'Accuracy using rbf_kcpa :', accuracy_rbf_kpca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "JCZ-J0bWJzfk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0baf41fd-2ad9-488c-d2db-84d0c1be730c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using polynomial_kcpa of degree 3: 0.605\n"
          ]
        }
      ],
      "source": [
        "# Now you can use transformed_poly_train and transformed_poly_test for classification\n",
        "predicted_labels_poly = myclassifier(transformed_poly_train, train_labels.values, transformed_poly_test)\n",
        "\n",
        "# Calculate accuracy using sklearn's accuracy_score function\n",
        "accuracy_poly_kpca = calculate_accuracy(test_labels.values, predicted_labels_poly)\n",
        "print(f'Accuracy using polynomial_kcpa of degree {degree_poly}:', accuracy_poly_kpca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GSQAmJHJJzg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b127a94a-cb5a-4bff-96de-e6a596f4d7ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using linear_kcpa for : 0.76\n"
          ]
        }
      ],
      "source": [
        "# Now you can use transformed_linear_train and transformed_test_kpca_linear for classification\n",
        "predicted_labels_linear = myclassifier(transformed_linear_train, train_labels.values, transformed_linear_test)\n",
        "\n",
        "# Calculate accuracy using sklearn's accuracy_score function\n",
        "accuracy_linear_kpca = calculate_accuracy(test_labels.values, predicted_labels_linear)\n",
        "print(f'Accuracy using linear_kcpa for :', accuracy_linear_kpca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vWhwyDI5Jzh3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Findings :\n",
        "\n",
        "## Comparing accuracies :\n",
        "\n",
        "- Accuracy using pca : 0.83\n",
        "- Accuracy using KPCA with rbf kernel : 0.025\n",
        "- Accuracy using KPCA with poly kernel : 0.605\n",
        "- Accuracy using KPCA with linear kernel : 0.76\n",
        "\n",
        "\n",
        "Accuracy obtained by using the reduced data with pca (without any kernel) produced better results than compared to other methods. simple PCA achieved an accuracy of 0.88. While rbf kernel performed poorly with 0.025 accuracy\n",
        "\n",
        "Given train data has around 10300 columns (features), but we have reduced the data to 120 principal components and yet we can achieve accuracies as high as 0.88.\n",
        "\n",
        "That means we are not loosing much of the information from the data, as we were able to predict 88% of the data correctly, even though we have reduced them.\n",
        "\n",
        "## Impact on classification performance :\n",
        "- Although we have reduced train data using differnet kernels or without kernel into same number of principal components, there is high difference in the accuracy (classification accuracy)\n",
        " - The data whcih is reduced using PCA performed better than rest of all and achieved an accuracy of 88%\n",
        " - While the data whcih is reduced using KPCA with rbf_kernel performed poorly and achieved an accuracy of 2.5%\n",
        " - So, the reduction technique that we used to reduce the data, greatly impacts the classification performance\n",
        "\n",
        "\n",
        "## Advantages and Disadvantages of using codes written from scratch over sklearn's functions :\n",
        "\n",
        "- Advantage is that ff we implement codes form scractch, we can customize the code for our requirements. We can learn and apply these learnings in other areas as well, whcih we cannot do if we have directly used sklearns functions\n",
        "\n",
        "- Disadvantage is that, as we have seen the PCA module from sklearn performed the operation 500 times faster than our my_pca code, so codes written from scratch are not efficient in terms of time taken. And also built-in functions are easy to use\n",
        "\n"
      ],
      "metadata": {
        "id": "P_ELYe58YRXh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0WGfxwVbYO4G"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pKRY8qpjJzjJ",
        "outputId": "e71f2616-326f-481b-b71d-bb74644b676e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covariance Matrix of Label Attribute:\n",
            "[[133.25]]\n"
          ]
        }
      ],
      "source": [
        "# Finding the covariance matrix of the train_labels from scratch\n",
        "\n",
        "# Step 1: Compute the mean of the attribute\n",
        "mean = np.mean(train_labels)\n",
        "\n",
        "# Step 2: Calculate squared differences\n",
        "squared_diff = (train_labels - mean) ** 2\n",
        "\n",
        "# Step 3: Sum up the squared differences\n",
        "sum_squared_diff = np.sum(squared_diff)\n",
        "\n",
        "# Step 4: Calculate the variance\n",
        "variance = sum_squared_diff / len(train_labels)\n",
        "\n",
        "# Step 5: Create a 1x1 covariance matrix with the variance value\n",
        "covariance_matrix = np.array([[variance]])\n",
        "\n",
        "# Print the covariance matrix\n",
        "print(\"Covariance Matrix of Label Attribute:\")\n",
        "print(covariance_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-tNFerS5gDV_"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}